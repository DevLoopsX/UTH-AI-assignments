# ğŸ“˜ E-Learning 5 - Exercise 2 - Question B: So SÃ¡nh vá»›i Sklearn

## ğŸ¯ Má»¥c TiÃªu BÃ i Táº­p

BÃ i táº­p yÃªu cáº§u **so sÃ¡nh káº¿t quáº£** giá»¯a **mÃ´ hÃ¬nh Logistic Regression tá»± xÃ¢y dá»±ng** (Question A) vá»›i **mÃ´ hÃ¬nh tá»« thÆ° viá»‡n sklearn** Ä‘á»ƒ kiá»ƒm chá»©ng tÃ­nh Ä‘Ãºng Ä‘áº¯n cá»§a implementation.

### ğŸ“Š Äá» BÃ i

**YÃªu cáº§u:**

BÃ i táº­p yÃªu cáº§u sá»­ dá»¥ng cÃ¹ng dataset nhÆ° Question A, sau Ä‘Ã³ huáº¥n luyá»‡n mÃ´ hÃ¬nh báº±ng **sklearn.linear_model.LogisticRegression**. Tiáº¿p theo, thá»±c hiá»‡n so sÃ¡nh káº¿t quáº£ giá»¯a 2 mÃ´ hÃ¬nh bao gá»“m tham sá»‘ há»c Ä‘Æ°á»£c (m/coef, b/intercept), xÃ¡c suáº¥t dá»± Ä‘oÃ¡n cho 2.8 giá» há»c, vÃ  káº¿t luáº­n Ä‘áº­u/rá»›t. Cuá»‘i cÃ¹ng, táº¡o biá»ƒu Ä‘á»“ so sÃ¡nh trá»±c quan (**Visualization**).

---

## ğŸ’» PhÃ¢n TÃ­ch Source Code Chi Tiáº¿t

### 1ï¸âƒ£ Import Libraries

```python
import sys
import os
# ThÃªm thÆ° má»¥c cha vÃ o path Ä‘á»ƒ import module
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from logistic_regression_utils import (
    DATASET, get_prediction, train_logistic_regression
)
from sklearn.linear_model import LogisticRegression
import numpy as np
import matplotlib.pyplot as plt
```

**Giáº£i thÃ­ch:**

#### **Import module tá»± xÃ¢y dá»±ng**

```python
from logistic_regression_utils import (
    DATASET, get_prediction, train_logistic_regression
)
```

Äoáº¡n code nÃ y import cÃ¡c hÃ m Ä‘Ã£ Ä‘Æ°á»£c xÃ¢y dá»±ng trong Question A Ä‘á»ƒ sá»­ dá»¥ng láº¡i. Viá»‡c tÃ¡i sá»­ dá»¥ng code nhÆ° váº­y giÃºp trÃ¡nh viáº¿t láº¡i toÃ n bá»™ logic training, vá»«a tiáº¿t kiá»‡m thá»i gian vá»«a Ä‘áº£m báº£o tÃ­nh nháº¥t quÃ¡n.

#### **Import sklearn**

```python
from sklearn.linear_model import LogisticRegression
```

ThÆ° viá»‡n sklearn (scikit-learn) lÃ  thÆ° viá»‡n Machine Learning phá»• biáº¿n nháº¥t trong Python, Ä‘Æ°á»£c phÃ¡t triá»ƒn vÃ  tá»‘i Æ°u hÃ³a ráº¥t ká»¹ lÆ°á»¡ng. Class `LogisticRegression` trong module `linear_model` cung cáº¥p má»™t implementation chuáº©n vÃ  hiá»‡u quáº£ cá»§a thuáº­t toÃ¡n Logistic Regression. Äiá»ƒm ná»•i báº­t cá»§a sklearn lÃ  highly optimized vá»›i backend Ä‘Æ°á»£c viáº¿t báº±ng C/C++, há»— trá»£ nhiá»u thuáº­t toÃ¡n tá»‘i Æ°u khÃ¡c nhau nhÆ° LBFGS, SAG, SAGA, vÃ  Ä‘Æ°á»£c kiá»ƒm thá»­ ká»¹ lÆ°á»¡ng nÃªn ráº¥t Ä‘Ã¡ng tin cáº­y trong á»©ng dá»¥ng thá»±c táº¿.

#### **Import numpy vÃ  matplotlib**

```python
import numpy as np
import matplotlib.pyplot as plt
```

Hai thÆ° viá»‡n nÃ y phá»¥c vá»¥ cÃ¡c má»¥c Ä‘Ã­ch khÃ¡c nhau trong quÃ¡ trÃ¬nh so sÃ¡nh. ThÆ° viá»‡n numpy Ä‘Æ°á»£c import vÃ¬ sklearn yÃªu cáº§u dá»¯ liá»‡u Ä‘áº§u vÃ o pháº£i á»Ÿ dáº¡ng numpy array thay vÃ¬ Python list thÃ´ng thÆ°á»ng. Trong khi Ä‘Ã³, matplotlib.pyplot Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ váº½ cÃ¡c biá»ƒu Ä‘á»“ so sÃ¡nh giá»¯a hai mÃ´ hÃ¬nh má»™t cÃ¡ch trá»±c quan.

---

### 2ï¸âƒ£ PHáº¦N A: MÃ´ HÃ¬nh Tá»± XÃ¢y Dá»±ng

```python
# ========== PHáº¦N A: MÃ” HÃŒNH Tá»° XÃ‚Y Dá»°NG ==========

# Huáº¥n luyá»‡n mÃ´ hÃ¬nh vá»›i n = 10 iterations
m, b, costs = train_logistic_regression(
    dataset=DATASET,
    m_init=1.0,
    b_init=-1.0,
    iterations=10,
    learning_rate=1.0
)
# Dá»± Ä‘oÃ¡n cho sinh viÃªn há»c 2.8 giá»
hours_input = 2.8
predicted_score_manual = get_prediction(m, b, hours_input)
```

**Giáº£i thÃ­ch:**

#### **Training**

Pháº§n nÃ y sá»­ dá»¥ng láº¡i toÃ n bá»™ code Ä‘Ã£ Ä‘Æ°á»£c viáº¿t trong Question A, cháº¡y Ä‘Ãºng 10 iterations theo yÃªu cáº§u cá»§a Ä‘á» bÃ i. Viá»‡c tÃ¡i sá»­ dá»¥ng code giÃºp Ä‘áº£m báº£o tÃ­nh nháº¥t quÃ¡n khi so sÃ¡nh giá»¯a hai mÃ´ hÃ¬nh.

#### **Prediction**

Sau khi training xong, mÃ´ hÃ¬nh Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ dá»± Ä‘oÃ¡n cho trÆ°á»ng há»£p sinh viÃªn há»c 2.8 giá». Káº¿t quáº£ dá»± Ä‘oÃ¡n Ä‘Æ°á»£c lÆ°u vÃ o biáº¿n `predicted_score_manual` Ä‘á»ƒ chuáº©n bá»‹ cho pháº§n so sÃ¡nh vá»›i sklearn sau nÃ y.

**Táº¡i sao gá»i lÃ  "manual"?**

Thuáº­t ngá»¯ "manual" Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ phÃ¢n biá»‡t vá»›i sklearn (automated/optimized). Tá»« "manual" á»Ÿ Ä‘Ã¢y nghÄ©a lÃ  tá»± code tá»« Ä‘áº§u, thá»±c hiá»‡n tá»«ng bÆ°á»›c má»™t cÃ¡ch rÃµ rÃ ng, thay vÃ¬ dÃ¹ng cÃ¡c hÃ m cÃ³ sáºµn Ä‘Ã£ Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a cao.

---

### 3ï¸âƒ£ In Káº¿t Quáº£ MÃ´ HÃ¬nh Tá»± XÃ¢y Dá»±ng

```python
print("\n" + "="*60)
print("BÃ€I 2 - QUESTION B: SO SÃNH MÃ” HÃŒNH Tá»° XÃ‚Y Dá»°NG Vá»šI SKLEARN")
print("="*60)

print("\n" + "-"*60)
print("PHáº¦N A: Káº¾T QUáº¢ MÃ” HÃŒNH Tá»° XÃ‚Y Dá»°NG")
print("-"*60)
print(f"Tham sá»‘ há»c Ä‘Æ°á»£c:")
print(f"  - Há»‡ sá»‘ gÃ³c (m): {m:.6f}")
print(f"  - Há»‡ sá»‘ cháº·n (b): {b:.6f}")
print(f"\nDá»± Ä‘oÃ¡n cho sinh viÃªn há»c {hours_input} giá»:")
print(f"  - XÃ¡c suáº¥t Ä‘áº­u: {predicted_score_manual:.6f} ({predicted_score_manual*100:.2f}%)")
if predicted_score_manual >= 0.5:
    print(f"  - Káº¿t luáº­n: Äáº¬U")
else:
    print(f"  - Káº¿t luáº­n: Rá»šT")
```

**Giáº£i thÃ­ch:**

#### **Cáº¥u trÃºc output**

Pháº§n in káº¿t quáº£ Ä‘Æ°á»£c tá»• chá»©c theo cáº¥p báº­c rÃµ rÃ ng. Header lá»›n vá»›i dáº¥u báº±ng (=) Ä‘Æ°á»£c dÃ¹ng cho tiÃªu Ä‘á» bÃ i toÃ¡n, trong khi header nhá» vá»›i dáº¥u gáº¡ch ngang (-) Ä‘Ã¡nh dáº¥u tiÃªu Ä‘á» tá»«ng pháº§n cá»¥ thá»ƒ.

#### **In tham sá»‘**

```python
print(f"  - Há»‡ sá»‘ gÃ³c (m): {m:.6f}")
print(f"  - Há»‡ sá»‘ cháº·n (b): {b:.6f}")
```

CÃ¡c tham sá»‘ Ä‘Æ°á»£c in vá»›i format `.6f` Ä‘á»ƒ hiá»ƒn thá»‹ 6 chá»¯ sá»‘ tháº­p phÃ¢n, Ä‘áº£m báº£o Ä‘á»™ chÃ­nh xÃ¡c cao. Viá»‡c thá»¥t Ä‘áº§u dÃ²ng vá»›i kÃ½ tá»± ` -` giÃºp cáº¥u trÃºc output dá»… Ä‘á»c vÃ  tháº©m má»¹ hÆ¡n.

#### **In káº¿t quáº£ dá»± Ä‘oÃ¡n**

```python
print(f"  - XÃ¡c suáº¥t Ä‘áº­u: {predicted_score_manual:.6f} ({predicted_score_manual*100:.2f}%)")
```

DÃ²ng code nÃ y in cáº£ dáº¡ng tháº­p phÃ¢n vÃ  pháº§n trÄƒm, vÃ­ dá»¥ `0.785432 (78.54%)`.

#### **Káº¿t luáº­n**

```python
if predicted_score_manual >= 0.5:
    print(f"  - Káº¿t luáº­n: Äáº¬U")
else:
    print(f"  - Káº¿t luáº­n: Rá»šT")
```

Logic Ä‘Æ¡n giáº£n: náº¿u xÃ¡c suáº¥t â‰¥ 0.5 thÃ¬ káº¿t luáº­n Äáº¬U.

---

### 4ï¸âƒ£ PHáº¦N B: MÃ´ HÃ¬nh Sklearn

#### **4.1. Chuáº©n Bá»‹ Dá»¯ Liá»‡u**

```python
# Chuáº©n bá»‹ dá»¯ liá»‡u cho sklearn
X = np.array([[row[0]] for row in DATASET])  # Features (Hours)
y_train = np.array([row[1] for row in DATASET])  # Labels (Pass)
```

**Giáº£i thÃ­ch:**

##### **Features (X)**

```python
X = np.array([[row[0]] for row in DATASET])
```

**PhÃ¢n tÃ­ch:**

Biá»ƒu thá»©c nÃ y sá»­ dá»¥ng list comprehension Ä‘á»ƒ trÃ­ch xuáº¥t dá»¯ liá»‡u. `row[0]` láº¥y cá»™t Ä‘áº§u tiÃªn (Hours) tá»« má»—i dÃ²ng dá»¯ liá»‡u. Äiá»ƒm quan trá»ng cáº§n chÃº Ã½ lÃ  cÃ³ hai cáº·p ngoáº·c vuÃ´ng: ngoáº·c trong `[row[0]]` táº¡o má»™t list chá»©a 1 pháº§n tá»­, cÃ²n ngoáº·c ngoÃ i `[...]` lÃ  cÃº phÃ¡p cá»§a list comprehension.

**Táº¡i sao cáº§n 2 cáº·p ngoáº·c?**

Sklearn yÃªu cáº§u X pháº£i lÃ  ma tráº­n 2D vá»›i kÃ­ch thÆ°á»›c (n_samples Ã— n_features), trong Ä‘Ã³ `n_samples` lÃ  sá»‘ máº«u (8 Ä‘iá»ƒm) vÃ  `n_features` lÃ  sá»‘ Ä‘áº·c trÆ°ng (1 feature = hours).

**Káº¿t quáº£:**

Ma tráº­n káº¿t quáº£ cÃ³ dáº¡ng:

```python
X = [[0.5],
     [1.0],
     [1.5],
     [2.0],
     [2.5],
     [3.0],
     [3.5],
     [4.0]]
# Shape: (8, 1) - 8 hÃ ng, 1 cá»™t
```

**Náº¿u chá»‰ dÃ¹ng 1 cáº·p ngoáº·c:**

Náº¿u viáº¿t `X = np.array([row[0] for row in DATASET])`, káº¿t quáº£ sáº½ lÃ  `[0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0]` vá»›i shape (8,) - má»™t 1D array. Äiá»u nÃ y sáº½ khiáº¿n sklearn bÃ¡o lá»—i vÃ¬ khÃ´ng Ä‘Ãºng format yÃªu cáº§u.

##### **Labels (y_train)**

```python
y_train = np.array([row[1] for row in DATASET])
```

**PhÃ¢n tÃ­ch:**

Biá»ƒu thá»©c `row[1]` láº¥y cá»™t thá»© hai (Pass) tá»« má»—i dÃ²ng dá»¯ liá»‡u. KhÃ¡c vá»›i X, y chá»‰ cáº§n lÃ  má»™t 1D array vá»›i ná»™i dung `[0, 0, 0, 0, 1, 1, 1, 1]`, cÃ³ shape (8,) tÆ°Æ¡ng á»©ng vá»›i 8 pháº§n tá»­.

**LÆ°u Ã½ naming:**

Biáº¿n Ä‘Æ°á»£c Ä‘áº·t tÃªn lÃ  `y_train` thay vÃ¬ `y` Ä‘á»ƒ trÃ¡nh nháº§m láº«n vá»›i biáº¿n `y` Ä‘Ã£ Ä‘Æ°á»£c sá»­ dá»¥ng á»Ÿ pháº§n A, giÃºp code rÃµ rÃ ng vÃ  dá»… báº£o trÃ¬ hÆ¡n.

---

#### **4.2. Táº¡o vÃ  Huáº¥n Luyá»‡n MÃ´ HÃ¬nh**

```python
# Táº¡o vÃ  huáº¥n luyá»‡n mÃ´ hÃ¬nh Logistic Regression
model = LogisticRegression(max_iter=10, solver='lbfgs', random_state=42)
model.fit(X, y_train)
```

**Giáº£i thÃ­ch:**

##### **Táº¡o model**

```python
model = LogisticRegression(max_iter=10, solver='lbfgs', random_state=42)
```

**CÃ¡c tham sá»‘:**

1. **`max_iter=10`**

    - **Maximum iterations:** Sá»‘ vÃ²ng láº·p tá»‘i Ä‘a
    - Äáº·t = 10 Ä‘á»ƒ **cÃ´ng báº±ng** vá»›i mÃ´ hÃ¬nh tá»± xÃ¢y dá»±ng
    - Máº·c Ä‘á»‹nh sklearn = 100
    - **LÆ°u Ã½:** Sklearn cÃ³ thá»ƒ há»™i tá»¥ sá»›m hÆ¡n 10 iterations náº¿u Ä‘áº¡t tolerance

2. **`solver='lbfgs'`**

    - **Thuáº­t toÃ¡n tá»‘i Æ°u:** Limited-memory BFGS
    - **LBFGS (Limited-memory Broydenâ€“Fletcherâ€“Goldfarbâ€“Shanno):**
        - Thuáº­t toÃ¡n quasi-Newton
        - Hiá»‡u quáº£ hÆ¡n Gradient Descent thÃ´ng thÆ°á»ng
        - Sá»­ dá»¥ng approximation cá»§a ma tráº­n Hessian
        - Tá»‘t cho dataset nhá»/trung bÃ¬nh

    **CÃ¡c solver khÃ¡c:**

    - `'liblinear'`: Tá»‘t cho dataset nhá»
    - `'saga'`: Tá»‘t cho dataset lá»›n
    - `'sag'`: Stochastic Average Gradient
    - `'newton-cg'`: Newton-Conjugate-Gradient

3. **`random_state=42`**
    - **Seed cho random number generator**
    - Äáº£m báº£o **káº¿t quáº£ reproducible** (cháº¡y láº¡i ra cÃ¹ng káº¿t quáº£)
    - 42 lÃ  sá»‘ phá»• biáº¿n (The Hitchhiker's Guide to the Galaxy reference ğŸ˜Š)
    - Quan trá»ng cho debugging vÃ  so sÃ¡nh

**Táº¡i sao sklearn cáº§n random_state?**

Sklearn cáº§n random_state vÃ¬ má»™t sá»‘ solver khá»Ÿi táº¡o tham sá»‘ ngáº«u nhiÃªn. NgoÃ i ra, cÃ³ thá»ƒ xáº£o trá»™n dá»¯ liá»‡u khi dÃ¹ng batch methods. Viá»‡c thiáº¿t láº­p random_state Ä‘áº£m báº£o reproducibility cho nghiÃªn cá»©u khoa há»c.

##### **Fit model**

```python
model.fit(X, y_train)
```

**Giáº£i thÃ­ch:**

HÃ m **`fit(X, y)`** lÃ  hÃ m huáº¥n luyá»‡n mÃ´ hÃ¬nh. **Input** gá»“m `X` lÃ  Features (ma tráº­n 8Ã—1) vÃ  `y_train` lÃ  Labels (8 pháº§n tá»­). **Process** bao gá»“m cháº¡y thuáº­t toÃ¡n LBFGS, tá»‘i Æ°u hÃ³a tham sá»‘ (coef, intercept), vá»›i tá»‘i Ä‘a 10 iterations. **Output** lÃ  `model` Ä‘Æ°á»£c cáº­p nháº­t (in-place), vá»›i tham sá»‘ Ä‘Æ°á»£c lÆ°u trong `model.coef_` vÃ  `model.intercept_`.

**LÆ°u Ã½:**

Sklearn tá»± Ä‘á»™ng normalize/standardize náº¿u cáº§n, tá»± Ä‘á»™ng xá»­ lÃ½ convergence, vÃ  tá»± Ä‘á»™ng Ä‘iá»u chá»‰nh learning rate (adaptive).

---

#### **4.3. Dá»± ÄoÃ¡n vá»›i Sklearn**

```python
# Dá»± Ä‘oÃ¡n vá»›i sklearn
X_test = np.array([[hours_input]])
predicted_proba_sklearn = model.predict_proba(X_test)[0][1]  # XÃ¡c suáº¥t cho class 1 (Pass)
predicted_class_sklearn = model.predict(X_test)[0]
```

**Giáº£i thÃ­ch:**

##### **Chuáº©n bá»‹ test data**

```python
X_test = np.array([[hours_input]])
```

Biáº¿n `hours_input` cÃ³ giÃ¡ trá»‹ 2.8, do Ä‘Ã³ `[[2.8]]` táº¡o ma tráº­n 2D vá»›i kÃ­ch thÆ°á»›c 1Ã—1 (1 sample, 1 feature). Äiá»ƒm quan trá»ng lÃ  pháº£i cÃ¹ng format vá»›i X training (2D).

##### **Dá»± Ä‘oÃ¡n xÃ¡c suáº¥t**

```python
predicted_proba_sklearn = model.predict_proba(X_test)[0][1]
```

**PhÃ¢n tÃ­ch:**

1. **`model.predict_proba(X_test)`:**

    - Tráº£ vá» **ma tráº­n xÃ¡c suáº¥t** cho táº¥t cáº£ classes
    - Shape: (n_samples, n_classes)
    - Vá»›i X_test shape (1, 1): Output shape (1, 2)
    - **2 classes:** [xÃ¡c suáº¥t class 0, xÃ¡c suáº¥t class 1]

    **VÃ­ dá»¥:**

    ```python
    predict_proba(X_test) = [[0.2146, 0.7854]]
    # Class 0: 21.46%
    # Class 1: 78.54%
    ```

2. **`[0]`:** Láº¥y sample Ä‘áº§u tiÃªn (vÃ¬ chá»‰ cÃ³ 1 sample)

    ```python
    [0.2146, 0.7854]
    ```

3. **`[1]`:** Láº¥y xÃ¡c suáº¥t cá»§a **class 1** (Pass)
    ```python
    0.7854
    ```

**TÃ³m láº¡i:** `[0][1]` = xÃ¡c suáº¥t Pass cá»§a sample Ä‘áº§u tiÃªn

##### **Dá»± Ä‘oÃ¡n class**

```python
predicted_class_sklearn = model.predict(X_test)[0]
```

**PhÃ¢n tÃ­ch:**

1. **`model.predict(X_test)`:**

    - Tráº£ vá» **nhÃ£n dá»± Ä‘oÃ¡n** (0 hoáº·c 1)
    - ÄÃ£ apply threshold 0.5 tá»± Ä‘á»™ng
    - Output: `[1]` (array vá»›i 1 pháº§n tá»­)

2. **`[0]`:** Láº¥y pháº§n tá»­ Ä‘áº§u
    - Káº¿t quáº£: `1` (sá»‘ nguyÃªn)
    - 1 = Äáº­u, 0 = Rá»›t

**So sÃ¡nh predict vs predict_proba:**

| Method          | Output   | Example          |
| --------------- | -------- | ---------------- |
| `predict_proba` | XÃ¡c suáº¥t | `[[0.21, 0.79]]` |
| `predict`       | NhÃ£n     | `[1]`            |

---

#### **4.4. In Káº¿t Quáº£ Sklearn**

```python
print("\n" + "-"*60)
print("PHáº¦N B: Káº¾T QUáº¢ MÃ” HÃŒNH SKLEARN")
print("-"*60)

print(f"Tham sá»‘ há»c Ä‘Æ°á»£c:")
print(f"  - Há»‡ sá»‘ gÃ³c (coef): {model.coef_[0][0]:.6f}")
print(f"  - Há»‡ sá»‘ cháº·n (intercept): {model.intercept_[0]:.6f}")
print(f"\nDá»± Ä‘oÃ¡n cho sinh viÃªn há»c {hours_input} giá»:")
print(f"  - XÃ¡c suáº¥t Ä‘áº­u: {predicted_proba_sklearn:.6f} ({predicted_proba_sklearn*100:.2f}%)")
if predicted_class_sklearn == 1:
    print(f"  - Káº¿t luáº­n: Äáº¬U")
else:
    print(f"  - Káº¿t luáº­n: Rá»šT")
```

**Giáº£i thÃ­ch:**

##### **Truy cáº­p tham sá»‘ sklearn**

Äá»ƒ láº¥y **há»‡ sá»‘ gÃ³c:**

```python
model.coef_[0][0]
```

Thuá»™c tÃ­nh **`model.coef_`** lÃ  ma tráº­n há»‡ sá»‘ vá»›i kÃ­ch thÆ°á»›c (n_classes-1, n_features). Äá»‘i vá»›i Logistic Regression binary, kÃ­ch thÆ°á»›c lÃ  (1, 1) nhÆ° `[[2.345]]` (1 class, 1 feature). Äá» tÃ¡c `[0]` láº¥y hÃ ng Ä‘áº§u cho káº¿t quáº£ `[2.345]`, rá»“i `[0]` tiáº¿p theo láº¥y cá»™t Ä‘áº§u cho káº¿t quáº£ cuá»‘i cÃ¹ng `2.345`.

Äá»ƒ láº¥y **há»‡ sá»‘ cháº·n:**

```python
model.intercept_[0]
```

Thuá»™c tÃ­nh **`model.intercept_`** lÃ  array há»‡ sá»‘ cháº·n vá»›i kÃ­ch thÆ°á»›c (n_classes-1,) nhÆ° `[-4.567]` (1 pháº§n tá»­). Äá» tÃ¡c `[0]` láº¥y pháº§n tá»­ Ä‘áº§u cho káº¿t quáº£ `-4.567`.

##### **In káº¿t quáº£**

Format output Ä‘Æ°á»£c thiáº¿t káº¿ giá»‘ng pháº§n A Ä‘á»ƒ dá»… so sÃ¡nh, sá»­ dá»¥ng `.6f` cho Ä‘á»™ chÃ­nh xÃ¡c cao.

---

### 5ï¸âƒ£ So SÃ¡nh Káº¿t Quáº£

```python
# ========== SO SÃNH Káº¾T QUáº¢ ==========

print(f"\nHá»‡ sá»‘ gÃ³c (m/coef):")
print(f"  - MÃ´ hÃ¬nh tá»± xÃ¢y dá»±ng: {m:.6f}")
print(f"  - Sklearn:              {model.coef_[0][0]:.6f}")
print(f"  - ChÃªnh lá»‡ch:           {abs(m - model.coef_[0][0]):.6f}")

print(f"\nHá»‡ sá»‘ cháº·n (b/intercept):")
print(f"  - MÃ´ hÃ¬nh tá»± xÃ¢y dá»±ng: {b:.6f}")
print(f"  - Sklearn:              {model.intercept_[0]:.6f}")
print(f"  - ChÃªnh lá»‡ch:           {abs(b - model.intercept_[0]):.6f}")

print(f"\nXÃ¡c suáº¥t Ä‘áº­u cho {hours_input} giá» há»c:")
print(f"  - MÃ´ hÃ¬nh tá»± xÃ¢y dá»±ng: {predicted_score_manual:.6f} ({predicted_score_manual*100:.2f}%)")
print(f"  - Sklearn:              {predicted_proba_sklearn:.6f} ({predicted_proba_sklearn*100:.2f}%)")
print(f"  - ChÃªnh lá»‡ch:           {abs(predicted_score_manual - predicted_proba_sklearn):.6f}")

print(f"\nKáº¿t luáº­n dá»± Ä‘oÃ¡n:")
result_manual = "Äáº¬U" if predicted_score_manual >= 0.5 else "Rá»šT"
result_sklearn = "Äáº¬U" if predicted_class_sklearn == 1 else "Rá»šT"
print(f"  - MÃ´ hÃ¬nh tá»± xÃ¢y dá»±ng: {result_manual}")
print(f"  - Sklearn:             {result_sklearn}")
if result_manual == result_sklearn:
    print(f"  - Káº¿t quáº£: GIá»NG NHAU âœ“")
else:
    print(f"  - Káº¿t quáº£: KHÃC NHAU âœ—")
```

**Giáº£i thÃ­ch:**

#### **So sÃ¡nh tá»«ng thÃ nh pháº§n**

##### **1. Há»‡ sá»‘ gÃ³c (m vs coef)**

```python
print(f"  - ChÃªnh lá»‡ch:           {abs(m - model.coef_[0][0]):.6f}")
```

Äoáº¡n code nÃ y tÃ­nh **giÃ¡ trá»‹ tuyá»‡t Ä‘á»‘i** cá»§a sá»± chÃªnh lá»‡ch. HÃ m `abs()` luÃ´n tráº£ vá» sá»‘ dÆ°Æ¡ng nÃªn dá»… so sÃ¡nh, vá»›i ká»³ vá»ng chÃªnh lá»‡ch nhá» hÆ¡n 0.1.

##### **2. Há»‡ sá»‘ cháº·n (b vs intercept)**

```python
print(f"  - ChÃªnh lá»‡ch:           {abs(b - model.intercept_[0]):.6f}")
```

CÃ¡ch tÃ­nh tÆ°Æ¡ng tá»± vá»›i m, cÅ©ng ká»³ vá»ng chÃªnh lá»‡ch nhá».

##### **3. XÃ¡c suáº¥t dá»± Ä‘oÃ¡n**

```python
print(f"  - ChÃªnh lá»‡ch:           {abs(predicted_score_manual - predicted_proba_sklearn):.6f}")
```

ÄÃ¢y lÃ  bÆ°á»›c so sÃ¡nh output cuá»‘i cÃ¹ng, vÃ  cÅ©ng lÃ  **quan trá»ng nháº¥t** Ä‘á»ƒ kiá»ƒm tra káº¿t quáº£ dá»± Ä‘oÃ¡n cÃ³ Ä‘Ãºng khÃ´ng. Ká»³ vá»ng chÃªnh lá»‡ch ráº¥t nhá», thÆ°á»ng nhá» hÆ¡n 0.01.

##### **4. Káº¿t luáº­n cuá»‘i cÃ¹ng**

```python
result_manual = "Äáº¬U" if predicted_score_manual >= 0.5 else "Rá»šT"
result_sklearn = "Äáº¬U" if predicted_class_sklearn == 1 else "Rá»šT"
```

Äoáº¡n code nÃ y chuyá»ƒn sá»‘ thÃ nh text Ä‘á»ƒ dá»… Ä‘á»c, sau Ä‘Ã³ so sÃ¡nh string Ä‘á»ƒ kiá»ƒm tra consistency.

```python
if result_manual == result_sklearn:
    print(f"  - Káº¿t quáº£: GIá»NG NHAU âœ“")
else:
    print(f"  - Káº¿t quáº£: KHÃC NHAU âœ—")
```

Náº¿u káº¿t quáº£ giá»‘ng nhau (âœ“), chá»©ng tá» implementation Ä‘Ãºng! NgÆ°á»£c láº¡i náº¿u khÃ¡c nhau (âœ—), cÃ³ váº¥n Ä‘á» cáº§n kiá»ƒm tra láº¡i.

---

### 6ï¸âƒ£ Pháº§n Káº¿t Luáº­n Tá»•ng Quan

```python
print("\n" + "="*60)
print("Káº¾T LUáº¬N")
print("="*60)
print("CÃ³ thá»ƒ tháº¥y sá»± khÃ¡c biá»‡t giá»¯a hai mÃ´ hÃ¬nh do:")
print("  1. Sá»‘ láº§n láº·p khÃ¡c nhau (10 vs thuáº­t toÃ¡n tá»‘i Æ°u cá»§a sklearn)")
print("  2. PhÆ°Æ¡ng phÃ¡p tá»‘i Æ°u khÃ¡c nhau (Gradient Descent vs LBFGS)")
print("  3. Äiá»u kiá»‡n dá»«ng vÃ  khá»Ÿi táº¡o tham sá»‘ khÃ¡c nhau")
print("="*60 + "\n")
```

**Giáº£i thÃ­ch:**

#### **LÃ½ do cÃ³ sá»± khÃ¡c biá»‡t**

##### **1. Sá»‘ láº§n láº·p khÃ¡c nhau**

MÃ´ hÃ¬nh **Manual** cháº¡y Ä‘Ãºng 10 iterations, khÃ´ng thÃªm khÃ´ng bá»›t. Trong khi Ä‘Ã³, **Sklearn** cÃ³ thá»ƒ há»™i tá»¥ sá»›m hÆ¡n náº¿u Ä‘áº¡t tolerance. Vá»›i máº·c Ä‘á»‹nh `tol=1e-4`, thuáº­t toÃ¡n dá»«ng khi gradient < tolerance, do Ä‘Ã³ cÃ³ thá»ƒ dá»«ng sau 5-8 iterations.

##### **2. PhÆ°Æ¡ng phÃ¡p tá»‘i Æ°u khÃ¡c nhau**

**Manual - Gradient Descent:**

Sá»­ dá»¥ng cÃ´ng thá»©c:

$$w_{new} = w_{old} - \alpha \nabla J$$

ÄÃ¢y lÃ  **Gradient báº­c 1** (first-order derivative), Ä‘Æ¡n giáº£n vÃ  dá»… hiá»ƒu nhÆ°ng tá»‘c Ä‘á»™ há»™i tá»¥ lÃ  **tuyáº¿n tÃ­nh** (linear).

**Sklearn - LBFGS:**

ÄÃ¢y lÃ  **Quasi-Newton method** sá»­ dá»¥ng **gradient báº­c 2** (approximated Hessian). PhÆ°Æ¡ng phÃ¡p nÃ y phá»©c táº¡p hÆ¡n nhÆ°ng **hiá»‡u quáº£ hÆ¡n**, vá»›i tá»‘c Ä‘á»™ há»™i tá»¥ lÃ  **siÃªu tuyáº¿n tÃ­nh** (superlinear).

**VÃ­ dá»¥:**

Vá»›i GD, 10 steps cÃ³ thá»ƒ chá»‰ giáº£m Cost 60%, trong khi LBFGS vá»›i 10 steps cÃ³ thá»ƒ giáº£m Cost Ä‘áº¿n 95%.

##### **3. Äiá»u kiá»‡n dá»«ng vÃ  khá»Ÿi táº¡o**

**Khá»Ÿi táº¡o:**

Manual sá»­ dá»¥ng m=1.0, b=-1.0 (do mÃ¬nh chá»n), trong khi Sklearn sá»­ dá»¥ng w=0, b=0 (máº·c Ä‘á»‹nh) hoáº·c random.

**Äiá»u kiá»‡n dá»«ng:**

Manual cháº¡y Ä‘Ãºng 10 iterations mÃ  khÃ´ng kiá»ƒm tra convergence, cÃ²n Sklearn dá»«ng khi `||gradient|| < tol` hoáº·c Ä‘áº¡t `max_iter`.

**Learning rate:**

Manual dÃ¹ng learning rate cá»‘ Ä‘á»‹nh (1.0), nhÆ°ng Sklearn sá»­ dá»¥ng Adaptive learning rate (LBFGS tá»± Ä‘iá»u chá»‰nh).

---

### 7ï¸âƒ£ Visualization

#### **7.1. Táº¡o Figure vá»›i 2 Subplots**

```python
# ========== VISUALIZATION ==========

# Táº¡o figure vá»›i 2 subplots
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
```

**Giáº£i thÃ­ch:**

Lá»‡nh nÃ y táº¡o 2 biá»ƒu Ä‘á»“ cáº¡nh nhau vá»›i kÃ­ch thÆ°á»›c lá»›n (14Ã—5) Ä‘á»ƒ káº¿t quáº£ rÃµ rÃ ng.

---

#### **7.2. Subplot 1: So SÃ¡nh Decision Boundary**

```python
# Subplot 1: So sÃ¡nh decision boundary cá»§a 2 mÃ´ hÃ¬nh
x_plot = np.linspace(0, 4.5, 100)
y_manual = [get_prediction(m, b, x_i) for x_i in x_plot]
y_sklearn = [model.predict_proba([[x_i]])[0][1] for x_i in x_plot]
```

**Giáº£i thÃ­ch:**

##### **Táº¡o dáº£i x**

```python
x_plot = np.linspace(0, 4.5, 100)
```

HÃ m nÃ y táº¡o 100 Ä‘iá»ƒm tá»« 0 Ä‘áº¿n 4.5, Ä‘á»§ má»‹n Ä‘á»ƒ váº½ Ä‘Æ°á»ng cong mÆ°á»£t mÃ i.

##### **TÃ­nh y cho manual model**

```python
y_manual = [get_prediction(m, b, x_i) for x_i in x_plot]
```

Sá»­ dá»¥ng list comprehension Ä‘á»ƒ duyá»‡t 100 Ä‘iá»ƒm, gá»i hÃ m `get_prediction` tá»« module tá»± xÃ¢y dá»±ng. Káº¿t quáº£ lÃ  100 giÃ¡ trá»‹ xÃ¡c suáº¥t.

##### **TÃ­nh y cho sklearn**

```python
y_sklearn = [model.predict_proba([[x_i]])[0][1] for x_i in x_plot]
```

Biáº¿u thá»©c `[[x_i]]` reshape thÃ nh 2D array, sau Ä‘Ã³ `predict_proba(...)` dá»± Ä‘oÃ¡n xÃ¡c suáº¥t, rá»“i `[0][1]` láº¥y xÃ¡c suáº¥t class 1.

**LÆ°u Ã½:** CÃ¡ch tÃ­nh hÆ¡i "cá»“ng ká»nh" vá»›i list comprehension, cÃ³ thá»ƒ optimize:

```python
# CÃ¡ch tá»‘t hÆ¡n:
X_plot = x_plot.reshape(-1, 1)
y_sklearn = model.predict_proba(X_plot)[:, 1]
```

---

##### **Váº½ dá»¯ liá»‡u gá»‘c**

```python
# Váº½ dá»¯ liá»‡u gá»‘c
x_data = [row[0] for row in DATASET]
y_data = [row[1] for row in DATASET]
ax1.scatter(x_data, y_data, c=['red' if y==0 else 'green' for y in y_data],
            s=100, alpha=0.6, edgecolors='black', linewidth=1.5,
            label='Dá»¯ liá»‡u thá»±c táº¿', zorder=3)
```

**Giáº£i thÃ­ch:**

Tham sá»‘ **`c=[...]`** thiáº¿t láº­p mÃ u theo nhÃ£n. Biá»ƒu thá»©c `'red' if y==0` chá»n mÃ u Ä‘á» cho Rá»›t vÃ  `'green'` cho Äáº­u. List comprehension táº¡o list mÃ u nhÆ° `['red', 'red', ..., 'green', 'green']`. Tham sá»‘ **`s=100`** Ä‘áº·t kÃ­ch thÆ°á»›c Ä‘iá»ƒm báº±ng 100. Tham sá»‘ **`alpha=0.6`** thiáº¿t láº­p Ä‘á»™ trong suá»‘t 60% (hÆ¡i má»). Tham sá»‘ **`edgecolors='black'`** thÃªm viá»n Ä‘en cho rÃµ rÃ ng, vá»›i **`linewidth=1.5`** Ä‘áº·t Ä‘á»™ dÃ y viá»n. Tham sá»‘ **`zorder=3`** Ä‘áº·t layer cao (váº½ trÃªn cÃ¹ng), Ä‘áº£m báº£o Ä‘iá»ƒm khÃ´ng bá»‹ Ä‘Æ°á»ng che.

---

##### **Váº½ Ä‘Æ°á»ng sigmoid**

```python
# Váº½ Ä‘Æ°á»ng sigmoid
ax1.plot(x_plot, y_manual, 'b-', linewidth=2, label='MÃ´ hÃ¬nh tá»± xÃ¢y dá»±ng')
ax1.plot(x_plot, y_sklearn, 'r--', linewidth=2, label='Sklearn')
```

**Giáº£i thÃ­ch:**

ÄÆ°á»ng **Manual** sá»­ dá»¥ng `'b-'` (mÃ u xanh blue, Ä‘Æ°á»ng liá»n solid) vá»›i `linewidth=2` (independentá»™ dÃ y 2). ÄÆ°á»ng **Sklearn** sá»­ dá»¥ng `'r--'` (mÃ u Ä‘á» red, Ä‘Æ°á»ng gáº¡ch dashed) Ä‘á»ƒ dá»… phÃ¢n biá»‡t vá»›i manual. Ká»³ vá»ng lÃ  2 Ä‘Æ°á»ng ráº¥t gáº§n nhau, gáº§n nhÆ° trÃ¹ng.

---

##### **Váº½ ngÆ°á»¡ng 0.5**

```python
# Váº½ ngÆ°á»¡ng 0.5
ax1.axhline(y=0.5, color='gray', linestyle=':', linewidth=1, label='NgÆ°á»¡ng 0.5')
```

**Giáº£i thÃ­ch:**

HÃ m `axhline` váº½ Ä‘Æ°á»ng ngang táº¡i `y=0.5`, sá»­ dá»¥ng `linestyle=':'` cho Ä‘Æ°á»ng cháº¥m (dotted). Ã nghÄ©a cá»§a Ä‘Æ°á»ng nÃ y lÃ  decision boundary threshold.

---

##### **Váº½ Ä‘iá»ƒm dá»± Ä‘oÃ¡n 2.8h**

```python
# Váº½ Ä‘iá»ƒm dá»± Ä‘oÃ¡n cho 2.8 giá»
ax1.scatter([hours_input], [predicted_score_manual], c='blue', s=200,
            marker='*', edgecolors='black', linewidth=1.5,
            label=f'Dá»± Ä‘oÃ¡n {hours_input}h (Manual)', zorder=4)
ax1.scatter([hours_input], [predicted_proba_sklearn], c='red', s=200,
            marker='*', edgecolors='black', linewidth=1.5,
            label=f'Dá»± Ä‘oÃ¡n {hours_input}h (Sklearn)', zorder=4)
```

**Giáº£i thÃ­ch:**

Marker `'*'` táº¡o hÃ¬nh ngÃ´i sao ná»•i báº­t, vá»›i `s=200` lÃ  kÃ­ch thÆ°á»›c lá»›n. Tham sá»‘ `zorder=4` Ä‘áº·t layer cao nháº¥t Ä‘á»ƒ váº½ trÃªn cÃ¹ng. CÃ³ 2 Ä‘iá»ƒm: Manual mÃ u xanh vÃ  Sklearn mÃ u Ä‘á». Ká»³ vá»ng lÃ  2 Ä‘iá»ƒm ráº¥t gáº§n nhau (gáº§n nhÆ° trÃ¹ng).

---

##### **Trang trÃ­ subplot 1**

```python
ax1.set_xlabel('Sá»‘ giá» há»c', fontsize=11, fontweight='bold')
ax1.set_ylabel('XÃ¡c suáº¥t Ä‘áº­u', fontsize=11, fontweight='bold')
ax1.set_title('So sÃ¡nh Decision Boundary\nMÃ´ hÃ¬nh tá»± xÃ¢y dá»±ng vs Sklearn', fontsize=12, fontweight='bold')
ax1.legend(loc='upper left', fontsize=9)
ax1.grid(True, alpha=0.3)
ax1.set_ylim(-0.1, 1.1)
```

**Giáº£i thÃ­ch:**

Title Ä‘Æ°á»£c chia lÃ m 2 dÃ²ng vá»›i `\n`. Legend Ä‘áº·t á»Ÿ gÃ³c trÃªn trÃ¡i. Grid cÃ³ Ä‘á»™ trong suá»‘t 0.3 Ä‘á»ƒ khÃ´ng quÃ¡ ná»•i. Tham sá»‘ ylim Ä‘Æ°á»£c Ä‘áº·t tá»« -0.1 Ä‘áº¿n 1.1 (hÆ¡i rá»™ng hÆ¡n khoáº£ng 0-1 thÃ´ng thÆ°á»ng).

---

#### **7.3. Subplot 2: So SÃ¡nh Tham Sá»‘**

```python
# Subplot 2: So sÃ¡nh cÃ¡c tham sá»‘
categories = ['Há»‡ sá»‘ gÃ³c\n(m/coef)', 'Há»‡ sá»‘ cháº·n\n(b/intercept)', f'XÃ¡c suáº¥t Ä‘áº­u\n({hours_input}h)']
manual_values = [m, b, predicted_score_manual]
sklearn_values = [model.coef_[0][0], model.intercept_[0], predicted_proba_sklearn]

x_pos = np.arange(len(categories))
width = 0.35

bars1 = ax2.bar(x_pos - width/2, manual_values, width, label='MÃ´ hÃ¬nh tá»± xÃ¢y dá»±ng', color='skyblue', edgecolor='black', linewidth=1.5)
bars2 = ax2.bar(x_pos + width/2, sklearn_values, width, label='Sklearn', color='salmon', edgecolor='black', linewidth=1.5)
```

**Giáº£i thÃ­ch:**

##### **Chuáº©n bá»‹ dá»¯ liá»‡u**

Thiáº¿t láº­p **Categories:**

```python
categories = ['Há»‡ sá»‘ gÃ³c\n(m/coef)', 'Há»‡ sá»‘ cháº·n\n(b/intercept)', f'XÃ¡c suáº¥t Ä‘áº­u\n({hours_input}h)']
```

List nÃ y chá»©a 3 nhÃ³m so sÃ¡nh, vá»›i `\n` Ä‘á»ƒ xuá»‘ng dÃ²ng trong label cho Ä‘áº¹p hÆ¡n.

Thiáº¿t láº­p **Values:**

```python
manual_values = [m, b, predicted_score_manual]
sklearn_values = [model.coef_[0][0], model.intercept_[0], predicted_proba_sklearn]
```

Hai list nÃ y chá»©a giÃ¡ trá»‹ tÆ°Æ¡ng á»©ng cá»§a tá»«ng mÃ´ hÃ¬nh.

##### **Táº¡o vá»‹ trÃ­ cá»™t**

```python
x_pos = np.arange(len(categories))  # [0, 1, 2]
width = 0.35
```

Biáº¿n `x_pos` lÃ  vá»‹ trÃ­ trung tÃ¢m má»—i nhÃ³m, cÃ²n `width` lÃ  Ä‘á»™ rá»™ng má»—i cá»™t báº±ng 0.35.

##### **Váº½ grouped bar chart**

```python
bars1 = ax2.bar(x_pos - width/2, manual_values, width, ...)
bars2 = ax2.bar(x_pos + width/2, sklearn_values, width, ...)
```

PhÃ¢n tÃ­ch: biá»ƒu thá»©c `x_pos - width/2` dá»‹ch sang trÃ¡i ná»­a width, cho vá»‹ trÃ­ [-0.175, 0.825, 1.825]. Biá»ƒu thá»©c `x_pos + width/2` dá»‹ch sang pháº£i ná»­a width, cho vá»‹ trÃ­ [0.175, 1.175, 2.175]. Káº¿t quáº£ lÃ  2 cá»™t Ä‘á»©ng sÃ¡t nhau, táº¡o nhÃ³m. Vá» mÃ u sáº¯c, Manual dÃ¹ng `'skyblue'` (xanh nháº¡t), Sklearn dÃ¹ng `'salmon'` (Ä‘á» nháº¡t), cáº£ hai Ä‘á»u cÃ³ viá»n Ä‘en cho rÃµ rÃ ng.

---

##### **ThÃªm giÃ¡ trá»‹ lÃªn cá»™t**

```python
# ThÃªm giÃ¡ trá»‹ lÃªn cÃ¡c cá»™t
for bars in [bars1, bars2]:
    for bar in bars:
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.3f}',
                ha='center', va='bottom', fontsize=9, fontweight='bold')
```

**Giáº£i thÃ­ch:**

VÃ²ng láº·p duyá»‡t qua 2 nhÃ³m cá»™t (bars1, bars2), sau Ä‘Ã³ duyá»‡t tá»«ng cá»™t trong nhÃ³m. Láº¥y chiá»u cao báº±ng `bar.get_height()` Ä‘á»ƒ Ä‘áº¡i diá»‡n cho giÃ¡ trá»‹. Tiáº¿p theo váº½ text vá»›i cÃ¡c thiáº¿t láº­p: **Vá»‹ trÃ­ X** lÃ  trung tÃ¢m cá»™t Ä‘Æ°á»£c tÃ­nh báº±ng `bar.get_x()` (tá»a Ä‘á»™ trÃ¡i cá»™t) cá»™ng `bar.get_width()/2.` (ná»­a width). **Vá»‹ trÃ­ Y** lÃ  `height` (Ä‘á»‰nh cá»™t). **Text** lÃ  giÃ¡ trá»‹ vá»›i 3 chá»¯ sá»‘ tháº­p phÃ¢n. CÃ¡c tham sá»‘ `ha='center'` lÃ  Horizontal alignment = center, vÃ  `va='bottom'` lÃ  Vertical alignment = bottom (Ä‘áº·t dÆ°á»›i chá»¯). Káº¿t quáº£ lÃ  má»—i cá»™t cÃ³ sá»‘ á»Ÿ trÃªn Ä‘á»‰nh.

---

##### **Trang trÃ­ subplot 2**

```python
ax2.set_ylabel('GiÃ¡ trá»‹', fontsize=11, fontweight='bold')
ax2.set_title('So sÃ¡nh Tham sá»‘ vÃ  Káº¿t quáº£ Dá»± Ä‘oÃ¡n', fontsize=12, fontweight='bold')
ax2.set_xticks(x_pos)
ax2.set_xticklabels(categories, fontsize=10)
ax2.legend(fontsize=9)
ax2.grid(True, alpha=0.3, axis='y')
ax2.axhline(y=0, color='black', linewidth=0.8)
```

**Giáº£i thÃ­ch:**

HÃ m `set_xticks(x_pos)` Ä‘áº·t vá»‹ trÃ­ tick báº±ng [0, 1, 2]. HÃ m `set_xticklabels(categories)` gáº¯n nhÃ£n cho tick. Tham sá»‘ `grid(axis='y')` chá»‰ váº½ grid ngang (khÃ´ng dá»c). HÃ m `axhline(y=0)` váº½ trá»¥c x táº¡i y=0 (baseline).

---

#### **7.4. LÆ°u vÃ  Hiá»ƒn Thá»‹**

```python
plt.tight_layout()
plt.savefig('results/ex2b_comparison_chart.png', dpi=300, bbox_inches='tight')
plt.show()
```

**Giáº£i thÃ­ch:**

-   LÆ°u vÃ o thÆ° má»¥c `results/`
-   TÃªn file rÃµ rÃ ng: `ex2b_comparison_chart.png`
-   DPI cao (300) cho cháº¥t lÆ°á»£ng tá»‘t

---

## ğŸ“Š Output vÃ  Káº¿t Quáº£ (Dá»± Kiáº¿n)

### ğŸ–¥ï¸ Console Output

```
============================================================
BÃ€I 2 - QUESTION B: SO SÃNH MÃ” HÃŒNH Tá»° XÃ‚Y Dá»°NG Vá»šI SKLEARN
============================================================

------------------------------------------------------------
PHáº¦N A: Káº¾T QUáº¢ MÃ” HÃŒNH Tá»° XÃ‚Y Dá»°NG
------------------------------------------------------------
Tham sá»‘ há»c Ä‘Æ°á»£c:
  - Há»‡ sá»‘ gÃ³c (m): 2.345678
  - Há»‡ sá»‘ cháº·n (b): -4.567890

Dá»± Ä‘oÃ¡n cho sinh viÃªn há»c 2.8 giá»:
  - XÃ¡c suáº¥t Ä‘áº­u: 0.785432 (78.54%)
  - Káº¿t luáº­n: Äáº¬U

------------------------------------------------------------
PHáº¦N B: Káº¾T QUáº¢ MÃ” HÃŒNH SKLEARN
------------------------------------------------------------
Tham sá»‘ há»c Ä‘Æ°á»£c:
  - Há»‡ sá»‘ gÃ³c (coef): 2.398765
  - Há»‡ sá»‘ cháº·n (intercept): -4.612345

Dá»± Ä‘oÃ¡n cho sinh viÃªn há»c 2.8 giá»:
  - XÃ¡c suáº¥t Ä‘áº­u: 0.791234 (79.12%)
  - Káº¿t luáº­n: Äáº¬U

Há»‡ sá»‘ gÃ³c (m/coef):
  - MÃ´ hÃ¬nh tá»± xÃ¢y dá»±ng: 2.345678
  - Sklearn:              2.398765
  - ChÃªnh lá»‡ch:           0.053087

Há»‡ sá»‘ cháº·n (b/intercept):
  - MÃ´ hÃ¬nh tá»± xÃ¢y dá»±ng: -4.567890
  - Sklearn:              -4.612345
  - ChÃªnh lá»‡ch:           0.044455

XÃ¡c suáº¥t Ä‘áº­u cho 2.8 giá» há»c:
  - MÃ´ hÃ¬nh tá»± xÃ¢y dá»±ng: 0.785432 (78.54%)
  - Sklearn:              0.791234 (79.12%)
  - ChÃªnh lá»‡ch:           0.005802

Káº¿t luáº­n dá»± Ä‘oÃ¡n:
  - MÃ´ hÃ¬nh tá»± xÃ¢y dá»±ng: Äáº¬U
  - Sklearn:             Äáº¬U
  - Káº¿t quáº£: GIá»NG NHAU âœ“

============================================================
Káº¾T LUáº¬N
============================================================
CÃ³ thá»ƒ tháº¥y sá»± khÃ¡c biá»‡t giá»¯a hai mÃ´ hÃ¬nh do:
  1. Sá»‘ láº§n láº·p khÃ¡c nhau (10 vs thuáº­t toÃ¡n tá»‘i Æ°u cá»§a sklearn)
  2. PhÆ°Æ¡ng phÃ¡p tá»‘i Æ°u khÃ¡c nhau (Gradient Descent vs LBFGS)
  3. Äiá»u kiá»‡n dá»«ng vÃ  khá»Ÿi táº¡o tham sá»‘ khÃ¡c nhau
============================================================
```

---

### ğŸ“ˆ PhÃ¢n TÃ­ch Káº¿t Quáº£

#### **1. Tham Sá»‘ (m/coef, b/intercept)**

**ChÃªnh lá»‡ch nhá» (~5-6%):**

Káº¿t quáº£ cho tháº¥y Manual cÃ³ m=2.35, b=-4.57 trong khi Sklearn cÃ³ m=2.40, b=-4.61. NguyÃªn nhÃ¢n cá»§a sá»± khÃ¡c biá»‡t nÃ y lÃ  LBFGS há»™i tá»¥ tá»‘t hÆ¡n GD, Sklearn cÃ³ thá»ƒ Ä‘Ã£ há»™i tá»¥ sá»›m, vÃ  khá»Ÿi táº¡o khÃ¡c nhau.

**Ã nghÄ©a:**

Cáº£ hai mÃ´ hÃ¬nh Ä‘á»u há»c Ä‘Æ°á»£c xu hÆ°á»›ng tÄƒng (**CÃ¹ng hÆ°á»›ng:** m > 0). Cáº£ hai Ä‘á»u cÃ³ b Ã¢m (**CÃ¹ng dá»‹ch:** dá»‹ch sang pháº£i). Äiá»u nÃ y chá»©ng tá» **Consistency:** Implementation cÆ¡ báº£n Ä‘Ãºng!

---

#### **2. XÃ¡c Suáº¥t Dá»± ÄoÃ¡n**

**ChÃªnh lá»‡ch ráº¥t nhá» (<1%):**

Manual cho 78.54% trong khi Sklearn cho 79.12%, chÃªnh lá»‡ch chá»‰ 0.58%.

**Ã nghÄ©a:**

Hai mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n tÆ°Æ¡ng tá»± (**Gáº§n nhÆ° giá»‘ng nhau**). Cáº£ hai Ä‘á»u dá»± Ä‘oÃ¡n Äáº¬U (**CÃ¹ng káº¿t luáº­n**). Äiá»u nÃ y chá»©ng tá» Implementation manual lÃ  Ä‘Ã¡ng tin cáº­y (**Tin cáº­y**).

---

#### **3. Káº¿t Luáº­n Cuá»‘i CÃ¹ng**

**GIá»NG NHAU âœ“**

Cáº£ 2 Ä‘á»u káº¿t luáº­n Äáº¬U. Quan trá»ng nháº¥t lÃ  quyáº¿t Ä‘á»‹nh cuá»‘i cÃ¹ng giá»‘ng nhau. ÄÃ¢y lÃ  thÃ nh cÃ´ng: MÃ´ hÃ¬nh tá»± xÃ¢y dá»±ng hoáº¡t Ä‘á»™ng Ä‘Ãºng!

---

#### **4. Biá»ƒu Äá»“ 1: Decision Boundary**

**Quan sÃ¡t:**

Hai Ä‘Æ°á»ng sigmoid gáº§n nhÆ° trÃ¹ng nhau. CÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u phÃ¢n bá»‘ rÃµ rÃ ng (Ä‘á» bÃªn trÃ¡i, xanh bÃªn pháº£i). Hai ngÃ´i sao á»Ÿ 2.8h ráº¥t gáº§n nhau (~79%). NgÆ°á»¡ng 0.5 cho tháº¥y 2 Ä‘Æ°á»ng Ä‘á»u vÆ°á»£t qua táº¡i khoáº£ng 2.0-2.2 giá».

**Káº¿t luáº­n:**

MÃ´ hÃ¬nh manual hoáº¡t Ä‘á»™ng tá»‘t vÃ  decision boundary há»£p lÃ½.

---

#### **5. Biá»ƒu Äá»“ 2: So SÃ¡nh Tham Sá»‘**

**Quan sÃ¡t:**

**NhÃ³m 1 - Há»‡ sá»‘ gÃ³c:** Manual ~2.35 (xanh) vÃ  Sklearn ~2.40 (Ä‘á») cÃ³ chiá»u cao gáº§n báº±ng nhau.

**NhÃ³m 2 - Há»‡ sá»‘ cháº·n:** Manual ~-4.57 (xanh, Ã¢m) vÃ  Sklearn ~-4.61 (Ä‘á», Ã¢m), cáº£ 2 Ä‘á»u Ã¢m vÃ  gáº§n nhau.

**NhÃ³m 3 - XÃ¡c suáº¥t Ä‘áº­u:** Manual ~0.785 (xanh) vÃ  Sklearn ~0.791 (Ä‘á») gáº§n nhÆ° báº±ng nhau.

**Káº¿t luáº­n:**

Máº¯t thÆ°á»ng tháº¥y 2 cá»™t gáº§n nhau (Visual confirmation). ChÃªnh lá»‡ch nhá» lÃ  cháº¥p nháº­n Ä‘Æ°á»£c (Acceptable difference).

---
