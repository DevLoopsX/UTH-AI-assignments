import math
from sklearn.linear_model import LogisticRegression
import numpy as np
import matplotlib.pyplot as plt

# ========== D·ªÆ LI·ªÜU ==========
dataset = [
    [0.5, 0],  # 0.5 gi·ªù h·ªçc ‚Üí R·ªõt
    [1.0, 0],  # 1.0 gi·ªù h·ªçc ‚Üí R·ªõt
    [1.5, 0],  # 1.5 gi·ªù h·ªçc ‚Üí R·ªõt
    [2.0, 0],  # 2.0 gi·ªù h·ªçc ‚Üí R·ªõt
    [2.5, 1],  # 2.5 gi·ªù h·ªçc ‚Üí ƒê·∫≠u
    [3.0, 1],  # 3.0 gi·ªù h·ªçc ‚Üí ƒê·∫≠u
    [3.5, 1],  # 3.5 gi·ªù h·ªçc ‚Üí ƒê·∫≠u
    [4.0, 1]   # 4.0 gi·ªù h·ªçc ‚Üí ƒê·∫≠u
]

# ========== C√ÅC H√ÄM CHO M√î H√åNH T·ª∞ X√ÇY D·ª∞NG ==========

def get_prediction(m, b, x):
    z = m * x + b
    return 1 / (1 + math.exp(-z))

def get_cost(y, y_hat):
    n = len(y)
    total_cost = 0.0
    for yi, y_hat_i in zip(y, y_hat):
        total_cost += -(yi * math.log(y_hat_i) + (1 - yi) * math.log(1 - y_hat_i))
    return total_cost / n

def get_gradients(m, b, x, y, y_hat):
    n = len(y)
    dm = (1 / n) * sum((y_hat_i - yi) * xi for y_hat_i, yi, xi in zip(y_hat, y, x))
    db = (1 / n) * sum(y_hat_i - yi for y_hat_i, yi in zip(y_hat, y))
    return dm, db

# ========== PH·∫¶N A: M√î H√åNH T·ª∞ X√ÇY D·ª∞NG ==========

# Kh·ªüi t·∫°o tham s·ªë
m = 1.0
b = -1.0

# Si√™u tham s·ªë
iterations = 10
learning_rate = 1.0

# T√°ch d·ªØ li·ªáu
x = [row[0] for row in dataset]  # S·ªë gi·ªù h·ªçc
y = [row[1] for row in dataset]  # K·∫øt qu·∫£ (0: r·ªõt, 1: ƒë·∫≠u)

# Hu·∫•n luy·ªán m√¥ h√¨nh b·∫±ng Gradient Descent
for it in range(iterations):
    # Forward pass: T√≠nh d·ª± ƒëo√°n
    y_hat = [get_prediction(m, b, xi) for xi in x]

    # T√≠nh cost (kh√¥ng b·∫Øt bu·ªôc cho hu·∫•n luy·ªán, ch·ªâ ƒë·ªÉ theo d√µi)
    cost = get_cost(y, y_hat)

    # Backward pass: T√≠nh gradient
    dm, db = get_gradients(m, b, x, y, y_hat)

    # Update parameters
    m -= learning_rate * dm
    b -= learning_rate * db

# D·ª± ƒëo√°n cho sinh vi√™n h·ªçc 2.8 gi·ªù
hours_input = 2.8
predicted_score_manual = get_prediction(m, b, hours_input)

print("\n" + "="*60)
print("B√ÄI 2 - QUESTION B: SO S√ÅNH M√î H√åNH T·ª∞ X√ÇY D·ª∞NG V·ªöI SKLEARN")
print("="*60)

print("\n" + "-"*60)
print("PH·∫¶N A: K·∫æT QU·∫¢ M√î H√åNH T·ª∞ X√ÇY D·ª∞NG")
print("-"*60)
print(f"Tham s·ªë h·ªçc ƒë∆∞·ª£c:")
print(f"  - H·ªá s·ªë g√≥c (m): {m:.6f}")
print(f"  - H·ªá s·ªë ch·∫∑n (b): {b:.6f}")
print(f"\nD·ª± ƒëo√°n cho sinh vi√™n h·ªçc {hours_input} gi·ªù:")
print(f"  - X√°c su·∫•t ƒë·∫≠u: {predicted_score_manual:.6f} ({predicted_score_manual*100:.2f}%)")
if predicted_score_manual >= 0.5:
    print(f"  - K·∫øt lu·∫≠n: ƒê·∫¨U")
else:
    print(f"  - K·∫øt lu·∫≠n: R·ªöT")

# ========== PH·∫¶N B: M√î H√åNH SKLEARN ==========

print("\n" + "-"*60)
print("PH·∫¶N B: K·∫æT QU·∫¢ M√î H√åNH SKLEARN")
print("-"*60)

# Chu·∫©n b·ªã d·ªØ li·ªáu cho sklearn
X = np.array([[row[0]] for row in dataset])  # Features (Hours)
y_train = np.array([row[1] for row in dataset])  # Labels (Pass)

# T·∫°o v√† hu·∫•n luy·ªán m√¥ h√¨nh Logistic Regression
model = LogisticRegression(max_iter=10, solver='lbfgs', random_state=42)
model.fit(X, y_train)

# D·ª± ƒëo√°n v·ªõi sklearn
X_test = np.array([[hours_input]])
predicted_proba_sklearn = model.predict_proba(X_test)[0][1]  # X√°c su·∫•t cho class 1 (Pass)
predicted_class_sklearn = model.predict(X_test)[0]

print(f"Tham s·ªë h·ªçc ƒë∆∞·ª£c:")
print(f"  - H·ªá s·ªë g√≥c (coef): {model.coef_[0][0]:.6f}")
print(f"  - H·ªá s·ªë ch·∫∑n (intercept): {model.intercept_[0]:.6f}")
print(f"\nD·ª± ƒëo√°n cho sinh vi√™n h·ªçc {hours_input} gi·ªù:")
print(f"  - X√°c su·∫•t ƒë·∫≠u: {predicted_proba_sklearn:.6f} ({predicted_proba_sklearn*100:.2f}%)")
if predicted_class_sklearn == 1:
    print(f"  - K·∫øt lu·∫≠n: ƒê·∫¨U")
else:
    print(f"  - K·∫øt lu·∫≠n: R·ªöT")

# ========== SO S√ÅNH K·∫æT QU·∫¢ ==========

print(f"\nH·ªá s·ªë g√≥c (m/coef):")
print(f"  - M√¥ h√¨nh t·ª± x√¢y d·ª±ng: {m:.6f}")
print(f"  - Sklearn:              {model.coef_[0][0]:.6f}")
print(f"  - Ch√™nh l·ªách:           {abs(m - model.coef_[0][0]):.6f}")

print(f"\nH·ªá s·ªë ch·∫∑n (b/intercept):")
print(f"  - M√¥ h√¨nh t·ª± x√¢y d·ª±ng: {b:.6f}")
print(f"  - Sklearn:              {model.intercept_[0]:.6f}")
print(f"  - Ch√™nh l·ªách:           {abs(b - model.intercept_[0]):.6f}")

print(f"\nX√°c su·∫•t ƒë·∫≠u cho {hours_input} gi·ªù h·ªçc:")
print(f"  - M√¥ h√¨nh t·ª± x√¢y d·ª±ng: {predicted_score_manual:.6f} ({predicted_score_manual*100:.2f}%)")
print(f"  - Sklearn:              {predicted_proba_sklearn:.6f} ({predicted_proba_sklearn*100:.2f}%)")
print(f"  - Ch√™nh l·ªách:           {abs(predicted_score_manual - predicted_proba_sklearn):.6f}")

print(f"\nK·∫øt lu·∫≠n d·ª± ƒëo√°n:")
result_manual = "ƒê·∫¨U" if predicted_score_manual >= 0.5 else "R·ªöT"
result_sklearn = "ƒê·∫¨U" if predicted_class_sklearn == 1 else "R·ªöT"
print(f"  - M√¥ h√¨nh t·ª± x√¢y d·ª±ng: {result_manual}")
print(f"  - Sklearn:             {result_sklearn}")
if result_manual == result_sklearn:
    print(f"  - K·∫øt qu·∫£: GI·ªêNG NHAU ‚úì")
else:
    print(f"  - K·∫øt qu·∫£: KH√ÅC NHAU ‚úó")

print("\n" + "="*60)
print("K·∫æT LU·∫¨N")
print("="*60)
print("C√≥ th·ªÉ th·∫•y s·ª± kh√°c bi·ªát gi·ªØa hai m√¥ h√¨nh do:")
print("  1. S·ªë l·∫ßn l·∫∑p kh√°c nhau (10 vs thu·∫≠t to√°n t·ªëi ∆∞u c·ªßa sklearn)")
print("  2. Ph∆∞∆°ng ph√°p t·ªëi ∆∞u kh√°c nhau (Gradient Descent vs LBFGS)")
print("  3. ƒêi·ªÅu ki·ªán d·ª´ng v√† kh·ªüi t·∫°o tham s·ªë kh√°c nhau")
print("="*60 + "\n")

# ========== VISUALIZATION ==========

# T·∫°o figure v·ªõi 2 subplots
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

# Subplot 1: So s√°nh decision boundary c·ªßa 2 m√¥ h√¨nh
x_plot = np.linspace(0, 4.5, 100)
y_manual = [get_prediction(m, b, x_i) for x_i in x_plot]
y_sklearn = [model.predict_proba([[x_i]])[0][1] for x_i in x_plot]

# V·∫Ω d·ªØ li·ªáu g·ªëc
x_data = [row[0] for row in dataset]
y_data = [row[1] for row in dataset]
ax1.scatter(x_data, y_data, c=['red' if y==0 else 'green' for y in y_data],
            s=100, alpha=0.6, edgecolors='black', linewidth=1.5,
            label='D·ªØ li·ªáu th·ª±c t·∫ø', zorder=3)

# V·∫Ω ƒë∆∞·ªùng sigmoid
ax1.plot(x_plot, y_manual, 'b-', linewidth=2, label='M√¥ h√¨nh t·ª± x√¢y d·ª±ng')
ax1.plot(x_plot, y_sklearn, 'r--', linewidth=2, label='Sklearn')

# V·∫Ω ng∆∞·ª°ng 0.5
ax1.axhline(y=0.5, color='gray', linestyle=':', linewidth=1, label='Ng∆∞·ª°ng 0.5')

# V·∫Ω ƒëi·ªÉm d·ª± ƒëo√°n cho 2.8 gi·ªù
ax1.scatter([hours_input], [predicted_score_manual], c='blue', s=200,
            marker='*', edgecolors='black', linewidth=1.5,
            label=f'D·ª± ƒëo√°n {hours_input}h (Manual)', zorder=4)
ax1.scatter([hours_input], [predicted_proba_sklearn], c='red', s=200,
            marker='*', edgecolors='black', linewidth=1.5,
            label=f'D·ª± ƒëo√°n {hours_input}h (Sklearn)', zorder=4)

ax1.set_xlabel('S·ªë gi·ªù h·ªçc', fontsize=11, fontweight='bold')
ax1.set_ylabel('X√°c su·∫•t ƒë·∫≠u', fontsize=11, fontweight='bold')
ax1.set_title('So s√°nh Decision Boundary\nM√¥ h√¨nh t·ª± x√¢y d·ª±ng vs Sklearn', fontsize=12, fontweight='bold')
ax1.legend(loc='upper left', fontsize=9)
ax1.grid(True, alpha=0.3)
ax1.set_ylim(-0.1, 1.1)

# Subplot 2: So s√°nh c√°c tham s·ªë
categories = ['H·ªá s·ªë g√≥c\n(m/coef)', 'H·ªá s·ªë ch·∫∑n\n(b/intercept)', f'X√°c su·∫•t ƒë·∫≠u\n({hours_input}h)']
manual_values = [m, b, predicted_score_manual]
sklearn_values = [model.coef_[0][0], model.intercept_[0], predicted_proba_sklearn]

x_pos = np.arange(len(categories))
width = 0.35

bars1 = ax2.bar(x_pos - width/2, manual_values, width, label='M√¥ h√¨nh t·ª± x√¢y d·ª±ng', color='skyblue', edgecolor='black', linewidth=1.5)
bars2 = ax2.bar(x_pos + width/2, sklearn_values, width, label='Sklearn', color='salmon', edgecolor='black', linewidth=1.5)

# Th√™m gi√° tr·ªã l√™n c√°c c·ªôt
for bars in [bars1, bars2]:
    for bar in bars:
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.3f}',
                ha='center', va='bottom', fontsize=9, fontweight='bold')

ax2.set_ylabel('Gi√° tr·ªã', fontsize=11, fontweight='bold')
ax2.set_title('So s√°nh Tham s·ªë v√† K·∫øt qu·∫£ D·ª± ƒëo√°n', fontsize=12, fontweight='bold')
ax2.set_xticks(x_pos)
ax2.set_xticklabels(categories, fontsize=10)
ax2.legend(fontsize=9)
ax2.grid(True, alpha=0.3, axis='y')
ax2.axhline(y=0, color='black', linewidth=0.8)

plt.tight_layout()
plt.savefig('results/ex2b_comparison_chart.png', dpi=300, bbox_inches='tight')
print("üìä Bi·ªÉu ƒë·ªì ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o 'results/ex2b_comparison_chart.png'")
plt.show()
