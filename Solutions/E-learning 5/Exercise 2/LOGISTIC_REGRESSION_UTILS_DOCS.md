# ğŸ“š TÃ i Liá»‡u Module `logistic_regression_utils.py`

## ğŸ¯ Má»¥c ÄÃ­ch

File `logistic_regression_utils.py` lÃ  module tiá»‡n Ã­ch chá»©a cÃ¡c hÃ m vÃ  dá»¯ liá»‡u dÃ¹ng chung cho cáº£ Question A vÃ  Question B trong Exercise 2. Module nÃ y cung cáº¥p implementation hoÃ n chá»‰nh cá»§a thuáº­t toÃ¡n Logistic Regression tá»« Ä‘áº§u (from scratch), khÃ´ng sá»­ dá»¥ng thÆ° viá»‡n Machine Learning cÃ³ sáºµn.

---

## ğŸ“Š Dataset

```python
DATASET = [
    [0.5, 0],
    [1.0, 0],
    [1.5, 0],
    [2.0, 0],
    [2.5, 1],
    [3.0, 1],
    [3.5, 1],
    [4.0, 1]
]
```

**Giáº£i thÃ­ch:**

Dataset nÃ y biá»ƒu diá»…n má»‘i quan há»‡ giá»¯a sá»‘ giá» há»c (cá»™t 1) vÃ  káº¿t quáº£ Ä‘áº­u/rá»›t (cá»™t 2) cá»§a 8 sinh viÃªn. Má»—i dÃ²ng trong dataset lÃ  má»™t máº£ng gá»“m 2 pháº§n tá»­: pháº§n tá»­ thá»© nháº¥t `[row[0]]` lÃ  sá»‘ giá» há»c (feature/Ä‘áº·c trÆ°ng), pháº§n tá»­ thá»© hai `[row[1]]` lÃ  nhÃ£n káº¿t quáº£ vá»›i 0 = rá»›t vÃ  1 = Ä‘áº­u.

**Äáº·c Ä‘iá»ƒm cá»§a dataset:**

Dataset cÃ³ tÃ­nh cÃ¢n báº±ng vá»›i 4 máº«u rá»›t (0.5-2.0 giá») vÃ  4 máº«u Ä‘áº­u (2.5-4.0 giá»). CÃ³ ranh giá»›i rÃµ rÃ ng giá»¯a hai lá»›p, táº¡o khoáº£ng cÃ¡ch giá»¯a 2.0 vÃ  2.5 giá», giÃºp mÃ´ hÃ¬nh dá»… dÃ ng há»c Ä‘Æ°á»£c pattern. Dataset phÃ¹ há»£p cho binary classification, Ä‘á»§ Ä‘Æ¡n giáº£n Ä‘á»ƒ minh há»a nhÆ°ng váº«n thá»ƒ hiá»‡n Ä‘Æ°á»£c Ä‘áº§y Ä‘á»§ cÆ¡ cháº¿ hoáº¡t Ä‘á»™ng cá»§a Logistic Regression.

---

## ğŸ”§ CÃ¡c HÃ m Trong Module

### 1ï¸âƒ£ HÃ m `get_prediction(m, b, x)`

```python
def get_prediction(m, b, x):
    # Sigmoid function
    y = m * x + b
    return 1 / (1 + math.exp(-y))
```

**Má»¥c Ä‘Ã­ch:**

HÃ m nÃ y thá»±c hiá»‡n forward propagation, tÃ­nh toÃ¡n xÃ¡c suáº¥t dá»± Ä‘oÃ¡n cho má»™t giÃ¡ trá»‹ Ä‘áº§u vÃ o x dá»±a trÃªn tham sá»‘ m vÃ  b.

**Tham sá»‘:**

-   **`m`** (float): Há»‡ sá»‘ gÃ³c (slope/weight) cá»§a mÃ´ hÃ¬nh, xÃ¡c Ä‘á»‹nh Ä‘á»™ dá»‘c cá»§a Ä‘Æ°á»ng sigmoid
-   **`b`** (float): Há»‡ sá»‘ cháº·n (bias/intercept), xÃ¡c Ä‘á»‹nh vá»‹ trÃ­ dá»‹ch chuyá»ƒn cá»§a Ä‘Æ°á»ng sigmoid
-   **`x`** (float): GiÃ¡ trá»‹ Ä‘áº§u vÃ o cáº§n dá»± Ä‘oÃ¡n (sá»‘ giá» há»c)

**GiÃ¡ trá»‹ tráº£ vá»:**

HÃ m tráº£ vá» xÃ¡c suáº¥t thuá»™c lá»›p 1 (Ä‘áº­u), lÃ  sá»‘ thá»±c trong khoáº£ng (0, 1).

**CÃ¡ch hoáº¡t Ä‘á»™ng:**

BÆ°á»›c Ä‘áº§u tiÃªn tÃ­nh giÃ¡ trá»‹ tuyáº¿n tÃ­nh (linear combination):

$$z = m \times x + b$$

ÄÃ¢y lÃ  phÆ°Æ¡ng trÃ¬nh Ä‘Æ°á»ng tháº³ng cÆ¡ báº£n, trong Ä‘Ã³ m kiá»ƒm soÃ¡t Ä‘á»™ dá»‘c vÃ  b kiá»ƒm soÃ¡t Ä‘iá»ƒm cáº¯t trá»¥c y.

BÆ°á»›c thá»© hai Ã¡p dá»¥ng hÃ m sigmoid Ä‘á»ƒ chuyá»ƒn Ä‘á»•i z thÃ nh xÃ¡c suáº¥t:

$$\sigma(z) = \frac{1}{1 + e^{-z}}$$

HÃ m sigmoid "nÃ©n" giÃ¡ trá»‹ z (cÃ³ thá»ƒ tá»« Ã¢m vÃ´ cÃ¹ng Ä‘áº¿n dÆ°Æ¡ng vÃ´ cÃ¹ng) vÃ o khoáº£ng (0, 1), phÃ¹ há»£p Ä‘á»ƒ biá»ƒu diá»…n xÃ¡c suáº¥t.

**VÃ­ dá»¥ sá»­ dá»¥ng:**

```python
m, b = 2.0, -4.0
hours = 2.8
probability = get_prediction(m, b, hours)
# z = 2.0 * 2.8 + (-4.0) = 1.6
# sigmoid(1.6) â‰ˆ 0.832
# XÃ¡c suáº¥t Ä‘áº­u â‰ˆ 83.2%
```

**LÆ°u Ã½ ká»¹ thuáº­t:**

HÃ m sá»­ dá»¥ng `math.exp()` tá»« thÆ° viá»‡n built-in Python thay vÃ¬ numpy. Do Ä‘Ã³, hÃ m nÃ y chá»‰ xá»­ lÃ½ Ä‘Æ°á»£c má»™t giÃ¡ trá»‹ scalar táº¡i má»™t thá»i Ä‘iá»ƒm, khÃ´ng thá»ƒ xá»­ lÃ½ array/vector nhÆ° numpy. Náº¿u cáº§n dá»± Ä‘oÃ¡n cho nhiá»u Ä‘iá»ƒm, cáº§n dÃ¹ng vÃ²ng láº·p hoáº·c list comprehension.

---

### 2ï¸âƒ£ HÃ m `get_cost(y, y_hat)`

```python
def get_cost(y, y_hat):
    # Binary cross-entropy
    k = len(y)
    total_cost = 0.0
    for yi, y_hat_i in zip(y, y_hat):
        total_cost += -(yi * math.log(y_hat_i) + (1 - yi) * math.log(1 - y_hat_i))
    return total_cost / k
```

**Má»¥c Ä‘Ã­ch:**

HÃ m nÃ y tÃ­nh toÃ¡n Binary Cross-Entropy Loss, Ä‘o lÆ°á»ng má»©c Ä‘á»™ sai lá»‡ch giá»¯a dá»± Ä‘oÃ¡n vÃ  thá»±c táº¿. GiÃ¡ trá»‹ cost cÃ ng nhá» thÃ¬ mÃ´ hÃ¬nh cÃ ng tá»‘t.

**Tham sá»‘:**

-   **`y`** (list): Danh sÃ¡ch nhÃ£n thá»±c táº¿, má»—i pháº§n tá»­ lÃ  0 hoáº·c 1. VÃ­ dá»¥: `[0, 0, 0, 0, 1, 1, 1, 1]`
-   **`y_hat`** (list): Danh sÃ¡ch xÃ¡c suáº¥t dá»± Ä‘oÃ¡n, má»—i pháº§n tá»­ trong khoáº£ng (0, 1). VÃ­ dá»¥: `[0.1, 0.2, 0.3, 0.4, 0.6, 0.7, 0.8, 0.9]`

**GiÃ¡ trá»‹ tráº£ vá»:**

GiÃ¡ trá»‹ cost trung bÃ¬nh (float), luÃ´n lÃ  sá»‘ dÆ°Æ¡ng. Cost cÃ ng nhá» cÃ ng tá»‘t, vá»›i cost = 0 lÃ  lÃ½ tÆ°á»Ÿng (dá»± Ä‘oÃ¡n hoÃ n háº£o).

**CÃ´ng thá»©c toÃ¡n há»c:**

$$J = \frac{1}{k} \sum_{i=1}^{k} -\left[ y_i \log(h_i) + (1-y_i) \log(1-h_i) \right]$$

Trong Ä‘Ã³:

-   k lÃ  sá»‘ lÆ°á»£ng máº«u
-   yi lÃ  nhÃ£n thá»±c táº¿ (0 hoáº·c 1)
-   hi lÃ  xÃ¡c suáº¥t dá»± Ä‘oÃ¡n

**CÃ¡ch hoáº¡t Ä‘á»™ng:**

HÃ m Ä‘áº¿m sá»‘ lÆ°á»£ng máº«u `k = len(y)`, sau Ä‘Ã³ khá»Ÿi táº¡o biáº¿n tÃ­ch lÅ©y `total_cost = 0.0`. VÃ²ng láº·p duyá»‡t qua tá»«ng cáº·p (yi, y_hat_i) báº±ng `zip()`:

```python
for yi, y_hat_i in zip(y, y_hat):
```

Vá»›i má»—i cáº·p, tÃ­nh loss theo cÃ´ng thá»©c:

$$loss_i = -(y_i \log(h_i) + (1-y_i) \log(1-h_i))$$

**Logic cá»§a cÃ´ng thá»©c:**

Khi yi = 1 (thá»±c táº¿ Ä‘áº­u): cÃ´ng thá»©c rÃºt gá»n thÃ nh:

$$-\log(h_i)$$

Náº¿u hi gáº§n 1 (dá»± Ä‘oÃ¡n Ä‘Ãºng): -log(1) â‰ˆ 0 â†’ cost tháº¥p

Náº¿u hi gáº§n 0 (dá»± Ä‘oÃ¡n sai): -log(0) â†’ +âˆ â†’ cost ráº¥t cao

Khi yi = 0 (thá»±c táº¿ rá»›t): cÃ´ng thá»©c rÃºt gá»n thÃ nh:

$$-\log(1-h_i)$$

Náº¿u hi gáº§n 0 (dá»± Ä‘oÃ¡n Ä‘Ãºng): -log(1) â‰ˆ 0 â†’ cost tháº¥p

Náº¿u hi gáº§n 1 (dá»± Ä‘oÃ¡n sai): -log(0) â†’ +âˆ â†’ cost ráº¥t cao

Cuá»‘i cÃ¹ng, tráº£ vá» trung bÃ¬nh: `total_cost / k`

**âš ï¸ LÆ°u Ã½ quan trá»ng:**

HÃ m nÃ y KHÃ”NG cÃ³ epsilon (giÃ¡ trá»‹ nhá» Ä‘á»ƒ trÃ¡nh log(0)). Trong thá»±c táº¿, Ä‘iá»u nÃ y cÃ³ thá»ƒ gÃ¢y lá»—i náº¿u y_hat chá»©a giÃ¡ trá»‹ 0 hoáº·c 1 chÃ­nh xÃ¡c. NÃªn cáº£i tiáº¿n thÃ nh:

```python
epsilon = 1e-15
total_cost += -(yi * math.log(y_hat_i + epsilon) +
               (1 - yi) * math.log(1 - y_hat_i + epsilon))
```

---

### 3ï¸âƒ£ HÃ m `get_gradients(m, b, x, y, y_hat)`

```python
def get_gradients(m, b, x, y, y_hat):
    # Calculate gradients
    k = len(y)
    dm = (1 / k) * sum((y_hat_i - yi) * xi for y_hat_i, yi, xi in zip(y_hat, y, x))
    db = (1 / k) * sum(y_hat_i - yi for y_hat_i, yi in zip(y_hat, y))
    return dm, db
```

**Má»¥c Ä‘Ã­ch:**

HÃ m nÃ y tÃ­nh Ä‘áº¡o hÃ m (gradient) cá»§a hÃ m cost theo m vÃ  b, chá»‰ ra hÆ°á»›ng vÃ  má»©c Ä‘á»™ cáº§n Ä‘iá»u chá»‰nh tham sá»‘ Ä‘á»ƒ giáº£m cost.

**Tham sá»‘:**

-   **`m, b`** (float): Tham sá»‘ hiá»‡n táº¡i (khÃ´ng thá»±c sá»± sá»­ dá»¥ng trong hÃ m, chá»‰ Ä‘á»ƒ tÆ°Æ¡ng thÃ­ch signature)
-   **`x`** (list): Danh sÃ¡ch giÃ¡ trá»‹ features. VÃ­ dá»¥: `[0.5, 1.0, 1.5, ..., 4.0]`
-   **`y`** (list): Danh sÃ¡ch nhÃ£n thá»±c táº¿. VÃ­ dá»¥: `[0, 0, 0, ..., 1]`
-   **`y_hat`** (list): Danh sÃ¡ch xÃ¡c suáº¥t dá»± Ä‘oÃ¡n. VÃ­ dá»¥: `[h1, h2, ..., h8]`

**GiÃ¡ trá»‹ tráº£ vá»:**

Tuple gá»“m 2 giÃ¡ trá»‹: `(dm, db)` - gradient cá»§a m vÃ  b.

**CÃ´ng thá»©c toÃ¡n há»c:**

Gradient cá»§a m (há»‡ sá»‘ gÃ³c):

$$\frac{\partial J}{\partial m} = \frac{1}{k}\sum_{i=1}^{k}(h_i - y_i) \cdot x_i$$

Gradient cá»§a b (há»‡ sá»‘ cháº·n):

$$\frac{\partial J}{\partial b} = \frac{1}{k}\sum_{i=1}^{k}(h_i - y_i)$$

**CÃ¡ch hoáº¡t Ä‘á»™ng:**

Äá»ƒ tÃ­nh dm, hÃ m sá»­ dá»¥ng generator expression Pythonic:

```python
dm = (1 / k) * sum((y_hat_i - yi) * xi for y_hat_i, yi, xi in zip(y_hat, y, x))
```

Biá»ƒu thá»©c nÃ y duyá»‡t Ä‘á»“ng thá»i qua 3 list (y_hat, y, x), tÃ­nh sai sá»‘ `(y_hat_i - yi)` nhÃ¢n vá»›i feature `xi`, sau Ä‘Ã³ tá»•ng há»£p vÃ  láº¥y trung bÃ¬nh.

TÆ°Æ¡ng tá»± cho db nhÆ°ng khÃ´ng nhÃ¢n vá»›i xi:

```python
db = (1 / k) * sum(y_hat_i - yi for y_hat_i, yi in zip(y_hat, y))
```

**Ã nghÄ©a:**

Gradient dÆ°Æ¡ng: tham sá»‘ cáº§n giáº£m Ä‘á»ƒ giáº£m cost

Gradient Ã¢m: tham sá»‘ cáº§n tÄƒng Ä‘á»ƒ giáº£m cost

Äá»™ lá»›n gradient: cho biáº¿t má»©c Ä‘á»™ cáº§n Ä‘iá»u chá»‰nh

**VÃ­ dá»¥ tÃ­nh toÃ¡n:**

```python
x = [0.5, 1.0, 1.5, 2.0]
y = [0, 0, 0, 1]
y_hat = [0.1, 0.2, 0.3, 0.8]

errors = [0.1, 0.2, 0.3, -0.2]
weighted = [0.1*0.5, 0.2*1.0, 0.3*1.5, -0.2*2.0]
         = [0.05, 0.2, 0.45, -0.4]
sum = 0.3
dm = 0.3 / 4 = 0.075
```

---

### 4ï¸âƒ£ HÃ m `get_accuracy(y, y_hat)`

```python
def get_accuracy(y, y_hat):
    correct_predictions = sum((1 if y_hat_i >= 0.5 else 0) == yi for y_hat_i, yi in zip(y_hat, y))
    return correct_predictions / len(y)
```

**Má»¥c Ä‘Ã­ch:**

HÃ m nÃ y tÃ­nh Ä‘á»™ chÃ­nh xÃ¡c (accuracy) cá»§a mÃ´ hÃ¬nh, lÃ  tá»· lá»‡ pháº§n trÄƒm dá»± Ä‘oÃ¡n Ä‘Ãºng.

**Tham sá»‘:**

-   **`y`** (list): NhÃ£n thá»±c táº¿
-   **`y_hat`** (list): XÃ¡c suáº¥t dá»± Ä‘oÃ¡n

**GiÃ¡ trá»‹ tráº£ vá»:**

Accuracy (float) tá»« 0.0 (0%) Ä‘áº¿n 1.0 (100%).

**CÃ¡ch hoáº¡t Ä‘á»™ng:**

BÆ°á»›c 1: Chuyá»ƒn xÃ¡c suáº¥t thÃ nh nhÃ£n dá»± Ä‘oÃ¡n vá»›i ngÆ°á»¡ng 0.5:

```python
1 if y_hat_i >= 0.5 else 0
```

BÆ°á»›c 2: So sÃ¡nh vá»›i nhÃ£n thá»±c táº¿:

```python
(... == yi)
```

Káº¿t quáº£ lÃ  True (Ä‘Ãºng) hoáº·c False (sai).

BÆ°á»›c 3: Äáº¿m sá»‘ dá»± Ä‘oÃ¡n Ä‘Ãºng báº±ng `sum()`. Python tá»± Ä‘á»™ng chuyá»ƒn True=1, False=0.

BÆ°á»›c 4: TÃ­nh tá»· lá»‡:

$$Accuracy = \frac{\text{S\u1ed1 d\u1ef1 \u0111o\u00e1n \u0111\u00fang}}{\text{T\u1ed5ng s\u1ed1 m\u1eabu}}$$

**VÃ­ dá»¥:**

```python
y = [0, 0, 1, 1, 1]
y_hat = [0.2, 0.6, 0.7, 0.8, 0.3]

# Chuyá»ƒn thÃ nh nhÃ£n:
predictions = [0, 1, 1, 1, 0]

# So sÃ¡nh:
# 0 == 0 âœ“
# 1 == 0 âœ—
# 1 == 1 âœ“
# 1 == 1 âœ“
# 0 == 1 âœ—

accuracy = 3 / 5 = 0.6 = 60%
```

---

### 5ï¸âƒ£ HÃ m `train_logistic_regression(...)`

```python
def train_logistic_regression(dataset=DATASET, m_init=1.0, b_init=-1.0, iterations=10, learning_rate=1.0):
    m = m_init
    b = b_init

    x = [row[0] for row in dataset]
    y = [row[1] for row in dataset]

    costs = []

    for it in range(iterations):
        y_hat = [get_prediction(m, b, xi) for xi in x]

        cost = get_cost(y, y_hat)
        costs.append(cost)

        dm, db = get_gradients(m, b, x, y, y_hat)

        m -= learning_rate * dm
        b -= learning_rate * db

    return m, b, costs
```

**Má»¥c Ä‘Ã­ch:**

ÄÃ¢y lÃ  hÃ m chÃ­nh thá»±c hiá»‡n thuáº­t toÃ¡n Gradient Descent, huáº¥n luyá»‡n mÃ´ hÃ¬nh Logistic Regression tá»« Ä‘áº§u Ä‘áº¿n cuá»‘i.

**Tham sá»‘:**

-   **`dataset`** (list, máº·c Ä‘á»‹nh = DATASET): Dá»¯ liá»‡u training dáº¡ng `[[x1, y1], [x2, y2], ...]`
-   **`m_init`** (float, máº·c Ä‘á»‹nh = 1.0): GiÃ¡ trá»‹ khá»Ÿi táº¡o cho há»‡ sá»‘ gÃ³c
-   **`b_init`** (float, máº·c Ä‘á»‹nh = -1.0): GiÃ¡ trá»‹ khá»Ÿi táº¡o cho há»‡ sá»‘ cháº·n
-   **`iterations`** (int, máº·c Ä‘á»‹nh = 10): Sá»‘ vÃ²ng láº·p training
-   **`learning_rate`** (float, máº·c Ä‘á»‹nh = 1.0): Tá»‘c Ä‘á»™ há»c

**GiÃ¡ trá»‹ tráº£ vá»:**

Tuple gá»“m 3 pháº§n tá»­: `(m, b, costs)`

-   **m**: Há»‡ sá»‘ gÃ³c tá»‘i Æ°u sau training
-   **b**: Há»‡ sá»‘ cháº·n tá»‘i Æ°u sau training
-   **costs**: List chá»©a giÃ¡ trá»‹ cost táº¡i má»—i iteration

**CÃ¡c bÆ°á»›c thá»±c hiá»‡n:**

**BÆ°á»›c 1: Khá»Ÿi táº¡o tham sá»‘**

```python
m = m_init
b = b_init
```

**BÆ°á»›c 2: TÃ¡ch dataset thÃ nh x vÃ  y**

```python
x = [row[0] for row in dataset]
y = [row[1] for row in dataset]
```

Káº¿t quáº£:

```python
x = [0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0]
y = [0, 0, 0, 0, 1, 1, 1, 1]
```

**BÆ°á»›c 3: Khá»Ÿi táº¡o list lÆ°u cost history**

```python
costs = []
```

**BÆ°á»›c 4: VÃ²ng láº·p training (Gradient Descent)**

```python
for it in range(iterations):
```

Trong má»—i iteration:

_4.1. Forward Propagation:_

```python
y_hat = [get_prediction(m, b, xi) for xi in x]
```

TÃ­nh xÃ¡c suáº¥t dá»± Ä‘oÃ¡n cho táº¥t cáº£ Ä‘iá»ƒm.

_4.2. TÃ­nh Cost:_

```python
cost = get_cost(y, y_hat)
costs.append(cost)
```

ÄÃ¡nh giÃ¡ cháº¥t lÆ°á»£ng mÃ´ hÃ¬nh hiá»‡n táº¡i vÃ  lÆ°u vÃ o history.

_4.3. Backward Propagation (tÃ­nh gradient):_

```python
dm, db = get_gradients(m, b, x, y, y_hat)
```

TÃ­nh hÆ°á»›ng vÃ  má»©c Ä‘á»™ cáº§n Ä‘iá»u chá»‰nh.

_4.4. Cáº­p nháº­t tham sá»‘ (Gradient Descent step):_

```python
m -= learning_rate * dm
b -= learning_rate * db
```

CÃ´ng thá»©c:

$$m_{new} = m_{old} - \alpha \frac{\partial J}{\partial m}$$

$$b_{new} = b_{old} - \alpha \frac{\partial J}{\partial b}$$

**BÆ°á»›c 5: Tráº£ vá» káº¿t quáº£**

```python
return m, b, costs
```

**VÃ­ dá»¥ sá»­ dá»¥ng:**

```python
# Training vá»›i default parameters
m, b, costs = train_logistic_regression()

# Training tÃ¹y chá»‰nh
m, b, costs = train_logistic_regression(
    dataset=DATASET,
    m_init=0.0,
    b_init=0.0,
    iterations=50,
    learning_rate=0.5
)

print(f"Optimal m: {m:.4f}")
print(f"Optimal b: {b:.4f}")
print(f"Final cost: {costs[-1]:.4f}")
```

**Quan sÃ¡t cost convergence:**

```python
import matplotlib.pyplot as plt

plt.plot(range(len(costs)), costs)
plt.xlabel('Iteration')
plt.ylabel('Cost')
plt.title('Cost Function Convergence')
plt.show()
```

Cost nÃªn giáº£m dáº§n qua cÃ¡c iteration. Náº¿u cost tÄƒng hoáº·c dao Ä‘á»™ng, cÃ³ thá»ƒ learning rate quÃ¡ lá»›n.

---

## ğŸ“ So SÃ¡nh Vá»›i Exercise 1

| Äáº·c Ä‘iá»ƒm           | Exercise 1    | Exercise 2 (Module nÃ y) |
| ------------------ | ------------- | ----------------------- |
| ThÆ° viá»‡n           | NumPy         | math (built-in)         |
| Style code         | Vectorized    | Loop-based              |
| Dataset size       | 6 máº«u         | 8 máº«u                   |
| Epsilon trong cost | CÃ³ (1e-15)    | KhÃ´ng cÃ³ âš ï¸             |
| Print progress     | KhÃ´ng         | KhÃ´ng                   |
| Return values      | w, b, 3 lists | m, b, 1 list            |
| Tá»‘c Ä‘á»™             | Nhanh hÆ¡n     | Cháº­m hÆ¡n                |

**Æ¯u Ä‘iá»ƒm cá»§a module nÃ y:**

-   KhÃ´ng phá»¥ thuá»™c vÃ o thÆ° viá»‡n ngoÃ i (chá»‰ dÃ¹ng math built-in)
-   Code dá»… hiá»ƒu, tá»«ng bÆ°á»›c rÃµ rÃ ng
-   PhÃ¹ há»£p cho má»¥c Ä‘Ã­ch há»c táº­p

**NhÆ°á»£c Ä‘iá»ƒm:**

-   Thiáº¿u epsilon â†’ cÃ³ thá»ƒ lá»—i vá»›i log(0)
-   Cháº­m hÆ¡n NumPy vá»›i dataset lá»›n
-   Chá»‰ xá»­ lÃ½ Ä‘Æ°á»£c scalar, khÃ´ng xá»­ lÃ½ Ä‘Æ°á»£c batch

---

## ğŸ”„ Luá»“ng Sá»­ Dá»¥ng Äiá»ƒn HÃ¬nh

```python
# 1. Import module
from logistic_regression_utils import (
    DATASET,
    get_prediction,
    train_logistic_regression
)

# 2. Training
m, b, costs = train_logistic_regression(
    dataset=DATASET,
    iterations=10,
    learning_rate=1.0
)

# 3. Prediction
hours_input = 2.8
probability = get_prediction(m, b, hours_input)

# 4. Classification
if probability >= 0.5:
    result = "Äáº¬U"
else:
    result = "Rá»šT"

print(f"XÃ¡c suáº¥t: {probability:.4f} â†’ {result}")
```

---