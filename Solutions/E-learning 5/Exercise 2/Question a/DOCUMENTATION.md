# ๐ E-Learning 5 - Exercise 2 - Question A: Logistic Regression Training

## ๐ฏ Mแปฅc Tiรชu Bรi Tแบญp

Bรi tแบญp yรชu cแบงu **lแบญp trรฌnh vร huแบฅn luyแปn mแปt mรด hรฌnh Logistic Regression** tแปซ ฤแบงu (khรดng dรนng thฦฐ viแปn mรกy hแปc) ฤแป dแปฑ ฤoรกn xรกc suแบฅt sinh viรชn ฤแบญu/rแปt dแปฑa trรชn sแป giแป hแปc.

### ๐ ฤแป Bรi

**Dataset:** Quan hแป giแปฏa thแปi gian tแปฑ hแปc vร kแบฟt quแบฃ ฤแบงu/rแปt cแปงa sinh viรชn

| Hours (Giแป hแปc) | Pass (Kแบฟt quแบฃ) |
| --------------- | -------------- |
| 0.5             | 0 (Rแปt)        |
| 1.0             | 0 (Rแปt)        |
| 1.5             | 0 (Rแปt)        |
| 2.0             | 0 (Rแปt)        |
| 2.5             | 1 (ฤแบญu)        |
| 3.0             | 1 (ฤแบญu)        |
| 3.5             | 1 (ฤแบญu)        |
| 4.0             | 1 (ฤแบญu)        |

**Yรชu cแบงu:**

-   Lแบญp trรฌnh thuแบญt toรกn Logistic Regression vแปi **sแป lแบงn huแบฅn luyแปn n = 10**
-   Sau ฤรณ **dแปฑ ฤoรกn** khi sinh viรชn tแปฑ hแปc **2.8 giแป** thรฌ xรกc suแบฅt ฤแบญu lร bao nhiรชu?
-   Kแบฟt luแบญn: Sinh viรชn sแบฝ **ฤแบญu hay rแปt**?

**Lฦฐu รฝ:** Khรดng ฤฦฐแปฃc sแปญ dแปฅng thฦฐ viแปn sklearn (chแป ฤฦฐแปฃc dรนng thฦฐ viแปn cฦก bแบฃn)

---

## ๐ป Phรขn Tรญch Source Code Chi Tiแบฟt

### 1๏ธโฃ Import Module vร Cแบฅu Trรบc Project

```python
import sys
import os

# Thรชm thฦฐ mแปฅc cha vรo path ฤแป import module
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from logistic_regression_utils import (
    DATASET, get_prediction, train_logistic_regression
)
```

**Giแบฃi thรญch:**

#### **Import sys vร os**

-   **`sys`:** Module hแป thแปng Python, dรนng ฤแป thao tรกc vแปi mรดi trฦฐแปng runtime
-   **`os`:** Module hแป ฤiแปu hรnh, dรนng ฤแป lรm viแปc vแปi file vร thฦฐ mแปฅc

#### **Thรชm ฤฦฐแปng dแบซn module**

```python
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
```

**Phรขn tรญch tแปซng bฦฐแปc:**

1. **`__file__`:** Biแบฟn ฤแบทc biแปt chแปฉa ฤฦฐแปng dแบซn file Python hiแปn tแบกi

    - Vรญ dแปฅ: `d:/UTH-AI-assignments/.../Exercise 2/Question a/ex2a_logistic_regression.py`

2. **`os.path.abspath(__file__)`:** Chuyแปn thรnh ฤฦฐแปng dแบซn tuyแปt ฤแปi

    - ฤแบฃm bแบฃo ฤฦฐแปng dแบซn ฤแบงy ฤแปง, khรดng phแปฅ thuแปc thฦฐ mแปฅc lรm viแปc

3. **`os.path.dirname(...)`:** Lแบฅy thฦฐ mแปฅc cha (lแบงn 1)

    - Kแบฟt quแบฃ: `d:/UTH-AI-assignments/.../Exercise 2/Question a/`

4. **`os.path.dirname(...)` (lแบงn 2):** Lแบฅy thฦฐ mแปฅc cha tiแบฟp (lแบงn 2)

    - Kแบฟt quแบฃ: `d:/UTH-AI-assignments/.../Exercise 2/`

5. **`sys.path.insert(0, ...)`:** Thรชm ฤฦฐแปng dแบซn vรo **ฤแบงu** danh sรกch tรฌm kiแบฟm module
    - `0`: Vแป trรญ ฤแบงu tiรชn (ฦฐu tiรชn cao nhแบฅt)
    - Cho phรฉp Python tรฌm thแบฅy file `logistic_regression_utils.py` แป thฦฐ mแปฅc cha

**Tแบกi sao cแบงn lรm thแบฟ nรy?**

-   File `logistic_regression_utils.py` nแบฑm แป thฦฐ mแปฅc `Exercise 2/` (cha)
-   File hiแปn tแบกi nแบฑm แป `Exercise 2/Question a/` (con)
-   Python mแบทc ฤแปnh chแป tรฌm module trong thฦฐ mแปฅc hiแปn tแบกi
-   Phแบฃi thรชm ฤฦฐแปng dแบซn thแปง cรดng ฤแป import tแปซ thฦฐ mแปฅc cha

**Cแบฅu trรบc thฦฐ mแปฅc:**

```
Exercise 2/
โโโ logistic_regression_utils.py  โ Module chแปฉa cรกc hรm
โโโ Question a/
โ   โโโ ex2a_logistic_regression.py  โ File nรy
โโโ Question b/
    โโโ ex2b_logistic_regression.py
```

#### **Import cรกc hรm tแปซ module**

```python
from logistic_regression_utils import (
    DATASET, get_prediction, train_logistic_regression
)
```

**Giแบฃi thรญch:**

-   **`DATASET`:** Hแบฑng sแป chแปฉa dแปฏ liแปu training
-   **`get_prediction`:** Hรm dแปฑ ฤoรกn xรกc suแบฅt cho input mแปi
-   **`train_logistic_regression`:** Hรm huแบฅn luyแปn mรด hรฌnh

**Lแปฃi รญch cแปงa cรกch tแป chแปฉc nรy:**

-   โ **Tรกi sแปญ dแปฅng code:** Cรกc hรm dรนng chung cho Question a vร b
-   โ **Dแป bแบฃo trรฌ:** Sแปญa logic แป 1 chแป, cแบฃ 2 file ฤแปu ฤฦฐแปฃc cแบญp nhแบญt
-   โ **Code sแบกch:** File chรญnh chแป tแบญp trung vรo logic cแปฅ thแป

---

### 2๏ธโฃ Huแบฅn Luyแปn Mรด Hรฌnh

```python
# Huแบฅn luyแปn mรด hรฌnh vแปi n = 10 iterations
m, b, costs = train_logistic_regression(
    dataset=DATASET,
    m_init=1.0,
    b_init=-1.0,
    iterations=10,
    learning_rate=1.0
)
```

**Giแบฃi thรญch:**

#### **Gแปi hรm training**

Hรm `train_logistic_regression` nhแบญn cรกc tham sแป:

1. **`dataset=DATASET`**

    - Truyแปn dแปฏ liแปu training (8 ฤiแปm dแปฏ liแปu)
    - Dataset ฤรฃ ฤฦฐแปฃc ฤแปnh nghฤฉa trong `logistic_regression_utils.py`

2. **`m_init=1.0`**

    - **m** (slope/hแป sแป gรณc) khแปi tแบกo = 1.0
    - Tฦฐฦกng ฤฦฐฦกng vแปi **w** trong Exercise 1
    - Giรก trแป ban ฤแบงu khรกc 0 โ cรณ hฦฐแปng hแปc ngay tแปซ ฤแบงu

3. **`b_init=-1.0`**

    - **b** (bias/hแป sแป chแบทn) khแปi tแบกo = -1.0
    - Giรก trแป รขm โ dแปch sigmoid sang phแบฃi
    - Phรน hแปฃp vแปi dแปฏ liแปu (cแบงn threshold khoแบฃng 2-2.5 giแป)

4. **`iterations=10`**

    - Chแป chแบกy **10 vรฒng lแบทp** (รญt hฦกn rแบฅt nhiแปu so vแปi Exercise 1 cรณ 1000)
    - ฤแป bรi yรชu cแบงu n = 10
    - Vแปi learning_rate lแปn, 10 iterations cรณ thแป ฤแปง

5. **`learning_rate=1.0`**
    - Tแปc ฤแป hแปc rแบฅt **cao** (gแบฅp 10,000 lแบงn Exercise 1)
    - Cho phรฉp mรด hรฌnh hแปc nhanh trong รญt iteration
    - **Rแปงi ro:** Cรณ thแป overshooting nแบฟu khรดng cแบฉn thแบญn

#### **Kแบฟt quแบฃ trแบฃ vแป**

```python
m, b, costs = train_logistic_regression(...)
```

-   **`m`:** Hแป sแป gรณc sau khi training
-   **`b`:** Hแป sแป chแบทn sau khi training
-   **`costs`:** List chแปฉa giรก trแป Cost qua 10 iterations

**So sรกnh vแปi Exercise 1:**

| Tham sแป       | Exercise 1        | Exercise 2          |
| ------------- | ----------------- | ------------------- |
| Tham sแป       | w, b              | m, b                |
| Khแปi tแบกo      | 0, 0              | 1.0, -1.0           |
| Learning Rate | 0.0001            | 1.0                 |
| Iterations    | 1000              | 10                  |
| Chiแบฟn lฦฐแปฃc    | Hแปc chแบญm, แปn ฤแปnh | Hแปc nhanh, mแบกo hiแปm |

---

### 3๏ธโฃ Dแปฑ ฤoรกn Cho Input Mแปi

```python
hours_input = 2.8
predicted_score = get_prediction(m, b, hours_input)
```

**Giแบฃi thรญch:**

-   **`hours_input = 2.8`:** Sinh viรชn hแปc 2.8 giแป (ฤแป bรi yรชu cแบงu)
-   **`get_prediction(m, b, hours_input)`:** Gแปi hรm dแปฑ ฤoรกn
    -   Truyแปn vรo:
        -   `m, b`: Tham sแป ฤรฃ hแปc ฤฦฐแปฃc
        -   `hours_input`: Giรก trแป x cแบงn dแปฑ ฤoรกn
    -   Trแบฃ vแป: Xรกc suแบฅt ฤแบญu (giรก trแป tแปซ 0 ฤแบฟn 1)

**Cรดng thแปฉc bรชn trong hรm:**

```python
def get_prediction(m, b, x):
    y = m * x + b
    return 1 / (1 + math.exp(-y))
```

Vแปi m, b ฤรฃ hแปc vร x = 2.8:

1. Tรญnh z = m ร 2.8 + b
2. Tรญnh sigmoid(z) = 1 / (1 + e^(-z))

**Vรญ dแปฅ vแปi m=2.0, b=-4.0:**

-   z = 2.0 ร 2.8 + (-4.0) = 5.6 - 4.0 = 1.6
-   sigmoid(1.6) = 1 / (1 + e^(-1.6)) โ 0.832

โ Xรกc suแบฅt ฤแบญu โ 83.2%

---

### 4๏ธโฃ In Kแบฟt Quแบฃ vร Kแบฟt Luแบญn

```python
print("\n" + "-"*40)

print(f"Kแบฟt quแบฃ dแปฑ ฤoรกn cho {hours_input} giแป hแปc:")
print(f"ฤiแปm sแป dแปฑ ฤoรกn: {predicted_score:.4f}")
print(f"Xรกc suแบฅt ฤแบญu: {predicted_score:.4f} ({predicted_score*100:.2f}%)")

if predicted_score >= 0.5:
    print("=> Kแบฟt luแบญn: ฤแบฌU")
else:
    print("=> Kแบฟt luแบญn: RแปT")

print("-"*40 + "\n")
```

**Giแบฃi thรญch:**

#### **In tiรชu ฤแป**

```python
print("\n" + "-"*40)
```

-   `"\n"`: Xuแปng dรฒng
-   `"-"*40`: In 40 dแบฅu gแบกch ngang (separator ฤแบนp mแบฏt)

#### **In kแบฟt quแบฃ dแปฑ ฤoรกn**

```python
print(f"Kแบฟt quแบฃ dแปฑ ฤoรกn cho {hours_input} giแป hแปc:")
```

-   In sแป giแป input (2.8)

```python
print(f"ฤiแปm sแป dแปฑ ฤoรกn: {predicted_score:.4f}")
```

-   In xรกc suแบฅt vแปi 4 chแปฏ sแป thแบญp phรขn (vรญ dแปฅ: 0.8324)

```python
print(f"Xรกc suแบฅt ฤแบญu: {predicted_score:.4f} ({predicted_score*100:.2f}%)")
```

-   In cแบฃ dแบกng thแบญp phรขn (0.8324) vร phแบงn trฤm (83.24%)
-   `.2f`: 2 chแปฏ sแป thแบญp phรขn cho phแบงn trฤm

#### **Phรขn loแบกi (Classification)**

```python
if predicted_score >= 0.5:
    print("=> Kแบฟt luแบญn: ฤแบฌU")
else:
    print("=> Kแบฟt luแบญn: RแปT")
```

**Ngฦฐแปกng quyแบฟt ฤแปnh (Decision Threshold):**

-   **Xรกc suแบฅt โฅ 0.5:** Dแปฑ ฤoรกn lแปp **dฦฐฦกng** (y=1, ฤแบฌU)
-   **Xรกc suแบฅt < 0.5:** Dแปฑ ฤoรกn lแปp **รขm** (y=0, RแปT)

**Tแบกi sao chแปn 0.5?**

-   0.5 lร **ฤiแปm cรขn bแบฑng** (50-50)
-   Tแบกi ฤiแปm nรy, sigmoid cแบฏt trแปฅc y
-   z = 0 โ sigmoid(0) = 0.5
-   Lร ngฦฐแปกng chuแบฉn cho bรi toรกn cรขn bแบฑng (balanced dataset)

**Cรณ thแป ฤiแปu chแปnh threshold:**

-   Nแบฟu muแปn "cแบฉn thแบญn hฦกn" โ threshold = 0.7 (phแบฃi rแบฅt chแบฏc mแปi kแบฟt luแบญn ฤแบญu)
-   Nแบฟu muแปn "dแป dรฃi hฦกn" โ threshold = 0.3 (dแป kแบฟt luแบญn ฤแบญu)
-   Trade-off giแปฏa Precision vร Recall

#### **Kแบฟt thรบc**

```python
print("-"*40 + "\n")
```

-   In separator ฤรณng
-   Xuแปng dรฒng ฤแป thoรกng

---

## ๐ Phรขn Tรญch Module `logistic_regression_utils.py`

ฤแป hiแปu rรต hฦกn, phรขn tรญch cรกc hรm trong module:

### **1. Dataset Definition**

```python
DATASET = [
    [0.5, 0],
    [1.0, 0],
    [1.5, 0],
    [2.0, 0],
    [2.5, 1],
    [3.0, 1],
    [3.5, 1],
    [4.0, 1]
]
```

**Giแบฃi thรญch:**

-   **Danh sรกch 2D:** Mแปi phแบงn tแปญ lร `[hours, pass]`
-   **8 mแบซu dแปฏ liแปu:**
    -   4 mแบซu rแปt (0.5-2.0 giแป โ y=0)
    -   4 mแบซu ฤแบญu (2.5-4.0 giแป โ y=1)

**Phรขn tรญch dataset:**

-   **Dแปฏ liแปu cรขn bแบฑng:** 50% ฤแบญu, 50% rแปt
-   **Phรขn chia rรต rรng:** Cรณ khoแบฃng trแปng giแปฏa 2.0 vร 2.5
-   **Tuyแบฟn tรญnh khรก tแปt:** Cรณ thแป vแบฝ ฤฦฐแปng phรขn chia rรต rรng
-   **Decision boundary dแปฑ kiแบฟn:** Khoแบฃng 2.2-2.3 giแป

---

### **2. Hรm get_prediction**

```python
def get_prediction(m, b, x):
    # Sigmoid function
    y = m * x + b
    return 1 / (1 + math.exp(-y))
```

**Giแบฃi thรญch:**

Hรm nรy thแปฑc hiแปn **forward propagation** (truyแปn tiแบฟn):

1. **Tรญnh giรก trแป tuyแบฟn tรญnh:**

    ```python
    y = m * x + b
    ```

    - `y` แป ฤรขy thแปฑc ra lร `z` (pre-activation)
    - Phฦฐฦกng trรฌnh ฤฦฐแปng thแบณng: $z = mx + b$

2. **รp dแปฅng sigmoid:**
    ```python
    return 1 / (1 + math.exp(-y))
    ```
    - $\sigma(z) = \frac{1}{1 + e^{-z}}$
    - Chuyแปn z thรnh xรกc suแบฅt (0, 1)

**Vรญ dแปฅ sแปญ dแปฅng:**

```python
m, b = 2.0, -4.0
prob = get_prediction(m, b, 2.8)
# z = 2.0 * 2.8 + (-4.0) = 1.6
# sigmoid(1.6) โ 0.832
# Xรกc suแบฅt ฤแบญu โ 83.2%
```

---

### **3. Hรm get_cost**

```python
def get_cost(y, y_hat):
    # Binary cross-entropy
    k = len(y)
    total_cost = 0.0
    for yi, y_hat_i in zip(y, y_hat):
        total_cost += -(yi * math.log(y_hat_i) + (1 - yi) * math.log(1 - y_hat_i))
    return total_cost / k
```

**Giแบฃi thรญch:**

Hรm tรญnh **Binary Cross-Entropy Loss** - giแปng Exercise 1 nhฦฐng implement khรกc.

#### **Tham sแป:**

-   **`y`:** List cรกc nhรฃn thแปฑc tแบฟ `[0, 0, 0, 0, 1, 1, 1, 1]`
-   **`y_hat`:** List cรกc xรกc suแบฅt dแปฑ ฤoรกn `[0.1, 0.2, 0.3, 0.4, 0.6, 0.7, 0.8, 0.9]`

#### **Cรกch tรญnh:**

1. **ฤแบฟm sแป mแบซu:**

    ```python
    k = len(y)
    ```

    - k = 8 (sแป mแบซu trong dataset)

2. **Khแปi tแบกo tแปng cost:**

    ```python
    total_cost = 0.0
    ```

3. **Duyแปt tแปซng cแบทp (yi, y_hat_i):**

    ```python
    for yi, y_hat_i in zip(y, y_hat):
    ```

    - `zip(y, y_hat)`: Ghรฉp tแปซng cแบทp tฦฐฦกng แปฉng
    - Vรญ dแปฅ: (0, 0.1), (0, 0.2), ..., (1, 0.9)

4. **Cแปng dแปn loss:**

    ```python
    total_cost += -(yi * math.log(y_hat_i) + (1 - yi) * math.log(1 - y_hat_i))
    ```

    - **Nแบฟu yi = 1:** Chแป tรญnh `-log(y_hat_i)`
    - **Nแบฟu yi = 0:** Chแป tรญnh `-log(1 - y_hat_i)`

5. **Trung bรฌnh:**
    ```python
    return total_cost / k
    ```
    - Chia cho k ฤแป lแบฅy trung bรฌnh

**So sรกnh vแปi Exercise 1:**

|         | Exercise 1 | Exercise 2      |
| ------- | ---------- | --------------- |
| Library | numpy      | math (built-in) |
| Style   | Vectorized | Loop            |
| Epsilon | 1e-15      | Khรดng cรณ        |
| Tแปc ฤแป  | Nhanh hฦกn  | Chแบญm hฦกn        |

**Lฦฐu รฝ:** Exercise 2 khรดng cรณ epsilon โ cรณ thแป gแบทp lแปi `log(0)` nแบฟu y_hat = 0 hoแบทc 1. Trong thแปฑc tแบฟ nรชn thรชm epsilon!

---

### **4. Hรm get_gradients**

```python
def get_gradients(m, b, x, y, y_hat):
    # Calculate gradients
    k = len(y)
    dm = (1 / k) * sum((y_hat_i - yi) * xi for y_hat_i, yi, xi in zip(y_hat, y, x))
    db = (1 / k) * sum(y_hat_i - yi for y_hat_i, yi in zip(y_hat, y))
    return dm, db
```

**Giแบฃi thรญch:**

Hรm tรญnh **gradient** (ฤแบกo hรm) cแปงa Cost function - giแปng Exercise 1.

#### **Tham sแป:**

-   **`m, b`:** Tham sแป hiแปn tแบกi (khรดng dรนng trong hรm nรy, chแป ฤแป tฦฐฦกng thรญch)
-   **`x`:** List giรก trแป features `[0.5, 1.0, 1.5, ..., 4.0]`
-   **`y`:** List nhรฃn thแปฑc tแบฟ `[0, 0, 0, ..., 1]`
-   **`y_hat`:** List xรกc suแบฅt dแปฑ ฤoรกn `[h1, h2, ..., h8]`

#### **Cรกch tรญnh:**

1. **Gradient cแปงa m (hแป sแป gรณc):**

    ```python
    dm = (1 / k) * sum((y_hat_i - yi) * xi for y_hat_i, yi, xi in zip(y_hat, y, x))
    ```

    **Cรดng thแปฉc toรกn hแปc:**
    $$\frac{\partial J}{\partial m} = \frac{1}{k}\sum_{i=1}^{k}(h_i - y_i) \cdot x_i$$

    **Phรขn tรญch:**

    - `(y_hat_i - yi)`: Sai sแป tแบกi ฤiแปm thแปฉ i
    - `* xi`: Nhรขn vแปi feature ฤแป tรญnh weighted error
    - `sum(...)`: Tแปng trรชn tแบฅt cแบฃ ฤiแปm
    - `(1 / k) *`: Trung bรฌnh

    **Generator expression:**

    - `for y_hat_i, yi, xi in zip(y_hat, y, x)`: Duyแปt qua 3 list cรนng lรบc
    - Pythonic vร gแปn hฦกn vรฒng for thรดng thฦฐแปng

2. **Gradient cแปงa b (hแป sแป chแบทn):**

    ```python
    db = (1 / k) * sum(y_hat_i - yi for y_hat_i, yi in zip(y_hat, y))
    ```

    **Cรดng thแปฉc toรกn hแปc:**
    $$\frac{\partial J}{\partial b} = \frac{1}{k}\sum_{i=1}^{k}(h_i - y_i)$$

    **Phรขn tรญch:**

    - Giแปng dm nhฦฐng **khรดng nhรขn** vแปi xi
    - Vรฌ ฤแบกo hรm cแปงa b lร 1

**Vรญ dแปฅ tรญnh dm:**

```python
x = [0.5, 1.0, 1.5, 2.0]
y = [0, 0, 0, 1]
y_hat = [0.1, 0.2, 0.3, 0.8]

errors = [0.1-0, 0.2-0, 0.3-0, 0.8-1] = [0.1, 0.2, 0.3, -0.2]
weighted = [0.1*0.5, 0.2*1.0, 0.3*1.5, -0.2*2.0] = [0.05, 0.2, 0.45, -0.4]
sum = 0.05 + 0.2 + 0.45 - 0.4 = 0.3
dm = 0.3 / 4 = 0.075
```

---

### **5. Hรm get_accuracy**

```python
def get_accuracy(y, y_hat):
    correct_predictions = sum((1 if y_hat_i >= 0.5 else 0) == yi for y_hat_i, yi in zip(y_hat, y))
    return correct_predictions / len(y)
```

**Giแบฃi thรญch:**

Hรm tรญnh **accuracy** (ฤแป chรญnh xรกc) cแปงa mรด hรฌnh.

#### **Cรกch hoแบกt ฤแปng:**

1. **Chuyแปn xรกc suแบฅt thรnh nhรฃn:**

    ```python
    1 if y_hat_i >= 0.5 else 0
    ```

    - Nแบฟu y_hat_i โฅ 0.5 โ dแปฑ ฤoรกn 1 (ฤแบฌU)
    - Ngฦฐแปฃc lแบกi โ dแปฑ ฤoรกn 0 (RแปT)

2. **So sรกnh vแปi nhรฃn thแปฑc:**

    ```python
    (... == yi)
    ```

    - True nแบฟu dแปฑ ฤoรกn ฤรบng
    - False nแบฟu dแปฑ ฤoรกn sai

3. **ฤแบฟm sแป dแปฑ ฤoรกn ฤรบng:**

    ```python
    correct_predictions = sum(...)
    ```

    - `sum` trรชn boolean: True=1, False=0
    - Kแบฟt quแบฃ: Tแปng sแป dแปฑ ฤoรกn ฤรบng

4. **Tรญnh tแปท lแป:**
    ```python
    return correct_predictions / len(y)
    ```
    - Sแป ฤรบng / Tแปng sแป mแบซu
    - Kแบฟt quแบฃ tแปซ 0.0 (0%) ฤแบฟn 1.0 (100%)

**Vรญ dแปฅ:**

```python
y = [0, 0, 1, 1, 1]
y_hat = [0.2, 0.6, 0.7, 0.8, 0.3]

# Chuyแปn thรnh nhรฃn:
predictions = [0, 1, 1, 1, 0]

# So sรกnh:
# 0 == 0 โ
# 1 == 0 โ
# 1 == 1 โ
# 1 == 1 โ
# 0 == 1 โ

# ฤแบฟm: 3 ฤรบng / 5 tแปng = 0.6 = 60%
accuracy = 3 / 5 = 0.6
```

---

### **6. Hรm train_logistic_regression**

```python
def train_logistic_regression(dataset=DATASET, m_init=1.0, b_init=-1.0, iterations=10, learning_rate=1.0):
    m = m_init
    b = b_init

    x = [row[0] for row in dataset]
    y = [row[1] for row in dataset]

    costs = []

    for it in range(iterations):
        y_hat = [get_prediction(m, b, xi) for xi in x]

        cost = get_cost(y, y_hat)
        costs.append(cost)

        dm, db = get_gradients(m, b, x, y, y_hat)

        m -= learning_rate * dm
        b -= learning_rate * db

    return m, b, costs
```

**Giแบฃi thรญch:**

ฤรขy lร **hรm chรญnh** thแปฑc hiแปn thuแบญt toรกn Gradient Descent - trรกi tim cแปงa bรi toรกn.

#### **Tham sแป:**

-   **`dataset`:** Dแปฏ liแปu training (mแบทc ฤแปnh DATASET)
-   **`m_init, b_init`:** Giรก trแป khแปi tแบกo (mแบทc ฤแปnh 1.0, -1.0)
-   **`iterations`:** Sแป vรฒng lแบทp (mแบทc ฤแปnh 10)
-   **`learning_rate`:** Tแปc ฤแป hแปc (mแบทc ฤแปnh 1.0)

#### **Bฦฐแปc 1: Khแปi tแบกo tham sแป**

```python
m = m_init
b = b_init
```

-   Gรกn giรก trแป ban ฤแบงu cho m, b

#### **Bฦฐแปc 2: Tรกch dแปฏ liแปu**

```python
x = [row[0] for row in dataset]
y = [row[1] for row in dataset]
```

**List comprehension:**

-   `row[0]`: Cแปt ฤแบงu tiรชn (hours)
-   `row[1]`: Cแปt thแปฉ hai (pass)

**Kแบฟt quแบฃ:**

```python
x = [0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0]
y = [0, 0, 0, 0, 1, 1, 1, 1]
```

#### **Bฦฐแปc 3: Khแปi tแบกo list lฦฐu cost**

```python
costs = []
```

-   ฤแป tracking quรก trรฌnh hแปc

#### **Bฦฐแปc 4: Vรฒng lแบทp training**

```python
for it in range(iterations):
```

-   Lแบทp `iterations` lแบงn (10 lแบงn)

**Bรชn trong mแปi iteration:**

**4.1. Forward propagation:**

```python
y_hat = [get_prediction(m, b, xi) for xi in x]
```

-   Tรญnh xรกc suแบฅt dแปฑ ฤoรกn cho tแปซng ฤiแปm
-   `y_hat` lร list 8 giรก trแป xรกc suแบฅt

**4.2. Tรญnh cost:**

```python
cost = get_cost(y, y_hat)
costs.append(cost)
```

-   ฤรกnh giรก chแบฅt lฦฐแปฃng mรด hรฌnh hiแปn tแบกi
-   Lฦฐu vรo list

**4.3. Backward propagation (tรญnh gradient):**

```python
dm, db = get_gradients(m, b, x, y, y_hat)
```

-   Tรญnh ฤแบกo hรm ฤแป biแบฟt hฦฐแปng ฤi

**4.4. Cแบญp nhแบญt tham sแป:**

```python
m -= learning_rate * dm
b -= learning_rate * db
```

-   Gradient Descent step
-   ฤi ngฦฐแปฃc hฦฐแปng gradient ฤแป giแบฃm cost

#### **Bฦฐแปc 5: Trแบฃ vแป kแบฟt quแบฃ**

```python
return m, b, costs
```

-   `m, b`: Tham sแป tแปi ฦฐu sau training
-   `costs`: Lแปch sแปญ cost (ฤแป phรขn tรญch)

**So sรกnh vแปi Exercise 1:**

|              | Exercise 1       | Exercise 2                |
| ------------ | ---------------- | ------------------------- |
| Hรm training | gradient_descent | train_logistic_regression |
| In progress  | Cรณ               | Khรดng                     |
| Lฦฐu history  | cost, w, b       | Chแป cost                  |
| Return       | w, b, 3 lists    | m, b, 1 list              |

---

## ๐ Output vร Kแบฟt Quแบฃ (Dแปฑ Kiแบฟn)

### ๐ฅ๏ธ Console Output

```
----------------------------------------
Kแบฟt quแบฃ dแปฑ ฤoรกn cho 2.8 giแป hแปc:
ฤiแปm sแป dแปฑ ฤoรกn: 0.7854
Xรกc suแบฅt ฤแบญu: 0.7854 (78.54%)
=> Kแบฟt luแบญn: ฤแบฌU
----------------------------------------
```

**Lฦฐu รฝ:** Giรก trแป cแปฅ thแป phแปฅ thuแปc vรo kแบฟt quแบฃ training thแปฑc tแบฟ.

---

### ๐ Phรขn Tรญch Kแบฟt Quแบฃ

#### **1. Xรกc suแบฅt ฤแบญu: ~78.54%**

**ร nghฤฉa:**

-   Sinh viรชn hแปc 2.8 giแป cรณ xรกc suแบฅt ฤแบญu gแบงn **80%**
-   Khรดng phแบฃi 100% vรฌ mรด hรฌnh hแปc tแปซ dแปฏ liแปu cรณ **uncertainty**
-   Dแปฑ ฤoรกn hแปฃp lรฝ vรฌ:
    -   2.8 giแป > 2.5 giแป (ฤiแปm ฤแบญu thแบฅp nhแบฅt)
    -   2.8 giแป gแบงn 3.0 giแป (ฤiแปm ฤแบญu chแบฏc chแบฏn)

#### **2. Kแบฟt luแบญn: ฤแบฌU**

**Logic:**

-   Xรกc suแบฅt 0.7854 โฅ 0.5 โ Phรขn loแบกi vรo lแปp 1 (ฤแบฌU)
-   Mแปฉc ฤแป tin cแบญy: **cao** (gแบงn 80%, khรดng phแบฃi 51%)

#### **3. Phรขn tรญch theo ngฦฐแปกng giแป hแปc**

Giแบฃ sแปญ mรด hรฌnh hแปc ฤฦฐแปฃc:

-   m โ 2.0, b โ -4.0
-   Decision boundary: m ร x + b = 0
    โ x = -b/m = 4.0/2.0 = 2.0 giแป

**Dแปฑ ฤoรกn theo ngฦฐแปกng:**

-   x < 2.0 giแป: Xรกc suแบฅt ฤแบญu < 50% โ RแปT
-   x = 2.0 giแป: Xรกc suแบฅt ฤแบญu = 50% โ Biรชn giแปi
-   x > 2.0 giแป: Xรกc suแบฅt ฤแบญu > 50% โ ฤแบฌU

**2.8 giแป > 2.0 giแป** โ ฤแบฌU (hแปฃp lรฝ!)

#### **4. So sรกnh vแปi dแปฏ liแปu thแปฑc**

| Hours   | Actual | Predicted (approx) |
| ------- | ------ | ------------------ |
| 0.5     | RแปT    | RแปT (~5%)          |
| 1.0     | RแปT    | RแปT (~12%)         |
| 1.5     | RแปT    | RแปT (~27%)         |
| 2.0     | RแปT    | Biรชn giแปi (~50%)   |
| 2.5     | ฤแบฌU    | ฤแบฌU (~73%)         |
| **2.8** | ?      | **ฤแบฌU (~79%)**     |
| 3.0     | ฤแบฌU    | ฤแบฌU (~88%)         |
| 3.5     | ฤแบฌU    | ฤแบฌU (~95%)         |
| 4.0     | ฤแบฌU    | ฤแบฌU (~98%)         |

**Nhแบญn xรฉt:**

-   Mรด hรฌnh dแปฑ ฤoรกn hแปฃp lรฝ vแปi dแปฏ liแปu
-   2.8 giแป nแบฑm giแปฏa 2.5 (73%) vร 3.0 (88%) โ ~79% lร hแปฃp lรฝ

---
