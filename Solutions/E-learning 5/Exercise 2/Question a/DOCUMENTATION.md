# ๐ E-Learning 5 - Exercise 2 - Question A: Logistic Regression Training

## ๐ฏ Mแปฅc Tiรชu Bรi Tแบญp

Bรi tแบญp yรชu cแบงu **lแบญp trรฌnh vร huแบฅn luyแปn mแปt mรด hรฌnh Logistic Regression** tแปซ ฤแบงu (khรดng dรนng thฦฐ viแปn mรกy hแปc) ฤแป dแปฑ ฤoรกn xรกc suแบฅt sinh viรชn ฤแบญu/rแปt dแปฑa trรชn sแป giแป hแปc.

### ๐ ฤแป Bรi

**Dataset:** Quan hแป giแปฏa thแปi gian tแปฑ hแปc vร kแบฟt quแบฃ ฤแบงu/rแปt cแปงa sinh viรชn

| Hours (Giแป hแปc) | Pass (Kแบฟt quแบฃ) |
| --------------- | -------------- |
| 0.5             | 0 (Rแปt)        |
| 1.0             | 0 (Rแปt)        |
| 1.5             | 0 (Rแปt)        |
| 2.0             | 0 (Rแปt)        |
| 2.5             | 1 (ฤแบญu)        |
| 3.0             | 1 (ฤแบญu)        |
| 3.5             | 1 (ฤแบญu)        |
| 4.0             | 1 (ฤแบญu)        |

**Yรชu cแบงu:**

-   Lแบญp trรฌnh thuแบญt toรกn Logistic Regression vแปi **sแป lแบงn huแบฅn luyแปn n = 10**
-   Sau ฤรณ **dแปฑ ฤoรกn** khi sinh viรชn tแปฑ hแปc **2.8 giแป** thรฌ xรกc suแบฅt ฤแบญu lร bao nhiรชu?
-   Kแบฟt luแบญn: Sinh viรชn sแบฝ **ฤแบญu hay rแปt**?

**Lฦฐu รฝ:** Khรดng ฤฦฐแปฃc sแปญ dแปฅng thฦฐ viแปn sklearn (chแป ฤฦฐแปฃc dรนng thฦฐ viแปn cฦก bแบฃn)

---

## ๐ป Phรขn Tรญch Source Code Chi Tiแบฟt

### 1๏ธโฃ Import Module vร Cแบฅu Trรบc Project

```python
import sys
import os

# Thรชm thฦฐ mแปฅc cha vรo path ฤแป import module
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from logistic_regression_utils import (
    DATASET, get_prediction, train_logistic_regression
)
```

**Giแบฃi thรญch:**

#### **Import sys vร os**

ฤoแบกn code bแบฏt ฤแบงu bแบฑng viแปc import hai module hแป thแปng quan trแปng. Module `sys` cung cแบฅp cรกc chแปฉc nฤng ฤแป thao tรกc vแปi mรดi trฦฐแปng runtime cแปงa Python, trong khi module `os` cho phรฉp lรm viแปc vแปi hแป ฤiแปu hรnh, ฤแบทc biแปt lร cรกc thao tรกc liรชn quan ฤแบฟn file vร thฦฐ mแปฅc.

#### **Thรชm ฤฦฐแปng dแบซn module**

```python
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
```

**Phรขn tรญch tแปซng bฦฐแปc:**

Quรก trรฌnh xแปญ lรฝ ฤฦฐแปng dแบซn diแปn ra qua nฤm bฦฐแปc tuแบงn tแปฑ. Bฦฐแปc ฤแบงu tiรชn, biแบฟn ฤแบทc biแปt `__file__` chแปฉa ฤฦฐแปng dแบซn cแปงa file Python hiแปn tแบกi, vรญ dแปฅ nhฦฐ `d:/UTH-AI-assignments/.../Exercise 2/Question a/ex2a_logistic_regression.py`. Tiแบฟp theo, hรm `os.path.abspath(__file__)` chuyแปn ฤแปi ฤฦฐแปng dแบซn nรy thรnh dแบกng tuyแปt ฤแปi, ฤแบฃm bแบฃo ฤฦฐแปng dแบซn ฤแบงy ฤแปง vร khรดng phแปฅ thuแปc vรo thฦฐ mแปฅc lรm viแปc hiแปn tแบกi.

Bฦฐแปc thแปฉ ba รกp dแปฅng `os.path.dirname(...)` lแบงn ฤแบงu ฤแป lแบฅy thฦฐ mแปฅc cha, cho kแบฟt quแบฃ `d:/UTH-AI-assignments/.../Exercise 2/Question a/`. Sau ฤรณ, รกp dแปฅng `os.path.dirname(...)` lแบงn thแปฉ hai ฤแป lรชn thรชm mแปt cแบฅp nแปฏa, thu ฤฦฐแปฃc `d:/UTH-AI-assignments/.../Exercise 2/`. Cuแปi cรนng, `sys.path.insert(0, ...)` thรชm ฤฦฐแปng dแบซn nรy vรo vแป trรญ ฤแบงu tiรชn (index 0) cแปงa danh sรกch tรฌm kiแบฟm module, tแบกo ฦฐu tiรชn cao nhแบฅt. ฤiแปu nรy cho phรฉp Python tรฌm thแบฅy file `logistic_regression_utils.py` nแบฑm แป thฦฐ mแปฅc cha.

**Tแบกi sao cแบงn lรm thแบฟ nรy?**

Lรฝ do cแบงn thao tรกc nรy xuแบฅt phรกt tแปซ cแบฅu trรบc thฦฐ mแปฅc cแปงa project. File `logistic_regression_utils.py` nแบฑm แป thฦฐ mแปฅc cha `Exercise 2/`, trong khi file hiแปn tแบกi nแบฑm แป thฦฐ mแปฅc con `Exercise 2/Question a/`. Do Python mแบทc ฤแปnh chแป tรฌm kiแบฟm module trong thฦฐ mแปฅc hiแปn tแบกi vร cรกc thฦฐ mแปฅc trong `sys.path`, nรชn cแบงn phแบฃi thรชm ฤฦฐแปng dแบซn thแปง cรดng ฤแป cรณ thแป import tแปซ thฦฐ mแปฅc cha.

**Cแบฅu trรบc thฦฐ mแปฅc:**

```
Exercise 2/
โโโ logistic_regression_utils.py  โ Module chแปฉa cรกc hรm
โโโ Question a/
โ   โโโ ex2a_logistic_regression.py  โ File nรy
โโโ Question b/
    โโโ ex2b_logistic_regression.py
```

#### **Import cรกc hรm tแปซ module**

```python
from logistic_regression_utils import (
    DATASET, get_prediction, train_logistic_regression
)
```

**Giแบฃi thรญch:**

ฤoแบกn import nรy lแบฅy ba thรnh phแบงn quan trแปng tแปซ module tiแปn รญch. Biแบฟn `DATASET` lร hแบฑng sแป chแปฉa toรn bแป dแปฏ liแปu training gแปm 8 cแบทp giรก trแป (hours, pass). Hรm `get_prediction` thแปฑc hiแปn dแปฑ ฤoรกn xรกc suแบฅt cho cรกc giรก trแป input mแปi dแปฑa trรชn tham sแป ฤรฃ hแปc. Cรฒn hรm `train_logistic_regression` chแปu trรกch nhiแปm huแบฅn luyแปn mรด hรฌnh vแปi thuแบญt toรกn Gradient Descent.

**Lแปฃi รญch cแปงa cรกch tแป chแปฉc nรy:**

Viแปc tแป chแปฉc code theo cรกch nรy mang lแบกi nhiแปu ฦฐu ฤiแปm quan trแปng. Thแปฉ nhแบฅt, cรกc hรm ฤฦฐแปฃc tรกi sแปญ dแปฅng cho cแบฃ Question A vร Question B, trรกnh viแปc viแบฟt lแบกi code trรนng lแบทp. Thแปฉ hai, khi cแบงn sแปญa ฤแปi logic, chแป cแบงn thay ฤแปi แป mแปt chแป trong file utils, vร cแบฃ hai file sแปญ dแปฅng ฤแปu ฤฦฐแปฃc cแบญp nhแบญt tแปฑ ฤแปng. Cuแปi cรนng, file chรญnh trแป nรชn sแบกch sแบฝ vร tแบญp trung vรo logic cแปฅ thแป cแปงa tแปซng cรขu hแปi.

---

### 2๏ธโฃ Huแบฅn Luyแปn Mรด Hรฌnh

```python
# Huแบฅn luyแปn mรด hรฌnh vแปi n = 10 iterations
m, b, costs = train_logistic_regression(
    dataset=DATASET,
    m_init=1.0,
    b_init=-1.0,
    iterations=10,
    learning_rate=1.0
)
```

**Giแบฃi thรญch:**

#### **Gแปi hรm training**

Hรm `train_logistic_regression` nhแบญn cรกc tham sแป:

1. **`dataset=DATASET`**

    - Truyแปn dแปฏ liแปu training (8 ฤiแปm dแปฏ liแปu)
    - Dataset ฤรฃ ฤฦฐแปฃc ฤแปnh nghฤฉa trong `logistic_regression_utils.py`

2. **`m_init=1.0`**

    - **m** (slope/hแป sแป gรณc) khแปi tแบกo = 1.0
    - Tฦฐฦกng ฤฦฐฦกng vแปi **w** trong Exercise 1
    - Giรก trแป ban ฤแบงu khรกc 0 โ cรณ hฦฐแปng hแปc ngay tแปซ ฤแบงu

3. **`b_init=-1.0`**

    - **b** (bias/hแป sแป chแบทn) khแปi tแบกo = -1.0
    - Giรก trแป รขm โ dแปch sigmoid sang phแบฃi
    - Phรน hแปฃp vแปi dแปฏ liแปu (cแบงn threshold khoแบฃng 2-2.5 giแป)

4. **`iterations=10`**

    - Chแป chแบกy **10 vรฒng lแบทp** (รญt hฦกn rแบฅt nhiแปu so vแปi Exercise 1 cรณ 1000)
    - ฤแป bรi yรชu cแบงu n = 10
    - Vแปi learning_rate lแปn, 10 iterations cรณ thแป ฤแปง

5. **`learning_rate=1.0`**
    - Tแปc ฤแป hแปc rแบฅt **cao** (gแบฅp 10,000 lแบงn Exercise 1)
    - Cho phรฉp mรด hรฌnh hแปc nhanh trong รญt iteration
    - **Rแปงi ro:** Cรณ thแป overshooting nแบฟu khรดng cแบฉn thแบญn

#### **Kแบฟt quแบฃ trแบฃ vแป**

```python
m, b, costs = train_logistic_regression(...)
```

-   **`m`:** Hแป sแป gรณc sau khi training
-   **`b`:** Hแป sแป chแบทn sau khi training
-   **`costs`:** List chแปฉa giรก trแป Cost qua 10 iterations

**So sรกnh vแปi Exercise 1:**

| Tham sแป       | Exercise 1        | Exercise 2          |
| ------------- | ----------------- | ------------------- |
| Tham sแป       | w, b              | m, b                |
| Khแปi tแบกo      | 0, 0              | 1.0, -1.0           |
| Learning Rate | 0.0001            | 1.0                 |
| Iterations    | 1000              | 10                  |
| Chiแบฟn lฦฐแปฃc    | Hแปc chแบญm, แปn ฤแปnh | Hแปc nhanh, mแบกo hiแปm |

---

### 3๏ธโฃ Dแปฑ ฤoรกn Cho Input Mแปi

```python
hours_input = 2.8
predicted_score = get_prediction(m, b, hours_input)
```

**Giแบฃi thรญch:**

Sau khi huแบฅn luyแปn xong, biแบฟn `hours_input` ฤฦฐแปฃc gรกn giรก trแป 2.8, thแป hiแปn trฦฐแปng hแปฃp sinh viรชn hแปc 2.8 giแป theo yรชu cแบงu cแปงa ฤแป bรi. Tiแบฟp ฤรณ, hรm `get_prediction(m, b, hours_input)` ฤฦฐแปฃc gแปi ฤแป thแปฑc hiแปn dแปฑ ฤoรกn. Hรm nรy nhแบญn ba tham sแป ฤแบงu vรo: cรกc tham sแป m vร b vแปซa hแปc ฤฦฐแปฃc tแปซ quรก trรฌnh training, cรนng vแปi giรก trแป x cแบงn dแปฑ ฤoรกn (2.8 giแป). Kแบฟt quแบฃ trแบฃ vแป lร xรกc suแบฅt ฤแบญu nแบฑm trong khoแบฃng tแปซ 0 ฤแบฟn 1.

**Cรดng thแปฉc bรชn trong hรm:**

Quรก trรฌnh tรญnh toรกn trong hรm `get_prediction` diแปn ra qua hai bฦฐแปc. Bฦฐแปc ฤแบงu tiรชn tรญnh giรก trแป tuyแบฟn tรญnh:

$$z = m \times x + b$$

Sau ฤรณ รกp dแปฅng hรm sigmoid ฤแป chuyแปn ฤแปi z thรnh xรกc suแบฅt:

$$\sigma(z) = \frac{1}{1 + e^{-z}}$$

Vแปi cรกc tham sแป m, b ฤรฃ hแปc vร x = 2.8, vรญ dแปฅ nแบฟu m=2.0 vร b=-4.0, quรก trรฌnh tรญnh toรกn nhฦฐ sau:

$$z = 2.0 \times 2.8 + (-4.0) = 5.6 - 4.0 = 1.6$$

$$\sigma(1.6) = \frac{1}{1 + e^{-1.6}} \approx 0.832$$

Kแบฟt quแบฃ nรy nghฤฉa lร xรกc suแบฅt ฤแบญu xแบฅp xแป 83.2%.

---

### 4๏ธโฃ In Kแบฟt Quแบฃ vร Kแบฟt Luแบญn

```python
print("\n" + "-"*40)

print(f"Kแบฟt quแบฃ dแปฑ ฤoรกn cho {hours_input} giแป hแปc:")
print(f"ฤiแปm sแป dแปฑ ฤoรกn: {predicted_score:.4f}")
print(f"Xรกc suแบฅt ฤแบญu: {predicted_score:.4f} ({predicted_score*100:.2f}%)")

if predicted_score >= 0.5:
    print("=> Kแบฟt luแบญn: ฤแบฌU")
else:
    print("=> Kแบฟt luแบญn: RแปT")

print("-"*40 + "\n")
```

**Giแบฃi thรญch:**

#### **In tiรชu ฤแป**

```python
print("\n" + "-"*40)
```

Lแปnh nรy tแบกo mแปt dรฒng phรขn cรกch bแบฑng cรกch xuแปng dรฒng (`"\n"`) vร sau ฤรณ in 40 dแบฅu gแบกch ngang. Kแบฟt quแบฃ lร mแปt separator ฤแบนp mแบฏt giรบp tรกch biแปt phแบงn kแบฟt quแบฃ khแปi cรกc phแบงn khรกc.

#### **In kแบฟt quแบฃ dแปฑ ฤoรกn**

Phแบงn nรy hiแปn thแป kแบฟt quแบฃ dแปฑ ฤoรกn dฦฐแปi nhiแปu dแบกng khรกc nhau. Lแปnh `print(f"Kแบฟt quแบฃ dแปฑ ฤoรกn cho {hours_input} giแป hแปc:")` in ra sแป giแป input (2.8) ฤแป ngฦฐแปi dรนng biแบฟt ฤang dแปฑ ฤoรกn cho trฦฐแปng hแปฃp nรo. Tiแบฟp theo, `print(f"ฤiแปm sแป dแปฑ ฤoรกn: {predicted_score:.4f}")` in xรกc suแบฅt vแปi 4 chแปฏ sแป thแบญp phรขn (vรญ dแปฅ 0.8324). Cuแปi cรนng, lแปnh `print(f"Xรกc suแบฅt ฤแบญu: {predicted_score:.4f} ({predicted_score*100:.2f}%)")` hiแปn thแป cแบฃ dแบกng thแบญp phรขn (0.8324) vร dแบกng phแบงn trฤm (83.24%), trong ฤรณ `.2f` format sแป phแบงn trฤm vแปi 2 chแปฏ sแป thแบญp phรขn.

#### **Phรขn loแบกi (Classification)**

```python
if predicted_score >= 0.5:
    print("=> Kแบฟt luแบญn: ฤแบฌU")
else:
    print("=> Kแบฟt luแบญn: RแปT")
```

**Ngฦฐแปกng quyแบฟt ฤแปnh (Decision Threshold):**

-   **Xรกc suแบฅt โฅ 0.5:** Dแปฑ ฤoรกn lแปp **dฦฐฦกng** (y=1, ฤแบฌU)
-   **Xรกc suแบฅt < 0.5:** Dแปฑ ฤoรกn lแปp **รขm** (y=0, RแปT)

**Tแบกi sao chแปn 0.5?**

-   0.5 lร **ฤiแปm cรขn bแบฑng** (50-50)
-   Tแบกi ฤiแปm nรy, sigmoid cแบฏt trแปฅc y
-   z = 0 โ sigmoid(0) = 0.5
-   Lร ngฦฐแปกng chuแบฉn cho bรi toรกn cรขn bแบฑng (balanced dataset)

**Cรณ thแป ฤiแปu chแปnh threshold:**

Ngฦฐแปกng quyแบฟt ฤแปnh cรณ thแป ฤฦฐแปฃc ฤiแปu chแปnh tรนy theo mแปฅc ฤรญch sแปญ dแปฅng. Nแบฟu cแบงn "cแบฉn thแบญn hฦกn", cรณ thแป ฤแบทt threshold = 0.7 ฤแป chแป kแบฟt luแบญn ฤแบญu khi rแบฅt chแบฏc chแบฏn. Ngฦฐแปฃc lแบกi, nแบฟu muแปn "dแป dรฃi hฦกn", cรณ thแป ฤแบทt threshold = 0.3 ฤแป dแป dรng kแบฟt luแบญn ฤแบญu hฦกn. Tuy nhiรชn, viแปc ฤiแปu chแปnh nรy tแบกo ra trade-off giแปฏa Precision vร Recall cแบงn cรขn nhแบฏc.

#### **Kแบฟt thรบc**

```python
print("-"*40 + "\n")
```

Dรฒng code nรy in separator ฤรณng vร xuแปng dรฒng ฤแป kแบฟt quแบฃ output trรดng thoรกng mแบฏt hฦกn.

---

## ๐ Phรขn Tรญch Module `logistic_regression_utils.py`

ฤแป hiแปu rรต hฦกn, phรขn tรญch cรกc hรm trong module:

### **1. Dataset Definition**

```python
DATASET = [
    [0.5, 0],
    [1.0, 0],
    [1.5, 0],
    [2.0, 0],
    [2.5, 1],
    [3.0, 1],
    [3.5, 1],
    [4.0, 1]
]
```

**Giแบฃi thรญch:**

ฤรขy lร mแปt danh sรกch 2D trong ฤรณ mแปi phแบงn tแปญ lร `[hours, pass]` ฤแบกi diแปn cho mแปt mแบซu dแปฏ liแปu. Dataset chแปฉa 8 mแบซu vแปi 4 mแบซu rแปt (tแปซ 0.5 ฤแบฟn 2.0 giแป vแปi y=0) vร 4 mแบซu ฤแบญu (tแปซ 2.5 ฤแบฟn 4.0 giแป vแปi y=1).

**Phรขn tรญch dataset:**

Dataset nรy cรณ dแปฏ liแปu cรขn bแบฑng vแปi 50% ฤแบญu vร 50% rแปt. ฤแบทc ฤiแปm nแปi bแบญt lร phรขn chia rรต rรng vแปi khoแบฃng trแปng giแปฏa 2.0 vร 2.5 giแป, do ฤรณ cรณ thแป vแบฝ ฤฦฐแปng phรขn chia khรก tuyแบฟn tรญnh. Decision boundary dแปฑ kiแบฟn sแบฝ nแบฑm khoแบฃng 2.2-2.3 giแป.

---

### **2. Hรm get_prediction**

```python
def get_prediction(m, b, x):
    # Sigmoid function
    y = m * x + b
    return 1 / (1 + math.exp(-y))
```

**Giแบฃi thรญch:**

Hรm nรy thแปฑc hiแปn **forward propagation** (truyแปn tiแบฟn) qua hai bฦฐแปc chรญnh. ฤแบงu tiรชn, tรญnh giรก trแป tuyแบฟn tรญnh:

```python
y = m * x + b
```

Biแบฟn `y` แป ฤรขy thแปฑc ra lร `z` (pre-activation), ฤแบกi diแปn cho phฦฐฦกng trรฌnh ฤฦฐแปng thแบณng:

$$z = mx + b$$

Tiแบฟp theo, รกp dแปฅng sigmoid ฤแป chuyแปn ฤแปi:

```python
return 1 / (1 + math.exp(-y))
```

Cรดng thแปฉc sigmoid:

$$\sigma(z) = \frac{1}{1 + e^{-z}}$$

Hรm nรy chuyแปn giรก trแป z thรnh xรกc suแบฅt trong khoแบฃng (0, 1).

**Vรญ dแปฅ sแปญ dแปฅng:**

```python
m, b = 2.0, -4.0
prob = get_prediction(m, b, 2.8)
# z = 2.0 * 2.8 + (-4.0) = 1.6
# sigmoid(1.6) โ 0.832
# Xรกc suแบฅt ฤแบญu โ 83.2%
```

---

### **3. Hรm get_cost**

```python
def get_cost(y, y_hat):
    # Binary cross-entropy
    k = len(y)
    total_cost = 0.0
    for yi, y_hat_i in zip(y, y_hat):
        total_cost += -(yi * math.log(y_hat_i) + (1 - yi) * math.log(1 - y_hat_i))
    return total_cost / k
```

**Giแบฃi thรญch:**

Hรm tรญnh **Binary Cross-Entropy Loss** tฦฐฦกng tแปฑ Exercise 1 nhฦฐng vแปi cรกch implement khรกc.

#### **Tham sแป:**

Tham sแป **`y`** lร list cรกc nhรฃn thแปฑc tแบฟ nhฦฐ `[0, 0, 0, 0, 1, 1, 1, 1]`, trong khi **`y_hat`** lร list cรกc xรกc suแบฅt dแปฑ ฤoรกn nhฦฐ `[0.1, 0.2, 0.3, 0.4, 0.6, 0.7, 0.8, 0.9]`.

#### **Cรกch tรญnh:**

ฤแบงu tiรชn, ฤแบฟm sแป mแบซu:

```python
k = len(y)
```

Biแบฟn k sแบฝ bแบฑng 8 (sแป mแบซu trong dataset). Tiแบฟp theo, khแปi tแบกo tแปng cost:

```python
total_cost = 0.0
```

Sau ฤรณ duyแปt tแปซng cแบทp (yi, y_hat_i):

```python
for yi, y_hat_i in zip(y, y_hat):
```

Hรm `zip(y, y_hat)` ghรฉp tแปซng cแบทp tฦฐฦกng แปฉng, vรญ dแปฅ (0, 0.1), (0, 0.2), ..., (1, 0.9). Trong mแปi vรฒng lแบทp, cแปng dแปn loss:

```python
total_cost += -(yi * math.log(y_hat_i) + (1 - yi) * math.log(1 - y_hat_i))
```

Nแบฟu yi = 1 thรฌ chแป tรญnh `-log(y_hat_i)`, cรฒn nแบฟu yi = 0 thรฌ chแป tรญnh `-log(1 - y_hat_i)`. Cuแปi cรนng, lแบฅy trung bรฌnh:

```python
return total_cost / k
```

Kแบฟt quแบฃ ฤฦฐแปฃc chia cho k ฤแป lแบฅy giรก trแป trung bรฌnh.

**So sรกnh vแปi Exercise 1:**

|         | Exercise 1 | Exercise 2      |
| ------- | ---------- | --------------- |
| Library | numpy      | math (built-in) |
| Style   | Vectorized | Loop            |
| Epsilon | 1e-15      | Khรดng cรณ        |
| Tแปc ฤแป  | Nhanh hฦกn  | Chแบญm hฦกn        |

**Lฦฐu รฝ:** Exercise 2 khรดng cรณ epsilon โ cรณ thแป gแบทp lแปi `log(0)` nแบฟu y_hat = 0 hoแบทc 1. Trong thแปฑc tแบฟ nรชn thรชm epsilon!

---

### **4. Hรm get_gradients**

```python
def get_gradients(m, b, x, y, y_hat):
    # Calculate gradients
    k = len(y)
    dm = (1 / k) * sum((y_hat_i - yi) * xi for y_hat_i, yi, xi in zip(y_hat, y, x))
    db = (1 / k) * sum(y_hat_i - yi for y_hat_i, yi in zip(y_hat, y))
    return dm, db
```

**Giแบฃi thรญch:**

Hรm tรญnh **gradient** (ฤแบกo hรm) cแปงa Cost function tฦฐฦกng tแปฑ Exercise 1.

#### **Tham sแป:**

Cรกc tham sแป **`m, b`** lร tham sแป hiแปn tแบกi (khรดng dรนng trong hรm nรy, chแป ฤแป tฦฐฦกng thรญch signature). Tham sแป **`x`** lร list giรก trแป features nhฦฐ `[0.5, 1.0, 1.5, ..., 4.0]`, **`y`** lร list nhรฃn thแปฑc tแบฟ `[0, 0, 0, ..., 1]`, vร **`y_hat`** lร list xรกc suแบฅt dแปฑ ฤoรกn `[h1, h2, ..., h8]`.

#### **Cรกch tรญnh:**

ฤแบงu tiรชn, tรญnh gradient cแปงa m (hแป sแป gรณc):

```python
dm = (1 / k) * sum((y_hat_i - yi) * xi for y_hat_i, yi, xi in zip(y_hat, y, x))
```

Cรดng thแปฉc toรกn hแปc:

$$\frac{\partial J}{\partial m} = \frac{1}{k}\sum_{i=1}^{k}(h_i - y_i) \cdot x_i$$

Trong cรดng thแปฉc, `(y_hat_i - yi)` lร sai sแป tแบกi ฤiแปm thแปฉ i, sau ฤรณ nhรขn vแปi `xi` ฤแป tรญnh weighted error. Hรm `sum(...)` tรญnh tแปng trรชn tแบฅt cแบฃ ฤiแปm, rแปi nhรขn vแปi `(1 / k)` ฤแป lแบฅy trung bรฌnh. Generator expression `for y_hat_i, yi, xi in zip(y_hat, y, x)` duyแปt qua 3 list cรนng lรบc mแปt cรกch Pythonic vร gแปn hฦกn vรฒng for thรดng thฦฐแปng.

Tiแบฟp theo, tรญnh gradient cแปงa b (hแป sแป chแบทn):

```python
db = (1 / k) * sum(y_hat_i - yi for y_hat_i, yi in zip(y_hat, y))
```

Cรดng thแปฉc toรกn hแปc:

$$\frac{\partial J}{\partial b} = \frac{1}{k}\sum_{i=1}^{k}(h_i - y_i)$$

Cรกch tรญnh tฦฐฦกng tแปฑ dm nhฦฐng khรดng nhรขn vแปi xi, vรฌ ฤแบกo hรm cแปงa b lร 1.

**Vรญ dแปฅ tรญnh dm:**

```python
x = [0.5, 1.0, 1.5, 2.0]
y = [0, 0, 0, 1]
y_hat = [0.1, 0.2, 0.3, 0.8]

errors = [0.1-0, 0.2-0, 0.3-0, 0.8-1] = [0.1, 0.2, 0.3, -0.2]
weighted = [0.1*0.5, 0.2*1.0, 0.3*1.5, -0.2*2.0] = [0.05, 0.2, 0.45, -0.4]
sum = 0.05 + 0.2 + 0.45 - 0.4 = 0.3
dm = 0.3 / 4 = 0.075
```

---

### **5. Hรm get_accuracy**

```python
def get_accuracy(y, y_hat):
    correct_predictions = sum((1 if y_hat_i >= 0.5 else 0) == yi for y_hat_i, yi in zip(y_hat, y))
    return correct_predictions / len(y)
```

**Giแบฃi thรญch:**

Hรm tรญnh **accuracy** (ฤแป chรญnh xรกc) cแปงa mรด hรฌnh.

#### **Cรกch hoแบกt ฤแปng:**

ฤแบงu tiรชn, chuyแปn xรกc suแบฅt thรnh nhรฃn:

```python
1 if y_hat_i >= 0.5 else 0
```

Nแบฟu y_hat_i โฅ 0.5 thรฌ dแปฑ ฤoรกn 1 (ฤแบฌU), ngฦฐแปฃc lแบกi dแปฑ ฤoรกn 0 (RแปT). Tiแบฟp theo, so sรกnh vแปi nhรฃn thแปฑc:

```python
(... == yi)
```

Kแบฟt quแบฃ trแบฃ vแป True nแบฟu dแปฑ ฤoรกn ฤรบng, False nแบฟu dแปฑ ฤoรกn sai. Sau ฤรณ ฤแบฟm sแป dแปฑ ฤoรกn ฤรบng:

```python
correct_predictions = sum(...)
```

Hรm `sum` trรชn boolean cho True=1 vร False=0, do ฤรณ kแบฟt quแบฃ lร tแปng sแป dแปฑ ฤoรกn ฤรบng. Cuแปi cรนng, tรญnh tแปท lแป:

```python
return correct_predictions / len(y)
```

Kแบฟt quแบฃ lร sแป ฤรบng chia cho tแปng sแป mแบซu, cho giรก trแป tแปซ 0.0 (0%) ฤแบฟn 1.0 (100%).

**Vรญ dแปฅ:**

```python
y = [0, 0, 1, 1, 1]
y_hat = [0.2, 0.6, 0.7, 0.8, 0.3]

# Chuyแปn thรnh nhรฃn:
predictions = [0, 1, 1, 1, 0]

# So sรกnh:
# 0 == 0 โ
# 1 == 0 โ
# 1 == 1 โ
# 1 == 1 โ
# 0 == 1 โ

# ฤแบฟm: 3 ฤรบng / 5 tแปng = 0.6 = 60%
accuracy = 3 / 5 = 0.6
```

---

### **6. Hรm train_logistic_regression**

```python
def train_logistic_regression(dataset=DATASET, m_init=1.0, b_init=-1.0, iterations=10, learning_rate=1.0):
    m = m_init
    b = b_init

    x = [row[0] for row in dataset]
    y = [row[1] for row in dataset]

    costs = []

    for it in range(iterations):
        y_hat = [get_prediction(m, b, xi) for xi in x]

        cost = get_cost(y, y_hat)
        costs.append(cost)

        dm, db = get_gradients(m, b, x, y, y_hat)

        m -= learning_rate * dm
        b -= learning_rate * db

    return m, b, costs
```

**Giแบฃi thรญch:**

ฤรขy lร **hรm chรญnh** thแปฑc hiแปn thuแบญt toรกn Gradient Descent, ฤรณng vai trรฒ trรกi tim cแปงa bรi toรกn.

#### **Tham sแป:**

Hรm nhแบญn tham sแป **`dataset`** lร dแปฏ liแปu training (mแบทc ฤแปnh DATASET), **`m_init, b_init`** lร giรก trแป khแปi tแบกo (mแบทc ฤแปnh 1.0, -1.0), **`iterations`** lร sแป vรฒng lแบทp (mแบทc ฤแปnh 10), vร **`learning_rate`** lร tแปc ฤแป hแปc (mแบทc ฤแปnh 1.0).

#### **Bฦฐแปc 1: Khแปi tแบกo tham sแป**

```python
m = m_init
b = b_init
```

ฤoแบกn code nรy gรกn giรก trแป ban ฤแบงu cho m vร b.

#### **Bฦฐแปc 2: Tรกch dแปฏ liแปu**

```python
x = [row[0] for row in dataset]
y = [row[1] for row in dataset]
```

Sแปญ dแปฅng list comprehension ฤแป trรญch xuแบฅt dแปฏ liแปu. Biแปu thแปฉc `row[0]` lแบฅy cแปt ฤแบงu tiรชn (hours), cรฒn `row[1]` lแบฅy cแปt thแปฉ hai (pass). Kแบฟt quแบฃ:

```python
x = [0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0]
y = [0, 0, 0, 0, 1, 1, 1, 1]
```

#### **Bฦฐแปc 3: Khแปi tแบกo list lฦฐu cost**

```python
costs = []
```

List nรy phแปฅc vแปฅ cho viแปc tracking quรก trรฌnh hแปc.

#### **Bฦฐแปc 4: Vรฒng lแบทp training**

```python
for it in range(iterations):
```

Vรฒng lแบทp nรy thแปฑc hiแปn `iterations` lแบงn (10 lแบงn).

**Bรชn trong mแปi iteration:**

ฤแบงu tiรชn lร **4.1. Forward propagation:**

```python
y_hat = [get_prediction(m, b, xi) for xi in x]
```

Bฦฐแปc nรy tรญnh xรกc suแบฅt dแปฑ ฤoรกn cho tแปซng ฤiแปm, kแบฟt quแบฃ `y_hat` lร list 8 giรก trแป xรกc suแบฅt. Tiแบฟp theo, **4.2. Tรญnh cost:**

```python
cost = get_cost(y, y_hat)
costs.append(cost)
```

ฤoแบกn code nรy ฤรกnh giรก chแบฅt lฦฐแปฃng mรด hรฌnh hiแปn tแบกi vร lฦฐu vรo list. Sau ฤรณ thแปฑc hiแปn **4.3. Backward propagation (tรญnh gradient):**

```python
dm, db = get_gradients(m, b, x, y, y_hat)
```

Bฦฐแปc nรy tรญnh ฤแบกo hรm ฤแป biแบฟt hฦฐแปng ฤi. Cuแปi cรนng lร **4.4. Cแบญp nhแบญt tham sแป:**

```python
m -= learning_rate * dm
b -= learning_rate * db
```

ฤรขy lร Gradient Descent step, ฤi ngฦฐแปฃc hฦฐแปng gradient ฤแป giแบฃm cost.

#### **Bฦฐแปc 5: Trแบฃ vแป kแบฟt quแบฃ**

```python
return m, b, costs
```

Hรm trแบฃ vแป `m, b` lร tham sแป tแปi ฦฐu sau training, vร `costs` lร lแปch sแปญ cost ฤแป phรขn tรญch.

**So sรกnh vแปi Exercise 1:**

|              | Exercise 1       | Exercise 2                |
| ------------ | ---------------- | ------------------------- |
| Hรm training | gradient_descent | train_logistic_regression |
| In progress  | Cรณ               | Khรดng                     |
| Lฦฐu history  | cost, w, b       | Chแป cost                  |
| Return       | w, b, 3 lists    | m, b, 1 list              |

---

## ๐ Output vร Kแบฟt Quแบฃ (Dแปฑ Kiแบฟn)

### ๐ฅ๏ธ Console Output

```
----------------------------------------
Kแบฟt quแบฃ dแปฑ ฤoรกn cho 2.8 giแป hแปc:
ฤiแปm sแป dแปฑ ฤoรกn: 0.7854
Xรกc suแบฅt ฤแบญu: 0.7854 (78.54%)
=> Kแบฟt luแบญn: ฤแบฌU
----------------------------------------
```

**Lฦฐu รฝ:** Giรก trแป cแปฅ thแป phแปฅ thuแปc vรo kแบฟt quแบฃ training thแปฑc tแบฟ.

---

### ๐ Phรขn Tรญch Kแบฟt Quแบฃ

#### **1. Xรกc suแบฅt ฤแบญu: ~78.54%**

**ร nghฤฉa:**

Sinh viรชn hแปc 2.8 giแป cรณ xรกc suแบฅt ฤแบญu gแบงn **80%**. Kแบฟt quแบฃ khรดng phแบฃi 100% vรฌ mรด hรฌnh hแปc tแปซ dแปฏ liแปu cรณ **uncertainty**. Dแปฑ ฤoรกn nรy lร hแปฃp lรฝ do 2.8 giแป lแปn hฦกn 2.5 giแป (ฤiแปm ฤแบญu thแบฅp nhแบฅt) vร gแบงn vแปi 3.0 giแป (ฤiแปm ฤแบญu chแบฏc chแบฏn).

#### **2. Kแบฟt luแบญn: ฤแบฌU**

**Logic:**

Xรกc suแบฅt 0.7854 โฅ 0.5 nรชn ฤฦฐแปฃc phรขn loแบกi vรo lแปp 1 (ฤแบฌU). Mแปฉc ฤแป tin cแบญy lร **cao** vรฌ xรกc suแบฅt gแบงn 80%, khรดng phแบฃi chแป 51%.

#### **3. Phรขn tรญch theo ngฦฐแปกng giแป hแปc**

Giแบฃ sแปญ mรด hรฌnh hแปc ฤฦฐแปฃc m โ 2.0 vร b โ -4.0. Decision boundary ฤฦฐแปฃc tรญnh khi m ร x + b = 0, do ฤรณ x = -b/m = 4.0/2.0 = 2.0 giแป.

**Dแปฑ ฤoรกn theo ngฦฐแปกng:**

Nแบฟu x < 2.0 giแป thรฌ xรกc suแบฅt ฤแบญu < 50% nรชn kแบฟt luแบญn RแปT. Nแบฟu x = 2.0 giแป thรฌ xรกc suแบฅt ฤแบญu = 50% (biรชn giแปi). Nแบฟu x > 2.0 giแป thรฌ xรกc suแบฅt ฤแบญu > 50% nรชn kแบฟt luแบญn ฤแบฌU. Vรฌ **2.8 giแป > 2.0 giแป** nรชn kแบฟt luแบญn ฤแบฌU lร hแปฃp lรฝ!

#### **4. So sรกnh vแปi dแปฏ liแปu thแปฑc**

| Hours   | Actual | Predicted (approx) |
| ------- | ------ | ------------------ |
| 0.5     | RแปT    | RแปT (~5%)          |
| 1.0     | RแปT    | RแปT (~12%)         |
| 1.5     | RแปT    | RแปT (~27%)         |
| 2.0     | RแปT    | Biรชn giแปi (~50%)   |
| 2.5     | ฤแบฌU    | ฤแบฌU (~73%)         |
| **2.8** | ?      | **ฤแบฌU (~79%)**     |
| 3.0     | ฤแบฌU    | ฤแบฌU (~88%)         |
| 3.5     | ฤแบฌU    | ฤแบฌU (~95%)         |
| 4.0     | ฤแบฌU    | ฤแบฌU (~98%)         |

**Nhแบญn xรฉt:**

Mรด hรฌnh dแปฑ ฤoรกn hแปฃp lรฝ vแปi dแปฏ liแปu thแปฑc tแบฟ. Giรก trแป 2.8 giแป nแบฑm giแปฏa 2.5 giแป (73%) vร 3.0 giแป (88%), do ฤรณ kแบฟt quแบฃ ~79% lร hoรn toรn hแปฃp lรฝ.

---
