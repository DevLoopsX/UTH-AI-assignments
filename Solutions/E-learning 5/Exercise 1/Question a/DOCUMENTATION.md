# ğŸ“˜ E-Learning 5 - Exercise 1 - Question A: TÃ­nh HÃ m Chi PhÃ­ J(w,b)

## ğŸ¯ Má»¥c TiÃªu BÃ i Táº­p

BÃ i táº­p yÃªu cáº§u tÃ­nh toÃ¡n giÃ¡ trá»‹ cá»§a **hÃ m chi phÃ­ (Cost Function)** J(w,b) cho bÃ i toÃ¡n **Logistic Regression** vá»›i cÃ¡c tham sá»‘ ban Ä‘áº§u Ä‘Æ°á»£c cho trÆ°á»›c.

### ğŸ“Š Äá» BÃ i

Cho táº­p dá»¯ liá»‡u nhÆ° sau:

| x   | y   |
| --- | --- |
| 0.5 | 0   |
| 1.0 | 0   |
| 1.5 | 0   |
| 3.0 | 1   |
| 2.0 | 1   |
| 1.0 | 1   |

**ThÃ´ng sá»‘ ban Ä‘áº§u:**

-   w (trá»ng sá»‘) = 0
-   b (bias) = 0
-   Î± (learning rate) = 0.0001

**YÃªu cáº§u:** TÃ­nh J(w,b) - HÃ m chi phÃ­ Binary Cross-Entropy

---

## ğŸ’» PhÃ¢n TÃ­ch Source Code Chi Tiáº¿t

### 1ï¸âƒ£ Import ThÆ° Viá»‡n vÃ  Khá»Ÿi Táº¡o Dá»¯ Liá»‡u

```python
import numpy as np
import matplotlib.pyplot as plt

X = np.array([0.5, 1, 1.5, 3, 2, 1])
y = np.array([0, 0, 0, 1, 1, 1])

# Khá»Ÿi táº¡o tham sá»‘ ban Ä‘áº§u cho thuáº­t toÃ¡n
w = 0
b = 0
alpha = 0.0001
```

**Giáº£i thÃ­ch:**

Äoáº¡n code nÃ y thá»±c hiá»‡n viá»‡c **chuáº©n bá»‹ mÃ´i trÆ°á»ng** vÃ  **khá»Ÿi táº¡o dá»¯ liá»‡u** cho bÃ i toÃ¡n Logistic Regression:

-   **`numpy`**: ThÆ° viá»‡n toÃ¡n há»c máº¡nh máº½ cho Python, há»— trá»£ tÃ­nh toÃ¡n vector hÃ³a (vectorization) giÃºp code cháº¡y nhanh hÆ¡n nhiá»u so vá»›i vÃ²ng láº·p thÃ´ng thÆ°á»ng. VÃ­ dá»¥: thay vÃ¬ dÃ¹ng vÃ²ng for Ä‘á»ƒ tÃ­nh toÃ¡n tá»«ng pháº§n tá»­, numpy cÃ³ thá»ƒ thá»±c hiá»‡n phÃ©p toÃ¡n trÃªn toÃ n bá»™ máº£ng cÃ¹ng lÃºc.

-   **`matplotlib.pyplot`**: ThÆ° viá»‡n váº½ Ä‘á»“ thá»‹ chuyÃªn nghiá»‡p trong Python, cho phÃ©p trá»±c quan hÃ³a dá»¯ liá»‡u vÃ  káº¿t quáº£ má»™t cÃ¡ch trá»±c quan, dá»… hiá»ƒu.

-   **Biáº¿n `X`**: Máº£ng numpy chá»©a **6 giÃ¡ trá»‹ Ä‘áº·c trÆ°ng** (features). ÄÃ¢y lÃ  biáº¿n Ä‘á»™c láº­p trong mÃ´ hÃ¬nh, cÃ³ thá»ƒ hiá»ƒu lÃ  cÃ¡c giÃ¡ trá»‹ Ä‘áº§u vÃ o Ä‘á»ƒ dá»± Ä‘oÃ¡n.

-   **Biáº¿n `y`**: Máº£ng numpy chá»©a **6 nhÃ£n** (labels) tÆ°Æ¡ng á»©ng vá»›i tá»«ng giÃ¡ trá»‹ trong X. Vá»›i bÃ i toÃ¡n phÃ¢n loáº¡i nhá»‹ phÃ¢n (binary classification), y chá»‰ nháº­n 2 giÃ¡ trá»‹:

    -   `0`: Thuá»™c lá»›p Ã¢m (negative class)
    -   `1`: Thuá»™c lá»›p dÆ°Æ¡ng (positive class)

-   **Biáº¿n `w` (weight/trá»ng sá»‘)**: LÃ  há»‡ sá»‘ gÃ³c cá»§a Ä‘Æ°á»ng phÃ¢n chia trong khÃ´ng gian Ä‘áº·c trÆ°ng. Khá»Ÿi táº¡o = 0 nghÄ©a lÃ  Ä‘Æ°á»ng tháº³ng ban Ä‘áº§u náº±m ngang, chÆ°a cÃ³ kháº£ nÄƒng phÃ¢n loáº¡i.

-   **Biáº¿n `b` (bias)**: LÃ  há»‡ sá»‘ cháº·n (intercept), xÃ¡c Ä‘á»‹nh vá»‹ trÃ­ cá»§a Ä‘Æ°á»ng phÃ¢n chia dá»‹ch chuyá»ƒn lÃªn/xuá»‘ng. Khá»Ÿi táº¡o = 0 nghÄ©a lÃ  Ä‘Æ°á»ng tháº³ng Ä‘i qua gá»‘c tá»a Ä‘á»™.

-   **Biáº¿n `alpha` (learning rate/tá»‘c Ä‘á»™ há»c)**: LÃ  bÆ°á»›c nháº£y khi cáº­p nháº­t tham sá»‘ trong thuáº­t toÃ¡n Gradient Descent. GiÃ¡ trá»‹ 0.0001 khÃ¡ nhá», giÃºp mÃ´ hÃ¬nh há»c cháº­m nhÆ°ng á»•n Ä‘á»‹nh, trÃ¡nh overshooting (nháº£y quÃ¡ xa khá»i Ä‘iá»ƒm tá»‘i Æ°u).

---

### 2ï¸âƒ£ HÃ m Sigmoid - Activation Function

```python
def sigmoid(z):
    return 1 / (1 + np.exp(-z))
```

**Giáº£i thÃ­ch:**

HÃ m **sigmoid** (cÃ²n gá»i lÃ  **logistic function**) lÃ  trÃ¡i tim cá»§a Logistic Regression. ÄÃ¢y lÃ  má»™t hÃ m kÃ­ch hoáº¡t (activation function) cÃ³ vai trÃ² cá»±c ká»³ quan trá»ng:

**CÃ´ng thá»©c toÃ¡n há»c:**
$$\sigma(z) = \frac{1}{1 + e^{-z}}$$

**Ã nghÄ©a vÃ  Ä‘áº·c Ä‘iá»ƒm:**

1. **Biáº¿n Ä‘á»•i giÃ¡ trá»‹:** HÃ m sigmoid nháº­n Ä‘áº§u vÃ o `z` cÃ³ thá»ƒ lÃ  báº¥t ká»³ sá»‘ thá»±c nÃ o (tá»« Ã¢m vÃ´ cÃ¹ng Ä‘áº¿n dÆ°Æ¡ng vÃ´ cÃ¹ng) vÃ  **nÃ©n** (compress) nÃ³ vÃ o khoáº£ng **(0, 1)**.

2. **Diá»…n giáº£i xÃ¡c suáº¥t:** Do giÃ¡ trá»‹ Ä‘áº§u ra luÃ´n náº±m trong khoáº£ng (0, 1), sigmoid hoÃ n háº£o Ä‘á»ƒ biá»ƒu diá»…n **xÃ¡c suáº¥t**:

    - Äáº§u ra â‰ˆ 0: XÃ¡c suáº¥t thuá»™c lá»›p 1 ráº¥t tháº¥p (gáº§n nhÆ° cháº¯c cháº¯n thuá»™c lá»›p 0)
    - Äáº§u ra â‰ˆ 0.5: KhÃ´ng cháº¯c cháº¯n, xÃ¡c suáº¥t thuá»™c lá»›p 0 vÃ  lá»›p 1 lÃ  báº±ng nhau
    - Äáº§u ra â‰ˆ 1: XÃ¡c suáº¥t thuá»™c lá»›p 1 ráº¥t cao (gáº§n nhÆ° cháº¯c cháº¯n thuá»™c lá»›p 1)

3. **HÃ¬nh dáº¡ng chá»¯ S (S-curve):**

    - Khi z â†’ -âˆ, Ïƒ(z) â†’ 0
    - Khi z = 0, Ïƒ(z) = 0.5 (Ä‘iá»ƒm giá»¯a)
    - Khi z â†’ +âˆ, Ïƒ(z) â†’ 1

4. **TÃ­nh cháº¥t Ä‘áº¡o hÃ m Ä‘áº¹p:** Äáº¡o hÃ m cá»§a sigmoid cÃ³ dáº¡ng Ïƒ'(z) = Ïƒ(z) Ã— (1 - Ïƒ(z)), ráº¥t thuáº­n tiá»‡n cho viá»‡c tÃ­nh gradient trong quÃ¡ trÃ¬nh há»c.

**VÃ­ dá»¥ cá»¥ thá»ƒ:**

-   sigmoid(0) = 0.5
-   sigmoid(5) â‰ˆ 0.993 (gáº§n 1)
-   sigmoid(-5) â‰ˆ 0.007 (gáº§n 0)

**Trong code:** `np.exp(-z)` tÃ­nh e^(-z) (e lÃ  sá»‘ Euler â‰ˆ 2.71828). Viá»‡c sá»­ dá»¥ng numpy giÃºp tÃ­nh toÃ¡n vectorization - cÃ³ thá»ƒ truyá»n vÃ o má»™t máº£ng z vÃ  nháº­n vá» má»™t máº£ng káº¿t quáº£ cÃ¹ng lÃºc.

---

### 3ï¸âƒ£ HÃ m TÃ­nh Chi PhÃ­ (Cost Function)

```python
def compute_cost(X, y, w, b):
    m = len(X) # Sá»‘ lÆ°á»£ng máº«u dá»¯ liá»‡u

    # BÆ°á»›c 1: TÃ­nh giÃ¡ trá»‹ tuyáº¿n tÃ­nh z = w*x + b
    z = w * X + b

    # BÆ°á»›c 2: ÄÆ°a qua hÃ m sigmoid Ä‘á»ƒ cÃ³ giÃ¡ trá»‹ dá»± Ä‘oÃ¡n h (hypothesis)
    h = sigmoid(z)

    # BÆ°á»›c 3: TÃ­nh lá»—i (Loss) báº±ng cÃ´ng thá»©c Binary Cross-Entropy
    # LÆ°u Ã½: ThÃªm 1e-15 (epsilon) Ä‘á»ƒ trÃ¡nh lá»—i toÃ¡n há»c log(0) náº¿u h=0 hoáº·c h=1
    cost = -(1/m) * np.sum(y * np.log(h + 1e-15) + (1 - y) * np.log(1 - h + 1e-15))
    return cost
```

**Giáº£i thÃ­ch:**

ÄÃ¢y lÃ  hÃ m cá»‘t lÃµi Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ **Ä‘á»™ tá»‘t** cá»§a mÃ´ hÃ¬nh Logistic Regression. HÃ m nÃ y tÃ­nh toÃ¡n **Binary Cross-Entropy Loss** - má»™t Ä‘á»™ Ä‘o chuáº©n cho bÃ i toÃ¡n phÃ¢n loáº¡i nhá»‹ phÃ¢n.

**Chi tiáº¿t tá»«ng bÆ°á»›c:**

#### **BÆ°á»›c 1: TÃ­nh giÃ¡ trá»‹ tuyáº¿n tÃ­nh**

```python
z = w * X + b
```

-   **CÃ´ng thá»©c:** $z = w \cdot x + b$ (tÃ­ch vÃ´ hÆ°á»›ng)
-   ÄÃ¢y lÃ  phÆ°Æ¡ng trÃ¬nh Ä‘Æ°á»ng tháº³ng cÆ¡ báº£n trong khÃ´ng gian 1 chiá»u
-   Vá»›i X lÃ  máº£ng, phÃ©p toÃ¡n nÃ y Ä‘Æ°á»£c **vector hÃ³a**: má»i pháº§n tá»­ trong X Ä‘á»u Ä‘Æ°á»£c nhÃ¢n vá»›i w vÃ  cá»™ng vá»›i b
-   **VÃ­ dá»¥:** Náº¿u w=2, b=1, X=[1, 2, 3] thÃ¬ z=[3, 5, 7]

#### **BÆ°á»›c 2: Ãp dá»¥ng hÃ m sigmoid**

```python
h = sigmoid(z)
```

-   Biáº¿n Ä‘á»•i z thÃ nh xÃ¡c suáº¥t dá»± Ä‘oÃ¡n h (hypothesis)
-   h Ä‘áº¡i diá»‡n cho $P(y=1|x;w,b)$ - xÃ¡c suáº¥t Ä‘á»ƒ y=1 khi biáº¿t x vá»›i tham sá»‘ w, b
-   GiÃ¡ trá»‹ h náº±m trong khoáº£ng (0, 1)

#### **BÆ°á»›c 3: TÃ­nh Binary Cross-Entropy**

```python
cost = -(1/m) * np.sum(y * np.log(h + 1e-15) + (1 - y) * np.log(1 - h + 1e-15))
```

**CÃ´ng thá»©c toÃ¡n há»c Ä‘áº§y Ä‘á»§:**

$$J(w,b) = -\frac{1}{m} \sum_{i=1}^{m} \left[ y^{(i)} \log(h^{(i)}) + (1-y^{(i)}) \log(1-h^{(i)}) \right]$$

**PhÃ¢n tÃ­ch cÃ´ng thá»©c:**

1. **TrÆ°á»ng há»£p y = 1 (thá»±c táº¿ lÃ  lá»›p dÆ°Æ¡ng):**

    - Pháº§n Ä‘Ã³ng gÃ³p: $-\log(h)$
    - Náº¿u h gáº§n 1 (dá»± Ä‘oÃ¡n Ä‘Ãºng): $-\log(1) = 0$ â†’ chi phÃ­ tháº¥p âœ“
    - Náº¿u h gáº§n 0 (dá»± Ä‘oÃ¡n sai): $-\log(0) = +\infty$ â†’ chi phÃ­ ráº¥t cao âœ—

2. **TrÆ°á»ng há»£p y = 0 (thá»±c táº¿ lÃ  lá»›p Ã¢m):**

    - Pháº§n Ä‘Ã³ng gÃ³p: $-\log(1-h)$
    - Náº¿u h gáº§n 0 (dá»± Ä‘oÃ¡n Ä‘Ãºng): $-\log(1-0) = 0$ â†’ chi phÃ­ tháº¥p âœ“
    - Náº¿u h gáº§n 1 (dá»± Ä‘oÃ¡n sai): $-\log(0) = +\infty$ â†’ chi phÃ­ ráº¥t cao âœ—

3. **Epsilon (1e-15):**

    - LÃ  má»™t giÃ¡ trá»‹ cá»±c nhá» (0.000000000000001) Ä‘Æ°á»£c thÃªm vÃ o Ä‘á»ƒ **trÃ¡nh lá»—i toÃ¡n há»c**
    - Khi h = 0 hoáº·c h = 1, log(0) khÃ´ng xÃ¡c Ä‘á»‹nh (undefined)
    - ThÃªm epsilon Ä‘áº£m báº£o log luÃ´n tÃ­nh Ä‘Æ°á»£c: log(0 + 1e-15) â‰ˆ -34.5 (sá»‘ Ã¢m lá»›n nhÆ°ng há»¯u háº¡n)

4. **Trung bÃ¬nh (1/m):**
    - Chia cho m Ä‘á»ƒ láº¥y trung bÃ¬nh chi phÃ­ trÃªn táº¥t cáº£ cÃ¡c máº«u
    - GiÃºp so sÃ¡nh cÃ´ng báº±ng giá»¯a cÃ¡c táº­p dá»¯ liá»‡u cÃ³ kÃ­ch thÆ°á»›c khÃ¡c nhau

**Táº¡i sao dÃ¹ng Cross-Entropy thay vÃ¬ Mean Squared Error?**

-   MSE: $(h-y)^2$ â†’ khÃ´ng lá»“i (non-convex) vá»›i sigmoid, nhiá»u local minimum
-   Cross-Entropy: â†’ hÃ m lá»“i (convex), cÃ³ 1 global minimum duy nháº¥t
-   Gradient Descent vá»›i Cross-Entropy há»™i tá»¥ nhanh vÃ  á»•n Ä‘á»‹nh hÆ¡n

---

### 4ï¸âƒ£ Pháº§n In Káº¿t Quáº£ VÃ  Visualization

#### **4.1. Pháº§n Header vÃ  TÃ­nh Cost**

```python
print("=" * 60)
print("BÃ€I 1 - CÃ‚U A: TÃ­nh J(w,b)")
print("=" * 60)
print(f"Tham sá»‘ ban Ä‘áº§u: w = {w}, b = {b}, alpha = {alpha}")
print()
J_wb = compute_cost(X, y, w, b)
```

**Giáº£i thÃ­ch:**

-   In ra tiÃªu Ä‘á» vÃ  thÃ´ng sá»‘ ban Ä‘áº§u Ä‘á»ƒ ngÆ°á»i Ä‘á»c dá»… theo dÃµi
-   Gá»i hÃ m `compute_cost(X, y, w, b)` Ä‘á»ƒ tÃ­nh giÃ¡ trá»‹ hÃ m chi phÃ­ vá»›i tham sá»‘ ban Ä‘áº§u
-   Káº¿t quáº£ Ä‘Æ°á»£c lÆ°u vÃ o biáº¿n `J_wb` (J of w and b)

#### **4.2. Táº¡o Figure vá»›i 2 Subplots**

```python
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
```

**Giáº£i thÃ­ch:**

-   Táº¡o má»™t khung hÃ¬nh (figure) chá»©a **2 biá»ƒu Ä‘á»“ con** (subplots) náº±m ngang
-   `1, 2`: 1 hÃ ng, 2 cá»™t
-   `figsize=(14, 5)`: KÃ­ch thÆ°á»›c 14 inch chiá»u rá»™ng, 5 inch chiá»u cao
-   `ax1`: Biá»ƒu Ä‘á»“ bÃªn trÃ¡i (dá»¯ liá»‡u vÃ  sigmoid)
-   `ax2`: Biá»ƒu Ä‘á»“ bÃªn pháº£i (cost function surface)

---

#### **4.3. Biá»ƒu Äá»“ 1: Dá»¯ Liá»‡u vÃ  Sigmoid Function**

```python
# Táº¡o dáº£i giÃ¡ trá»‹ x mÆ°á»£t mÃ  tá»« 0 Ä‘áº¿n 3.5 Ä‘á»ƒ váº½ Ä‘Æ°á»ng cong sigmoid
x_plot = np.linspace(0, 3.5, 100)
z_plot = w * x_plot + b
y_plot = sigmoid(z_plot)
```

**Giáº£i thÃ­ch:**

-   `np.linspace(0, 3.5, 100)`: Táº¡o 100 Ä‘iá»ƒm cÃ¡ch Ä‘á»u nhau tá»« 0 Ä‘áº¿n 3.5
-   TÃ­nh z vÃ  y tÆ°Æ¡ng á»©ng Ä‘á»ƒ váº½ Ä‘Æ°á»ng cong sigmoid **mÆ°á»£t mÃ ** (khÃ´ng bá»‹ gÃ³c cáº¡nh)

```python
# Váº½ cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u thá»±c táº¿
ax1.scatter(X[y == 0], y[y == 0], color='blue', s=150, marker='o',
            label='Class 0 (y=0)', edgecolors='black', linewidth=2)
ax1.scatter(X[y == 1], y[y == 1], color='red', s=150, marker='s',
            label='Class 1 (y=1)', edgecolors='black', linewidth=2)
```

**Giáº£i thÃ­ch:**

-   `X[y == 0]`: Lá»c cÃ¡c Ä‘iá»ƒm cÃ³ nhÃ£n y = 0 (lá»›p Ã¢m)

    -   Váº½ mÃ u **xanh**, hÃ¬nh **trÃ²n** (marker='o')
    -   `s=150`: KÃ­ch thÆ°á»›c Ä‘iá»ƒm
    -   `edgecolors='black'`: Viá»n mÃ u Ä‘en Ä‘á»ƒ dá»… phÃ¢n biá»‡t

-   `X[y == 1]`: Lá»c cÃ¡c Ä‘iá»ƒm cÃ³ nhÃ£n y = 1 (lá»›p dÆ°Æ¡ng)
    -   Váº½ mÃ u **Ä‘á»**, hÃ¬nh **vuÃ´ng** (marker='s')
    -   GiÃºp phÃ¢n biá»‡t rÃµ rÃ ng 2 lá»›p

```python
# Váº½ Ä‘Æ°á»ng dá»± Ä‘oÃ¡n Sigmoid
ax1.plot(x_plot, y_plot, 'g-', linewidth=2.5,
         label=f'Sigmoid: h(x) = Ïƒ({w}x + {b})')
```

**Giáº£i thÃ­ch:**

-   Váº½ Ä‘Æ°á»ng cong sigmoid vá»›i:
    -   `'g-'`: MÃ u xanh lÃ¡, Ä‘Æ°á»ng liá»n
    -   `linewidth=2.5`: Äá»™ dÃ y 2.5
-   **Vá»›i w=0, b=0:** ÄÆ°á»ng sigmoid sáº½ lÃ  Ä‘Æ°á»ng tháº³ng ngang táº¡i y=0.5 (vÃ¬ z=0 â†’ sigmoid(0)=0.5 cho má»i x)

```python
# Váº½ Ä‘Æ°á»ng biÃªn quyáº¿t Ä‘á»‹nh (Decision Boundary)
ax1.axhline(y=0.5, color='orange', linestyle='--', linewidth=2,
            label='Decision Boundary (h=0.5)')
```

**Giáº£i thÃ­ch:**

-   `axhline`: Váº½ Ä‘Æ°á»ng ngang (horizontal line)
-   **Decision Boundary** táº¡i h=0.5: NgÆ°á»¡ng phÃ¢n loáº¡i
    -   Náº¿u h â‰¥ 0.5 â†’ dá»± Ä‘oÃ¡n y=1
    -   Náº¿u h < 0.5 â†’ dá»± Ä‘oÃ¡n y=0
-   ÄÆ°á»ng nÃ y giÃºp tháº¥y rÃµ mÃ´ hÃ¬nh Ä‘ang phÃ¢n loáº¡i cÃ¡c Ä‘iá»ƒm nhÆ° tháº¿ nÃ o

```python
# Gáº¯n nhÃ£n toáº¡ Ä‘á»™ lÃªn tá»«ng Ä‘iá»ƒm
for i, (xi, yi) in enumerate(zip(X, y)):
    ax1.annotate(f'({xi}, {yi})', (xi, yi),
                textcoords="offset points", xytext=(0,10),
                ha='center', fontsize=9)
```

**Giáº£i thÃ­ch:**

-   Duyá»‡t qua tá»«ng cáº·p (xi, yi) trong dá»¯ liá»‡u
-   `annotate`: Gáº¯n nhÃ£n vÄƒn báº£n lÃªn biá»ƒu Ä‘á»“
-   `xytext=(0,10)`: Äáº·t text á»Ÿ vá»‹ trÃ­ cÃ¡ch Ä‘iá»ƒm dá»¯ liá»‡u 10 pixels vá» phÃ­a trÃªn
-   `ha='center'`: CÄƒn giá»¯a text theo chiá»u ngang
-   GiÃºp ngÆ°á»i xem dá»… Ä‘á»c giÃ¡ trá»‹ chÃ­nh xÃ¡c cá»§a tá»«ng Ä‘iá»ƒm

```python
# Trang trÃ­ biá»ƒu Ä‘á»“ 1
ax1.set_xlabel('x', fontsize=13, fontweight='bold')
ax1.set_ylabel('y', fontsize=13, fontweight='bold')
ax1.set_title(f'Dá»¯ liá»‡u vÃ  Sigmoid Function\nJ(w={w}, b={b}) = {J_wb:.8f}',
              fontsize=14, fontweight='bold')
ax1.legend(fontsize=10, loc='best')
ax1.grid(True, alpha=0.3, linestyle='--')
ax1.set_ylim([-0.1, 1.1])
ax1.set_xlim([0, 3.5])
```

**Giáº£i thÃ­ch:**

-   Äáº·t nhÃ£n trá»¥c x, y vá»›i font Ä‘áº­m (bold), kÃ­ch thÆ°á»›c 13
-   TiÃªu Ä‘á» hiá»ƒn thá»‹ giÃ¡ trá»‹ J(w,b) vá»›i 8 chá»¯ sá»‘ tháº­p phÃ¢n
-   `legend`: Hiá»ƒn thá»‹ chÃº thÃ­ch cÃ¡c Ä‘Æ°á»ng/Ä‘iá»ƒm, tá»± Ä‘á»™ng tÃ¬m vá»‹ trÃ­ tá»‘t nháº¥t (loc='best')
-   `grid`: LÆ°á»›i ná»n vá»›i Ä‘á»™ trong suá»‘t 0.3, Ä‘Æ°á»ng gáº¡ch ngang
-   `set_ylim/xlim`: Giá»›i háº¡n trá»¥c Ä‘á»ƒ biá»ƒu Ä‘á»“ thoÃ¡ng, khÃ´ng bá»‹ sÃ¡t mÃ©p

---

#### **4.4. Biá»ƒu Äá»“ 2: Máº·t Pháº³ng Cost Function**

```python
# Táº¡o lÆ°á»›i toáº¡ Ä‘á»™ (mesh grid) cho w vÃ  b
w_range = np.linspace(-2, 2, 50)
b_range = np.linspace(-2, 2, 50)
W, B = np.meshgrid(w_range, b_range)
Z = np.zeros_like(W)
```

**Giáº£i thÃ­ch:**

-   Táº¡o 50 giÃ¡ trá»‹ w tá»« -2 Ä‘áº¿n 2
-   Táº¡o 50 giÃ¡ trá»‹ b tá»« -2 Ä‘áº¿n 2
-   `meshgrid`: Táº¡o lÆ°á»›i 2D, má»—i Ä‘iá»ƒm trÃªn lÆ°á»›i lÃ  má»™t cáº·p (w, b)
-   `Z`: Ma tráº­n 50Ã—50 Ä‘á»ƒ lÆ°u giÃ¡ trá»‹ Cost táº¡i má»—i Ä‘iá»ƒm (w, b)

```python
# TÃ­nh Cost cho tá»«ng Ä‘iá»ƒm trÃªn lÆ°á»›i
for i in range(len(w_range)):
    for j in range(len(b_range)):
        Z[j, i] = compute_cost(X, y, W[j, i], B[j, i])
```

**Giáº£i thÃ­ch:**

-   Duyá»‡t qua táº¥t cáº£ 2500 cáº·p (w, b) trÃªn lÆ°á»›i
-   TÃ­nh Cost táº¡i má»—i Ä‘iá»ƒm vÃ  lÆ°u vÃ o ma tráº­n Z
-   Táº¡o ra "báº£n Ä‘á»“ Ä‘á»‹a hÃ¬nh" cá»§a hÃ m Cost: vÃ¹ng trÅ©ng lÃ  Ä‘iá»ƒm tá»‘i Æ°u

```python
# Váº½ Ä‘Æ°á»ng Ä‘á»“ng má»©c (Contour plot)
contour = ax2.contour(W, B, Z, levels=20, cmap='viridis')
ax2.clabel(contour, inline=True, fontsize=8)
```

**Giáº£i thÃ­ch:**

-   `contour`: Váº½ Ä‘Æ°á»ng Ä‘á»“ng má»©c (nhÆ° Ä‘Æ°á»ng bÃ¬nh Ä‘á»™ trÃªn báº£n Ä‘á»“ Ä‘á»‹a lÃ½)
-   `levels=20`: Váº½ 20 Ä‘Æ°á»ng má»©c khÃ¡c nhau
-   `cmap='viridis'`: Báº£ng mÃ u tá»« tÃ­m Ä‘áº­m (cao) Ä‘áº¿n vÃ ng (tháº¥p)
-   `clabel`: Hiá»ƒn thá»‹ sá»‘ liá»‡u trÃªn Ä‘Æ°á»ng Ä‘á»“ng má»©c Ä‘á»ƒ biáº¿t giÃ¡ trá»‹ Cost

```python
# ÄÃ¡nh dáº¥u vá»‹ trÃ­ hiá»‡n táº¡i (w=0, b=0)
ax2.plot(w, b, 'r*', markersize=20, label=f'(w={w}, b={b})')
```

**Giáº£i thÃ­ch:**

-   Váº½ ngÃ´i sao Ä‘á» táº¡i vá»‹ trÃ­ (w=0, b=0)
-   Cho tháº¥y Ä‘iá»ƒm khá»Ÿi Ä‘áº§u Ä‘ang á»Ÿ Ä‘Ã¢u trÃªn "báº£n Ä‘á»“" Cost
-   GiÃºp hiá»ƒu vá»‹ trÃ­ hiá»‡n táº¡i so vá»›i Ä‘iá»ƒm tá»‘i Æ°u (vÃ¹ng trÅ©ng nháº¥t)

---

#### **4.5. LÆ°u vÃ  Hiá»ƒn Thá»‹**

```python
plt.tight_layout()
plt.savefig('results/ex1a_cost_function_visualization.png',
            dpi=300, bbox_inches='tight')
plt.show()
```

**Giáº£i thÃ­ch:**

-   `tight_layout()`: Tá»± Ä‘á»™ng Ä‘iá»u chá»‰nh khoáº£ng cÃ¡ch giá»¯a cÃ¡c subplot Ä‘á»ƒ khÃ´ng bá»‹ chá»“ng láº¥n
-   `savefig`: LÆ°u hÃ¬nh vá»›i Ä‘á»™ phÃ¢n giáº£i cao (300 DPI - cháº¥t lÆ°á»£ng in áº¥n)
-   `bbox_inches='tight'`: Cáº¯t bá» khoáº£ng tráº¯ng thá»«a xung quanh
-   `show()`: Hiá»ƒn thá»‹ biá»ƒu Ä‘á»“ lÃªn mÃ n hÃ¬nh

---

#### **4.6. In Chi Tiáº¿t QuÃ¡ TrÃ¬nh TÃ­nh ToÃ¡n**

```python
print(f"Sá»‘ máº«u dá»¯ liá»‡u (m): {len(X)}")
print(f"\nDá»¯ liá»‡u tá»«ng Ä‘iá»ƒm:")

for i, (xi, yi) in enumerate(zip(X, y)):
    z_i = w * xi + b
    h_i = sigmoid(z_i)
    print(f"  x[{i}] = {xi}, y[{i}] = {yi} => z = {z_i:.1f}, h(x) = {h_i:.4f}")

print(f"\nCÃ´ng thá»©c Cost function: J(w,b) = -(1/m) * Î£[y*log(h) + (1-y)*log(1-h)]")
print(f"Káº¿t quáº£ cuá»‘i cÃ¹ng: J({w}, {b}) = {J_wb:.8f}")
print("=" * 60)
```

**Giáº£i thÃ­ch:**

-   In sá»‘ lÆ°á»£ng máº«u dá»¯ liá»‡u (m = 6)
-   Duyá»‡t qua tá»«ng Ä‘iá»ƒm, tÃ­nh vÃ  in:
    -   `z_i`: GiÃ¡ trá»‹ tuyáº¿n tÃ­nh táº¡i Ä‘iá»ƒm thá»© i
    -   `h_i`: XÃ¡c suáº¥t dá»± Ä‘oÃ¡n sau khi qua sigmoid
-   Hiá»ƒn thá»‹ cÃ´ng thá»©c Cost function Ä‘á»ƒ ngÆ°á»i Ä‘á»c hiá»ƒu rÃµ
-   In káº¿t quáº£ cuá»‘i cÃ¹ng vá»›i 8 chá»¯ sá»‘ tháº­p phÃ¢n

---

## ğŸ“Š Output vÃ  Káº¿t Quáº£

### ğŸ–¥ï¸ Console Output

```
============================================================
BÃ€I 1 - CÃ‚U A: TÃ­nh J(w,b)
============================================================
Tham sá»‘ ban Ä‘áº§u: w = 0, b = 0, alpha = 0.0001

Sá»‘ máº«u dá»¯ liá»‡u (m): 6

Dá»¯ liá»‡u tá»«ng Ä‘iá»ƒm:
  x[0] = 0.5, y[0] = 0 => z = 0.0, h(x) = 0.5000
  x[1] = 1.0, y[1] = 0 => z = 0.0, h(x) = 0.5000
  x[2] = 1.5, y[2] = 0 => z = 0.0, h(x) = 0.5000
  x[3] = 3.0, y[3] = 1 => z = 0.0, h(x) = 0.5000
  x[4] = 2.0, y[4] = 1 => z = 0.0, h(x) = 0.5000
  x[5] = 1.0, y[5] = 1 => z = 0.0, h(x) = 0.5000

CÃ´ng thá»©c Cost function: J(w,b) = -(1/m) * Î£[y*log(h) + (1-y)*log(1-h)]
Káº¿t quáº£ cuá»‘i cÃ¹ng: J(0, 0) = 0.69314718
============================================================
```

### ğŸ“ˆ PhÃ¢n TÃ­ch Káº¿t Quáº£

#### **1. GiÃ¡ trá»‹ J(0, 0) = 0.69314718**

ÄÃ¢y lÃ  giÃ¡ trá»‹ hÃ m chi phÃ­ khi mÃ´ hÃ¬nh **chÆ°a Ä‘Æ°á»£c huáº¥n luyá»‡n** (w=0, b=0):

-   **Ã nghÄ©a:** MÃ´ hÃ¬nh Ä‘ang dá»± Ä‘oÃ¡n xÃ¡c suáº¥t 0.5 cho má»i Ä‘iá»ƒm (hoÃ n toÃ n ngáº«u nhiÃªn)
-   **So sÃ¡nh vá»›i log(2):**
    -   $\ln(2) = 0.693147...$
    -   GiÃ¡ trá»‹ J(0,0) gáº§n báº±ng ln(2) khÃ´ng pháº£i ngáº«u nhiÃªn!

**Giáº£i thÃ­ch toÃ¡n há»c:**

Khi h = 0.5 cho má»i Ä‘iá»ƒm:

$$J = -\frac{1}{m}\sum_{i=1}^{m}[y_i\log(0.5) + (1-y_i)\log(0.5)]$$

$$= -\frac{1}{m}\sum_{i=1}^{m}\log(0.5)$$

$$= -\log(0.5) = \log(2) \approx 0.693$$

ÄÃ¢y lÃ  **chi phÃ­ tá»‘i Ä‘a** cá»§a má»™t mÃ´ hÃ¬nh phÃ¢n loáº¡i nhá»‹ phÃ¢n khi dá»± Ä‘oÃ¡n hoÃ n toÃ n ngáº«u nhiÃªn (50-50).

#### **2. Táº¡i sao h(x) = 0.5 cho má»i x?**

Vá»›i w=0, b=0:

-   $z = 0 \cdot x + 0 = 0$ (cho má»i x)
-   $h(x) = \sigma(0) = \frac{1}{1+e^0} = \frac{1}{2} = 0.5$

ÄÆ°á»ng sigmoid lÃ  Ä‘Æ°á»ng **tháº³ng ngang** táº¡i y=0.5, khÃ´ng cÃ³ kháº£ nÄƒng phÃ¢n loáº¡i.

#### **3. Biá»ƒu Ä‘á»“ 1: Dá»¯ liá»‡u vÃ  Sigmoid**

Biá»ƒu Ä‘á»“ nÃ y cho tháº¥y:

-   **CÃ¡c Ä‘iá»ƒm xanh (y=0)** á»Ÿ phÃ­a trÃ¡i (x nhá»)
-   **CÃ¡c Ä‘iá»ƒm Ä‘á» (y=1)** á»Ÿ phÃ­a pháº£i (x lá»›n)
-   **ÄÆ°á»ng sigmoid mÃ u xanh lÃ¡** lÃ  Ä‘Æ°á»ng ngang táº¡i 0.5 (do w=0, b=0)
-   **ÄÆ°á»ng cam gáº¡ch ngang** lÃ  decision boundary (h=0.5)

**Nháº­n xÃ©t:** MÃ´ hÃ¬nh hiá»‡n táº¡i **khÃ´ng phÃ¢n loáº¡i Ä‘Æ°á»£c gÃ¬** vÃ¬ táº¥t cáº£ Ä‘iá»ƒm Ä‘á»u Ä‘Æ°á»£c dá»± Ä‘oÃ¡n xÃ¡c suáº¥t 0.5.

#### **4. Biá»ƒu Ä‘á»“ 2: Cost Function Surface**

Biá»ƒu Ä‘á»“ nÃ y thá»ƒ hiá»‡n "Ä‘á»‹a hÃ¬nh" cá»§a hÃ m Cost trong khÃ´ng gian (w, b):

-   **MÃ u tÃ­m Ä‘áº­m:** VÃ¹ng cÃ³ Cost cao (mÃ´ hÃ¬nh tá»‡)
-   **MÃ u vÃ ng:** VÃ¹ng cÃ³ Cost tháº¥p (mÃ´ hÃ¬nh tá»‘t)
-   **NgÃ´i sao Ä‘á» táº¡i (0,0):** Vá»‹ trÃ­ ban Ä‘áº§u
-   **VÃ¹ng trÅ©ng (valley):** HÆ°á»›ng Ä‘áº¿n Ä‘iá»ƒm tá»‘i Æ°u

**Quan sÃ¡t:**

-   Äiá»ƒm (0,0) náº±m á»Ÿ vÃ¹ng cÃ³ Cost â‰ˆ 0.693 (khÃ´ng pháº£i tá»‡ nháº¥t nhÆ°ng cÅ©ng khÃ´ng tá»‘t)
-   CÃ³ má»™t vÃ¹ng trÅ©ng rÃµ rÃ ng hÆ°á»›ng vá» phÃ­a w dÆ°Æ¡ng, b Ã¢m
-   ÄÃ¢y lÃ  hÆ°á»›ng mÃ  Gradient Descent sáº½ Ä‘i Ä‘á»ƒ giáº£m Cost

---
