# ğŸ“˜ E-Learning 5 - Exercise 1 - Question A: TÃ­nh HÃ m Chi PhÃ­ J(w,b)

## ğŸ¯ Má»¥c TiÃªu BÃ i Táº­p

BÃ i táº­p yÃªu cáº§u tÃ­nh toÃ¡n giÃ¡ trá»‹ cá»§a **hÃ m chi phÃ­ (Cost Function)** J(w,b) cho bÃ i toÃ¡n **Logistic Regression** vá»›i cÃ¡c tham sá»‘ ban Ä‘áº§u Ä‘Æ°á»£c cho trÆ°á»›c.

### ğŸ“Š Äá» BÃ i

Cho táº­p dá»¯ liá»‡u nhÆ° sau:

| x   | y   |
| --- | --- |
| 0.5 | 0   |
| 1.0 | 0   |
| 1.5 | 0   |
| 3.0 | 1   |
| 2.0 | 1   |
| 1.0 | 1   |

**ThÃ´ng sá»‘ ban Ä‘áº§u:**

-   w (trá»ng sá»‘) = 0
-   b (bias) = 0
-   Î± (learning rate) = 0.0001

**YÃªu cáº§u:** TÃ­nh J(w,b) - HÃ m chi phÃ­ Binary Cross-Entropy

---

## ğŸ’» PhÃ¢n TÃ­ch Source Code Chi Tiáº¿t

### 1ï¸âƒ£ Import ThÆ° Viá»‡n vÃ  Khá»Ÿi Táº¡o Dá»¯ Liá»‡u

```python
import numpy as np
import matplotlib.pyplot as plt

X = np.array([0.5, 1, 1.5, 3, 2, 1])
y = np.array([0, 0, 0, 1, 1, 1])

# Khá»Ÿi táº¡o tham sá»‘ ban Ä‘áº§u cho thuáº­t toÃ¡n
w = 0
b = 0
alpha = 0.0001
```

**Giáº£i thÃ­ch:**

Äoáº¡n code nÃ y thá»±c hiá»‡n viá»‡c chuáº©n bá»‹ mÃ´i trÆ°á»ng vÃ  khá»Ÿi táº¡o dá»¯ liá»‡u cho bÃ i toÃ¡n Logistic Regression. Äáº§u tiÃªn, hai thÆ° viá»‡n quan trá»ng Ä‘Æ°á»£c import: `numpy` lÃ  thÆ° viá»‡n toÃ¡n há»c máº¡nh máº½ há»— trá»£ tÃ­nh toÃ¡n vector hÃ³a (vectorization), giÃºp code cháº¡y nhanh hÆ¡n nhiá»u so vá»›i vÃ²ng láº·p thÃ´ng thÆ°á»ng - thay vÃ¬ dÃ¹ng vÃ²ng for Ä‘á»ƒ tÃ­nh toÃ¡n tá»«ng pháº§n tá»­, numpy cÃ³ thá»ƒ thá»±c hiá»‡n phÃ©p toÃ¡n trÃªn toÃ n bá»™ máº£ng cÃ¹ng lÃºc; `matplotlib.pyplot` lÃ  thÆ° viá»‡n váº½ Ä‘á»“ thá»‹ chuyÃªn nghiá»‡p trong Python, cho phÃ©p trá»±c quan hÃ³a dá»¯ liá»‡u vÃ  káº¿t quáº£ má»™t cÃ¡ch trá»±c quan, dá»… hiá»ƒu.

Tiáº¿p theo, dá»¯ liá»‡u Ä‘Æ°á»£c khá»Ÿi táº¡o vá»›i biáº¿n `X` lÃ  máº£ng numpy chá»©a 6 giÃ¡ trá»‹ Ä‘áº·c trÆ°ng (features) - Ä‘Ã¢y lÃ  biáº¿n Ä‘á»™c láº­p trong mÃ´ hÃ¬nh, cÃ³ thá»ƒ hiá»ƒu lÃ  cÃ¡c giÃ¡ trá»‹ Ä‘áº§u vÃ o Ä‘á»ƒ dá»± Ä‘oÃ¡n. Biáº¿n `y` lÃ  máº£ng numpy chá»©a 6 nhÃ£n (labels) tÆ°Æ¡ng á»©ng vá»›i tá»«ng giÃ¡ trá»‹ trong X. Vá»›i bÃ i toÃ¡n phÃ¢n loáº¡i nhá»‹ phÃ¢n (binary classification), y chá»‰ nháº­n 2 giÃ¡ trá»‹: `0` thuá»™c lá»›p Ã¢m (negative class) vÃ  `1` thuá»™c lá»›p dÆ°Æ¡ng (positive class).

Vá» cÃ¡c tham sá»‘ cá»§a mÃ´ hÃ¬nh, biáº¿n `w` (weight/trá»ng sá»‘) lÃ  há»‡ sá»‘ gÃ³c cá»§a Ä‘Æ°á»ng phÃ¢n chia trong khÃ´ng gian Ä‘áº·c trÆ°ng - khá»Ÿi táº¡o báº±ng 0 nghÄ©a lÃ  Ä‘Æ°á»ng tháº³ng ban Ä‘áº§u náº±m ngang, chÆ°a cÃ³ kháº£ nÄƒng phÃ¢n loáº¡i. Biáº¿n `b` (bias) lÃ  há»‡ sá»‘ cháº·n (intercept), xÃ¡c Ä‘á»‹nh vá»‹ trÃ­ cá»§a Ä‘Æ°á»ng phÃ¢n chia dá»‹ch chuyá»ƒn lÃªn/xuá»‘ng - khá»Ÿi táº¡o báº±ng 0 nghÄ©a lÃ  Ä‘Æ°á»ng tháº³ng Ä‘i qua gá»‘c tá»a Ä‘á»™. Cuá»‘i cÃ¹ng, biáº¿n `alpha` (learning rate/tá»‘c Ä‘á»™ há»c) lÃ  bÆ°á»›c nháº£y khi cáº­p nháº­t tham sá»‘ trong thuáº­t toÃ¡n Gradient Descent - giÃ¡ trá»‹ 0.0001 khÃ¡ nhá», giÃºp mÃ´ hÃ¬nh há»c cháº­m nhÆ°ng á»•n Ä‘á»‹nh, trÃ¡nh overshooting (nháº£y quÃ¡ xa khá»i Ä‘iá»ƒm tá»‘i Æ°u).

---

### 2ï¸âƒ£ HÃ m Sigmoid - Activation Function

```python
def sigmoid(z):
    return 1 / (1 + np.exp(-z))
```

**Giáº£i thÃ­ch:**

HÃ m sigmoid (cÃ²n gá»i lÃ  logistic function) lÃ  trÃ¡i tim cá»§a Logistic Regression, Ä‘Ã¢y lÃ  má»™t hÃ m kÃ­ch hoáº¡t (activation function) cÃ³ vai trÃ² cá»±c ká»³ quan trá»ng. HÃ m nÃ y hoáº¡t Ä‘á»™ng theo cÃ´ng thá»©c toÃ¡n há»c:

$$\sigma(z) = \frac{1}{1 + e^{-z}}$$

Trong Ä‘Ã³ `z` cÃ³ thá»ƒ lÃ  báº¥t ká»³ sá»‘ thá»±c nÃ o tá»« Ã¢m vÃ´ cÃ¹ng Ä‘áº¿n dÆ°Æ¡ng vÃ´ cÃ¹ng, vÃ  hÃ m sigmoid sáº½ "nÃ©n" (compress) giÃ¡ trá»‹ nÃ y vÃ o khoáº£ng (0, 1).

Äáº·c Ä‘iá»ƒm quan trá»ng nháº¥t cá»§a sigmoid lÃ  kháº£ nÄƒng diá»…n giáº£i xÃ¡c suáº¥t. Do giÃ¡ trá»‹ Ä‘áº§u ra luÃ´n náº±m trong khoáº£ng (0, 1), sigmoid hoÃ n háº£o Ä‘á»ƒ biá»ƒu diá»…n xÃ¡c suáº¥t: khi Ä‘áº§u ra gáº§n 0 nghÄ©a lÃ  xÃ¡c suáº¥t thuá»™c lá»›p 1 ráº¥t tháº¥p (gáº§n nhÆ° cháº¯c cháº¯n thuá»™c lá»›p 0), khi Ä‘áº§u ra gáº§n 0.5 thÃ¬ khÃ´ng cháº¯c cháº¯n (xÃ¡c suáº¥t thuá»™c lá»›p 0 vÃ  lá»›p 1 báº±ng nhau), vÃ  khi Ä‘áº§u ra gáº§n 1 thÃ¬ xÃ¡c suáº¥t thuá»™c lá»›p 1 ráº¥t cao (gáº§n nhÆ° cháº¯c cháº¯n thuá»™c lá»›p 1).

HÃ m sigmoid cÃ³ hÃ¬nh dáº¡ng chá»¯ S (S-curve) Ä‘áº·c trÆ°ng. Khi z tiáº¿n vá» Ã¢m vÃ´ cÃ¹ng thÃ¬ Ïƒ(z) tiáº¿n vá» 0, khi z = 0 thÃ¬ Ïƒ(z) = 0.5 (Ä‘iá»ƒm giá»¯a), vÃ  khi z tiáº¿n vá» dÆ°Æ¡ng vÃ´ cÃ¹ng thÃ¬ Ïƒ(z) tiáº¿n vá» 1. VÃ­ dá»¥ cá»¥ thá»ƒ, sigmoid(0) = 0.5, sigmoid(5) â‰ˆ 0.993 (gáº§n 1), vÃ  sigmoid(-5) â‰ˆ 0.007 (gáº§n 0). Má»™t tÃ­nh cháº¥t quan trá»ng khÃ¡c lÃ  Ä‘áº¡o hÃ m cá»§a sigmoid cÃ³ dáº¡ng:

$$\sigma'(z) = \sigma(z) \times (1 - \sigma(z))$$

Äiá»u nÃ y ráº¥t thuáº­n tiá»‡n cho viá»‡c tÃ­nh gradient trong quÃ¡ trÃ¬nh há»c.

Trong code, `np.exp(-z)` tÃ­nh e^(-z) vá»›i e lÃ  sá»‘ Euler (â‰ˆ 2.71828). Viá»‡c sá»­ dá»¥ng numpy giÃºp tÃ­nh toÃ¡n vectorization, nghÄ©a lÃ  cÃ³ thá»ƒ truyá»n vÃ o má»™t máº£ng z vÃ  nháº­n vá» má»™t máº£ng káº¿t quáº£ cÃ¹ng lÃºc, giÃºp code cháº¡y nhanh hÆ¡n ráº¥t nhiá»u.

---

### 3ï¸âƒ£ HÃ m TÃ­nh Chi PhÃ­ (Cost Function)

```python
def compute_cost(X, y, w, b):
    m = len(X) # Sá»‘ lÆ°á»£ng máº«u dá»¯ liá»‡u

    # BÆ°á»›c 1: TÃ­nh giÃ¡ trá»‹ tuyáº¿n tÃ­nh z = w*x + b
    z = w * X + b

    # BÆ°á»›c 2: ÄÆ°a qua hÃ m sigmoid Ä‘á»ƒ cÃ³ giÃ¡ trá»‹ dá»± Ä‘oÃ¡n h (hypothesis)
    h = sigmoid(z)

    # BÆ°á»›c 3: TÃ­nh lá»—i (Loss) báº±ng cÃ´ng thá»©c Binary Cross-Entropy
    # LÆ°u Ã½: ThÃªm 1e-15 (epsilon) Ä‘á»ƒ trÃ¡nh lá»—i toÃ¡n há»c log(0) náº¿u h=0 hoáº·c h=1
    cost = -(1/m) * np.sum(y * np.log(h + 1e-15) + (1 - y) * np.log(1 - h + 1e-15))
    return cost
```

**Giáº£i thÃ­ch:**

ÄÃ¢y lÃ  hÃ m cá»‘t lÃµi Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ Ä‘á»™ tá»‘t cá»§a mÃ´ hÃ¬nh Logistic Regression, hÃ m nÃ y tÃ­nh toÃ¡n Binary Cross-Entropy Loss - má»™t Ä‘á»™ Ä‘o chuáº©n cho bÃ i toÃ¡n phÃ¢n loáº¡i nhá»‹ phÃ¢n. QuÃ¡ trÃ¬nh tÃ­nh toÃ¡n diá»…n ra qua ba bÆ°á»›c chÃ­nh.

BÆ°á»›c Ä‘áº§u tiÃªn lÃ  tÃ­nh giÃ¡ trá»‹ tuyáº¿n tÃ­nh theo cÃ´ng thá»©c:

$$z = w \cdot x + b$$

ÄÃ¢y lÃ  phÆ°Æ¡ng trÃ¬nh Ä‘Æ°á»ng tháº³ng cÆ¡ báº£n trong khÃ´ng gian 1 chiá»u. Vá»›i X lÃ  máº£ng, phÃ©p toÃ¡n nÃ y Ä‘Æ°á»£c vector hÃ³a, nghÄ©a lÃ  má»i pháº§n tá»­ trong X Ä‘á»u Ä‘Æ°á»£c nhÃ¢n vá»›i w vÃ  cá»™ng vá»›i b cÃ¹ng lÃºc. VÃ­ dá»¥, náº¿u w=2, b=1, vÃ  X=[1, 2, 3] thÃ¬ káº¿t quáº£ z=[3, 5, 7].

BÆ°á»›c thá»© hai lÃ  Ã¡p dá»¥ng hÃ m sigmoid Ä‘á»ƒ biáº¿n Ä‘á»•i z thÃ nh xÃ¡c suáº¥t dá»± Ä‘oÃ¡n h (hypothesis). GiÃ¡ trá»‹ h nÃ y Ä‘áº¡i diá»‡n cho:

$$P(y=1|x;w,b)$$

ÄÃ¢y lÃ  xÃ¡c suáº¥t Ä‘á»ƒ y=1 khi biáº¿t x vá»›i tham sá»‘ w, b. GiÃ¡ trá»‹ h luÃ´n náº±m trong khoáº£ng (0, 1).

BÆ°á»›c cuá»‘i cÃ¹ng lÃ  tÃ­nh Binary Cross-Entropy theo cÃ´ng thá»©c:

$$J(w,b) = -\frac{1}{m} \sum_{i=1}^{m} \left[ y^{(i)} \log(h^{(i)}) + (1-y^{(i)}) \log(1-h^{(i)}) \right]$$

CÃ´ng thá»©c nÃ y hoáº¡t Ä‘á»™ng thÃ´ng minh á»Ÿ chá»—: khi y=1 (thá»±c táº¿ lÃ  lá»›p dÆ°Æ¡ng), pháº§n Ä‘Ã³ng gÃ³p lÃ :

$$-\log(h)$$

Náº¿u h gáº§n 1 (dá»± Ä‘oÃ¡n Ä‘Ãºng) thÃ¬ cho chi phÃ­ tháº¥p, nhÆ°ng náº¿u h gáº§n 0 (dá»± Ä‘oÃ¡n sai) thÃ¬ cho chi phÃ­ ráº¥t cao. TÆ°Æ¡ng tá»±, khi y=0 (thá»±c táº¿ lÃ  lá»›p Ã¢m), pháº§n Ä‘Ã³ng gÃ³p lÃ :

$$-\log(1-h)$$

Náº¿u h gáº§n 0 (dá»± Ä‘oÃ¡n Ä‘Ãºng) thÃ¬ cho chi phÃ­ tháº¥p, cÃ²n náº¿u h gáº§n 1 (dá»± Ä‘oÃ¡n sai) thÃ¬ cho chi phÃ­ ráº¥t cao.

Trong code, epsilon (1e-15) lÃ  má»™t giÃ¡ trá»‹ cá»±c nhá» (0.000000000000001) Ä‘Æ°á»£c thÃªm vÃ o Ä‘á»ƒ trÃ¡nh lá»—i toÃ¡n há»c khi tÃ­nh log(0). Khi h = 0 hoáº·c h = 1, log(0) khÃ´ng xÃ¡c Ä‘á»‹nh (undefined), nhÆ°ng thÃªm epsilon Ä‘áº£m báº£o log luÃ´n tÃ­nh Ä‘Æ°á»£c: log(0 + 1e-15) â‰ˆ -34.5 (sá»‘ Ã¢m lá»›n nhÆ°ng há»¯u háº¡n). Viá»‡c chia cho m (sá»‘ máº«u) Ä‘á»ƒ láº¥y trung bÃ¬nh chi phÃ­ trÃªn táº¥t cáº£ cÃ¡c máº«u giÃºp so sÃ¡nh cÃ´ng báº±ng giá»¯a cÃ¡c táº­p dá»¯ liá»‡u cÃ³ kÃ­ch thÆ°á»›c khÃ¡c nhau.

Má»™t cÃ¢u há»i thÆ°á»ng gáº·p lÃ  táº¡i sao dÃ¹ng Cross-Entropy thay vÃ¬ Mean Squared Error (MSE). LÃ½ do lÃ  MSE cÃ³ dáº¡ng:

$$(h-y)^2$$

CÃ´ng thá»©c nÃ y táº¡o ra hÃ m khÃ´ng lá»“i (non-convex) khi káº¿t há»£p vá»›i sigmoid, dáº«n Ä‘áº¿n nhiá»u local minimum vÃ  khÃ³ tá»‘i Æ°u. Trong khi Ä‘Ã³, Cross-Entropy táº¡o ra hÃ m lá»“i (convex) cÃ³ 1 global minimum duy nháº¥t, giÃºp Gradient Descent há»™i tá»¥ nhanh vÃ  á»•n Ä‘á»‹nh hÆ¡n ráº¥t nhiá»u.

---

### 4ï¸âƒ£ Pháº§n In Káº¿t Quáº£ VÃ  Visualization

#### **4.1. Pháº§n Header vÃ  TÃ­nh Cost**

```python
print("=" * 60)
print("BÃ€I 1 - CÃ‚U A: TÃ­nh J(w,b)")
print("=" * 60)
print(f"Tham sá»‘ ban Ä‘áº§u: w = {w}, b = {b}, alpha = {alpha}")
print()
J_wb = compute_cost(X, y, w, b)
```

**Giáº£i thÃ­ch:**

Pháº§n code nÃ y báº¯t Ä‘áº§u báº±ng viá»‡c in ra tiÃªu Ä‘á» vÃ  thÃ´ng sá»‘ ban Ä‘áº§u Ä‘á»ƒ ngÆ°á»i Ä‘á»c dá»… theo dÃµi quÃ¡ trÃ¬nh thá»±c thi. Sau Ä‘Ã³, hÃ m `compute_cost(X, y, w, b)` Ä‘Æ°á»£c gá»i Ä‘á»ƒ tÃ­nh giÃ¡ trá»‹ hÃ m chi phÃ­ vá»›i tham sá»‘ ban Ä‘áº§u w=0 vÃ  b=0. Káº¿t quáº£ Ä‘Æ°á»£c lÆ°u vÃ o biáº¿n `J_wb` (viáº¿t táº¯t cá»§a J of w and b) Ä‘á»ƒ sá»­ dá»¥ng trong cÃ¡c pháº§n hiá»ƒn thá»‹ tiáº¿p theo. GiÃ¡ trá»‹ nÃ y sáº½ cho biáº¿t mÃ´ hÃ¬nh Ä‘ang hoáº¡t Ä‘á»™ng tá»‡ nhÆ° tháº¿ nÃ o trÆ°á»›c khi Ä‘Æ°á»£c huáº¥n luyá»‡n.

#### **4.2. Táº¡o Figure vá»›i 2 Subplots**

```python
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
```

**Giáº£i thÃ­ch:**

Má»™t khung hÃ¬nh (figure) chá»©a 2 biá»ƒu Ä‘á»“ con (subplots) náº±m ngang cáº¡nh nhau Ä‘Æ°á»£c táº¡o ra, vá»›i cáº¥u hÃ¬nh 1 hÃ ng vÃ  2 cá»™t. KÃ­ch thÆ°á»›c cá»§a figure Ä‘Æ°á»£c Ä‘áº·t lÃ  14 inch chiá»u rá»™ng vÃ  5 inch chiá»u cao (`figsize=(14, 5)`) Ä‘á»ƒ Ä‘áº£m báº£o cÃ¡c biá»ƒu Ä‘á»“ hiá»ƒn thá»‹ rÃµ rÃ ng vÃ  cÃ³ Ä‘á»§ khÃ´ng gian. Biáº¿n `ax1` Ä‘áº¡i diá»‡n cho biá»ƒu Ä‘á»“ bÃªn trÃ¡i sáº½ hiá»ƒn thá»‹ dá»¯ liá»‡u vÃ  Ä‘Æ°á»ng sigmoid, trong khi `ax2` lÃ  biá»ƒu Ä‘á»“ bÃªn pháº£i sáº½ thá»ƒ hiá»‡n bá» máº·t cost function trong khÃ´ng gian (w, b).

---

#### **4.3. Biá»ƒu Äá»“ 1: Dá»¯ Liá»‡u vÃ  Sigmoid Function**

```python
# Táº¡o dáº£i giÃ¡ trá»‹ x mÆ°á»£t mÃ  tá»« 0 Ä‘áº¿n 3.5 Ä‘á»ƒ váº½ Ä‘Æ°á»ng cong sigmoid
x_plot = np.linspace(0, 3.5, 100)
z_plot = w * x_plot + b
y_plot = sigmoid(z_plot)
```

**Giáº£i thÃ­ch:**

Äá»ƒ váº½ Ä‘Æ°á»£c Ä‘Æ°á»ng cong sigmoid mÆ°á»£t mÃ , cáº§n táº¡o má»™t dáº£i giÃ¡ trá»‹ x liÃªn tá»¥c. HÃ m `np.linspace(0, 3.5, 100)` táº¡o ra 100 Ä‘iá»ƒm cÃ¡ch Ä‘á»u nhau tá»« 0 Ä‘áº¿n 3.5, Ä‘á»§ dÃ y Ä‘áº·c Ä‘á»ƒ Ä‘Æ°á»ng cong khÃ´ng bá»‹ gÃ³c cáº¡nh hay gÃ£y khÃºc. Sau Ä‘Ã³, giÃ¡ trá»‹ z tÆ°Æ¡ng á»©ng vá»›i tá»«ng Ä‘iá»ƒm x Ä‘Æ°á»£c tÃ­nh theo cÃ´ng thá»©c:

$$z = w \times x + b$$

Rá»“i Ä‘Æ°a qua hÃ m sigmoid Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c giÃ¡ trá»‹ y, táº¡o thÃ nh Ä‘Æ°á»ng cong hoÃ n chá»‰nh Ä‘á»ƒ váº½ lÃªn biá»ƒu Ä‘á»“.

```python
# Váº½ cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u thá»±c táº¿
ax1.scatter(X[y == 0], y[y == 0], color='blue', s=150, marker='o',
            label='Class 0 (y=0)', edgecolors='black', linewidth=2)
ax1.scatter(X[y == 1], y[y == 1], color='red', s=150, marker='s',
            label='Class 1 (y=1)', edgecolors='black', linewidth=2)
```

**Giáº£i thÃ­ch:**

Trong pháº§n nÃ y, cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u thá»±c táº¿ Ä‘Æ°á»£c váº½ lÃªn biá»ƒu Ä‘á»“ Ä‘á»ƒ trá»±c quan hÃ³a dá»¯ liá»‡u. Vá»›i cÃ¡c Ä‘iá»ƒm thuá»™c lá»›p 0 (y=0), cÃº phÃ¡p `X[y == 0]` Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ lá»c ra nhá»¯ng Ä‘iá»ƒm cÃ³ nhÃ£n y báº±ng 0, sau Ä‘Ã³ váº½ chÃºng báº±ng mÃ u xanh dÆ°Æ¡ng (blue) vá»›i hÃ¬nh trÃ²n (marker='o'). KÃ­ch thÆ°á»›c Ä‘iá»ƒm Ä‘Æ°á»£c Ä‘áº·t lÃ  150 (`s=150`) Ä‘á»ƒ dá»… nhÃ¬n tháº¥y, vÃ  cÃ³ viá»n mÃ u Ä‘en (`edgecolors='black'`) vá»›i Ä‘á»™ dÃ y 2 (`linewidth=2`) Ä‘á»ƒ táº¡o sá»± phÃ¢n biá»‡t rÃµ rÃ ng.

TÆ°Æ¡ng tá»±, vá»›i cÃ¡c Ä‘iá»ƒm thuá»™c lá»›p 1 (y=1), `X[y == 1]` Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ lá»c ra nhá»¯ng Ä‘iá»ƒm cÃ³ nhÃ£n y báº±ng 1, vÃ  váº½ chÃºng báº±ng mÃ u Ä‘á» (red) vá»›i hÃ¬nh vuÃ´ng (marker='s'). Viá»‡c sá»­ dá»¥ng hai mÃ u khÃ¡c nhau (xanh cho lá»›p 0, Ä‘á» cho lá»›p 1) vÃ  hai hÃ¬nh dáº¡ng khÃ¡c nhau (trÃ²n vÃ  vuÃ´ng) giÃºp ngÆ°á»i xem dá»… dÃ ng phÃ¢n biá»‡t hai lá»›p dá»¯ liá»‡u ngay cáº£ khi xem trÃªn áº£nh Ä‘en tráº¯ng hoáº·c khi cÃ³ váº¥n Ä‘á» vá» mÃ u sáº¯c.

```python
# Váº½ Ä‘Æ°á»ng dá»± Ä‘oÃ¡n Sigmoid
ax1.plot(x_plot, y_plot, 'g-', linewidth=2.5,
         label=f'Sigmoid: h(x) = Ïƒ({w}x + {b})')
```

**Giáº£i thÃ­ch:**

Sau khi váº½ cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u, Ä‘Æ°á»ng cong sigmoid thá»ƒ hiá»‡n mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n hiá»‡n táº¡i Ä‘Æ°á»£c váº½. ÄÆ°á»ng nÃ y Ä‘Æ°á»£c váº½ báº±ng mÃ u xanh lÃ¡ cÃ¢y (green) vá»›i kiá»ƒu Ä‘Æ°á»ng liá»n (`'g-'`) vÃ  Ä‘á»™ dÃ y 2.5 (`linewidth=2.5`) Ä‘á»ƒ ná»•i báº­t trÃªn biá»ƒu Ä‘á»“. NhÃ£n (label) cá»§a Ä‘Æ°á»ng hiá»ƒn thá»‹ cÃ´ng thá»©c toÃ¡n há»c:

$$h(x) = \sigma(wx + b)$$

Vá»›i giÃ¡ trá»‹ w vÃ  b hiá»‡n táº¡i. Äiá»u Ä‘áº·c biá»‡t lÃ  vá»›i w=0 vÃ  b=0 nhÆ° trong trÆ°á»ng há»£p ban Ä‘áº§u, cÃ´ng thá»©c trá»Ÿ thÃ nh:

$$\sigma(0)$$

Cho má»i giÃ¡ trá»‹ x, dáº«n Ä‘áº¿n Ä‘Æ°á»ng sigmoid sáº½ lÃ  má»™t Ä‘Æ°á»ng tháº³ng ngang táº¡i y=0.5 bá»Ÿi vÃ¬ sigmoid(0) luÃ´n báº±ng 0.5 báº¥t ká»ƒ x lÃ  bao nhiÃªu.

```python
# Váº½ Ä‘Æ°á»ng biÃªn quyáº¿t Ä‘á»‹nh (Decision Boundary)
ax1.axhline(y=0.5, color='orange', linestyle='--', linewidth=2,
            label='Decision Boundary (h=0.5)')
```

**Giáº£i thÃ­ch:**

-   `axhline`: Váº½ Ä‘Æ°á»ng ngang (horizontal line)
-   **Decision Boundary** táº¡i h=0.5: NgÆ°á»¡ng phÃ¢n loáº¡i
    -   Náº¿u h â‰¥ 0.5 â†’ dá»± Ä‘oÃ¡n y=1
    -   Náº¿u h < 0.5 â†’ dá»± Ä‘oÃ¡n y=0
-   ÄÆ°á»ng nÃ y giÃºp tháº¥y rÃµ mÃ´ hÃ¬nh Ä‘ang phÃ¢n loáº¡i cÃ¡c Ä‘iá»ƒm nhÆ° tháº¿ nÃ o

```python
# Gáº¯n nhÃ£n toáº¡ Ä‘á»™ lÃªn tá»«ng Ä‘iá»ƒm
for i, (xi, yi) in enumerate(zip(X, y)):
    ax1.annotate(f'({xi}, {yi})', (xi, yi),
                textcoords="offset points", xytext=(0,10),
                ha='center', fontsize=9)
```

**Giáº£i thÃ­ch:**

Pháº§n code nÃ y duyá»‡t qua tá»«ng cáº·p (xi, yi) trong dá»¯ liá»‡u vÃ  gáº¯n nhÃ£n vÄƒn báº£n lÃªn biá»ƒu Ä‘á»“ báº±ng hÃ m `annotate`. Tham sá»‘ `xytext=(0,10)` xÃ¡c Ä‘á»‹nh vá»‹ trÃ­ cá»§a text cÃ¡ch Ä‘iá»ƒm dá»¯ liá»‡u 10 pixels vá» phÃ­a trÃªn, cÃ²n `ha='center'` cÄƒn giá»¯a text theo chiá»u ngang. Viá»‡c hiá»ƒn thá»‹ tá»a Ä‘á»™ trá»±c tiáº¿p trÃªn biá»ƒu Ä‘á»“ giÃºp ngÆ°á»i xem dá»… dÃ ng Ä‘á»c giÃ¡ trá»‹ chÃ­nh xÃ¡c cá»§a tá»«ng Ä‘iá»ƒm mÃ  khÃ´ng cáº§n Æ°á»›c lÆ°á»£ng tá»« cÃ¡c trá»¥c.

```python
# Trang trÃ­ biá»ƒu Ä‘á»“ 1
ax1.set_xlabel('x', fontsize=13, fontweight='bold')
ax1.set_ylabel('y', fontsize=13, fontweight='bold')
ax1.set_title(f'Dá»¯ liá»‡u vÃ  Sigmoid Function\nJ(w={w}, b={b}) = {J_wb:.8f}',
              fontsize=14, fontweight='bold')
ax1.legend(fontsize=10, loc='best')
ax1.grid(True, alpha=0.3, linestyle='--')
ax1.set_ylim([-0.1, 1.1])
ax1.set_xlim([0, 3.5])
```

**Giáº£i thÃ­ch:**

Pháº§n trang trÃ­ nÃ y thiáº¿t láº­p cÃ¡c thuá»™c tÃ­nh hiá»ƒn thá»‹ cho biá»ƒu Ä‘á»“. NhÃ£n trá»¥c x vÃ  y Ä‘Æ°á»£c Ä‘áº·t vá»›i font Ä‘áº­m (bold) vÃ  kÃ­ch thÆ°á»›c 13 Ä‘á»ƒ dá»… Ä‘á»c. TiÃªu Ä‘á» biá»ƒu Ä‘á»“ hiá»ƒn thá»‹ cÃ´ng thá»©c vÃ  giÃ¡ trá»‹ J(w,b) vá»›i 8 chá»¯ sá»‘ tháº­p phÃ¢n, cung cáº¥p thÃ´ng tin chi tiáº¿t vá» hÃ m chi phÃ­ táº¡i tham sá»‘ hiá»‡n táº¡i. HÃ m `legend` hiá»ƒn thá»‹ chÃº thÃ­ch cho cÃ¡c Ä‘Æ°á»ng vÃ  Ä‘iá»ƒm, vá»›i `loc='best'` cho phÃ©p matplotlib tá»± Ä‘á»™ng tÃ¬m vá»‹ trÃ­ tá»‘i Æ°u khÃ´ng bá»‹ che khuáº¥t dá»¯ liá»‡u. LÆ°á»›i ná»n Ä‘Æ°á»£c báº­t vá»›i Ä‘á»™ trong suá»‘t 0.3 vÃ  kiá»ƒu Ä‘Æ°á»ng gáº¡ch ngang, giÃºp ngÆ°á»i xem dá»… Æ°á»›c lÆ°á»£ng giÃ¡ trá»‹ mÃ  khÃ´ng lÃ m loÃ£ng biá»ƒu Ä‘á»“. Cuá»‘i cÃ¹ng, `set_ylim` vÃ  `set_xlim` giá»›i háº¡n pháº¡m vi cÃ¡c trá»¥c Ä‘á»ƒ biá»ƒu Ä‘á»“ thoÃ¡ng Ä‘Ã£ng, khÃ´ng bá»‹ sÃ¡t mÃ©p khung hÃ¬nh.

---

#### **4.4. Biá»ƒu Äá»“ 2: Máº·t Pháº³ng Cost Function**

```python
# Táº¡o lÆ°á»›i toáº¡ Ä‘á»™ (mesh grid) cho w vÃ  b
w_range = np.linspace(-2, 2, 50)
b_range = np.linspace(-2, 2, 50)
W, B = np.meshgrid(w_range, b_range)
Z = np.zeros_like(W)
```

**Giáº£i thÃ­ch:**

Äá»ƒ váº½ bá» máº·t Cost function trong khÃ´ng gian 2 chiá»u (w, b), trÆ°á»›c tiÃªn cáº§n táº¡o lÆ°á»›i tá»a Ä‘á»™ (mesh grid). HÃ m `np.linspace(-2, 2, 50)` táº¡o ra 50 giÃ¡ trá»‹ cÃ¡ch Ä‘á»u nhau tá»« -2 Ä‘áº¿n 2 cho cáº£ w vÃ  b, táº¡o thÃ nh dáº£i giÃ¡ trá»‹ kháº£o sÃ¡t. Sau Ä‘Ã³, hÃ m `meshgrid` káº¿t há»£p hai dáº£i giÃ¡ trá»‹ nÃ y thÃ nh lÆ°á»›i 2D, trong Ä‘Ã³ má»—i Ä‘iá»ƒm trÃªn lÆ°á»›i Ä‘áº¡i diá»‡n cho má»™t cáº·p tham sá»‘ (w, b) cá»¥ thá»ƒ. Ma tráº­n Z Ä‘Æ°á»£c khá»Ÿi táº¡o vá»›i kÃ­ch thÆ°á»›c 50Ã—50 báº±ng `zeros_like(W)` Ä‘á»ƒ lÆ°u giÃ¡ trá»‹ Cost táº¡i tá»«ng Ä‘iá»ƒm (w, b) trÃªn lÆ°á»›i, táº¡o thÃ nh "báº£n Ä‘á»“ Ä‘á»‹a hÃ¬nh" cá»§a hÃ m chi phÃ­.

```python
# TÃ­nh Cost cho tá»«ng Ä‘iá»ƒm trÃªn lÆ°á»›i
for i in range(len(w_range)):
    for j in range(len(b_range)):
        Z[j, i] = compute_cost(X, y, W[j, i], B[j, i])
```

**Giáº£i thÃ­ch:**

VÃ²ng láº·p lá»“ng nhau nÃ y duyá»‡t qua táº¥t cáº£ 2500 cáº·p (w, b) trÃªn lÆ°á»›i (50Ã—50), tÃ­nh giÃ¡ trá»‹ Cost táº¡i má»—i Ä‘iá»ƒm báº±ng hÃ m `compute_cost`, vÃ  lÆ°u káº¿t quáº£ vÃ o ma tráº­n Z táº¡i vá»‹ trÃ­ tÆ°Æ¡ng á»©ng. QuÃ¡ trÃ¬nh nÃ y táº¡o ra "báº£n Ä‘á»“ Ä‘á»‹a hÃ¬nh" hoÃ n chá»‰nh cá»§a hÃ m Cost trong khÃ´ng gian tham sá»‘, trong Ä‘Ã³ cÃ¡c vÃ¹ng trÅ©ng (giÃ¡ trá»‹ tháº¥p) Ä‘áº¡i diá»‡n cho cÃ¡c Ä‘iá»ƒm tá»‘i Æ°u, cÃ²n cÃ¡c vÃ¹ng cao (giÃ¡ trá»‹ lá»›n) Ä‘áº¡i diá»‡n cho cÃ¡c tham sá»‘ kÃ©m hiá»‡u quáº£.

```python
# Váº½ Ä‘Æ°á»ng Ä‘á»“ng má»©c (Contour plot)
contour = ax2.contour(W, B, Z, levels=20, cmap='viridis')
ax2.clabel(contour, inline=True, fontsize=8)
```

**Giáº£i thÃ­ch:**

HÃ m `contour` váº½ cÃ¡c Ä‘Æ°á»ng Ä‘á»“ng má»©c (contour lines) tÆ°Æ¡ng tá»± nhÆ° Ä‘Æ°á»ng bÃ¬nh Ä‘á»™ trÃªn báº£n Ä‘á»“ Ä‘á»‹a lÃ½, trong Ä‘Ã³ má»—i Ä‘Æ°á»ng ná»‘i cÃ¡c Ä‘iá»ƒm cÃ³ cÃ¹ng giÃ¡ trá»‹ Cost. Tham sá»‘ `levels=20` chá»‰ Ä‘á»‹nh váº½ 20 Ä‘Æ°á»ng má»©c khÃ¡c nhau Ä‘á»ƒ thá»ƒ hiá»‡n sá»± thay Ä‘á»•i cá»§a Cost má»™t cÃ¡ch chi tiáº¿t. Báº£ng mÃ u `viridis` Ä‘Æ°á»£c chá»n vá»›i gradient tá»« tÃ­m Ä‘áº­m (giÃ¡ trá»‹ Cost cao) qua xanh lÃ¡ sang vÃ ng (giÃ¡ trá»‹ Cost tháº¥p), giÃºp ngÆ°á»i xem dá»… dÃ ng nháº­n biáº¿t cÃ¡c vÃ¹ng tá»‘i Æ°u. HÃ m `clabel` vá»›i `inline=True` hiá»ƒn thá»‹ giÃ¡ trá»‹ sá»‘ trá»±c tiáº¿p trÃªn cÃ¡c Ä‘Æ°á»ng Ä‘á»“ng má»©c vá»›i cá»¡ chá»¯ 8, cung cáº¥p thÃ´ng tin Ä‘á»‹nh lÆ°á»£ng chÃ­nh xÃ¡c vá» giÃ¡ trá»‹ Cost táº¡i tá»«ng vÃ¹ng.

```python
# ÄÃ¡nh dáº¥u vá»‹ trÃ­ hiá»‡n táº¡i (w=0, b=0)
ax2.plot(w, b, 'r*', markersize=20, label=f'(w={w}, b={b})')
```

**Giáº£i thÃ­ch:**

Lá»‡nh nÃ y váº½ má»™t ngÃ´i sao Ä‘á» kÃ­ch thÆ°á»›c lá»›n (markersize=20) táº¡i vá»‹ trÃ­ (w=0, b=0) trÃªn biá»ƒu Ä‘á»“ contour, Ä‘Ã¡nh dáº¥u rÃµ rÃ ng Ä‘iá»ƒm khá»Ÿi Ä‘áº§u cá»§a tham sá»‘. Viá»‡c hiá»ƒn thá»‹ vá»‹ trÃ­ hiá»‡n táº¡i trÃªn "báº£n Ä‘á»“ Ä‘á»‹a hÃ¬nh" Cost giÃºp ngÆ°á»i xem dá»… dÃ ng so sÃ¡nh vá»›i vÃ¹ng tá»‘i Æ°u (vÃ¹ng trÅ©ng nháº¥t cÃ³ Cost tháº¥p), tá»« Ä‘Ã³ hiá»ƒu Ä‘Æ°á»£c mÃ´ hÃ¬nh cáº§n di chuyá»ƒn theo hÆ°á»›ng nÃ o Ä‘á»ƒ cáº£i thiá»‡n hiá»‡u suáº¥t.

---

#### **4.5. LÆ°u vÃ  Hiá»ƒn Thá»‹**

```python
plt.tight_layout()
plt.savefig('results/ex1a_cost_function_visualization.png',
            dpi=300, bbox_inches='tight')
plt.show()
```

**Giáº£i thÃ­ch:**

-   `tight_layout()`: Tá»± Ä‘á»™ng Ä‘iá»u chá»‰nh khoáº£ng cÃ¡ch giá»¯a cÃ¡c subplot Ä‘á»ƒ khÃ´ng bá»‹ chá»“ng láº¥n
-   `savefig`: LÆ°u hÃ¬nh vá»›i Ä‘á»™ phÃ¢n giáº£i cao (300 DPI - cháº¥t lÆ°á»£ng in áº¥n)
-   `bbox_inches='tight'`: Cáº¯t bá» khoáº£ng tráº¯ng thá»«a xung quanh
-   `show()`: Hiá»ƒn thá»‹ biá»ƒu Ä‘á»“ lÃªn mÃ n hÃ¬nh

---

#### **4.6. In Chi Tiáº¿t QuÃ¡ TrÃ¬nh TÃ­nh ToÃ¡n**

```python
print(f"Sá»‘ máº«u dá»¯ liá»‡u (m): {len(X)}")
print(f"\nDá»¯ liá»‡u tá»«ng Ä‘iá»ƒm:")

for i, (xi, yi) in enumerate(zip(X, y)):
    z_i = w * xi + b
    h_i = sigmoid(z_i)
    print(f"  x[{i}] = {xi}, y[{i}] = {yi} => z = {z_i:.1f}, h(x) = {h_i:.4f}")

print(f"\nCÃ´ng thá»©c Cost function: J(w,b) = -(1/m) * Î£[y*log(h) + (1-y)*log(1-h)]")
print(f"Káº¿t quáº£ cuá»‘i cÃ¹ng: J({w}, {b}) = {J_wb:.8f}")
print("=" * 60)
```

**Giáº£i thÃ­ch:**

Pháº§n cuá»‘i cá»§a chÆ°Æ¡ng trÃ¬nh in ra chi tiáº¿t quÃ¡ trÃ¬nh tÃ­nh toÃ¡n Ä‘á»ƒ ngÆ°á»i Ä‘á»c hiá»ƒu rÃµ tá»«ng bÆ°á»›c. Äáº§u tiÃªn, sá»‘ lÆ°á»£ng máº«u dá»¯ liá»‡u (m = 6) Ä‘Æ°á»£c in ra Ä‘á»ƒ biáº¿t quy mÃ´ táº­p dá»¯ liá»‡u. Tiáº¿p theo, vÃ²ng láº·p Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ duyá»‡t qua tá»«ng cáº·p (xi, yi) trong dá»¯ liá»‡u:

```python
for i, (xi, yi) in enumerate(zip(X, y)):
```

HÃ m `enumerate` giÃºp cÃ³ thÃªm chá»‰ sá»‘ i Ä‘á»ƒ Ä‘Ã¡nh sá»‘ cÃ¡c Ä‘iá»ƒm.

Vá»›i má»—i Ä‘iá»ƒm dá»¯ liá»‡u, giÃ¡ trá»‹ tuyáº¿n tÃ­nh `z_i = w * xi + b` (giÃ¡ trá»‹ trÆ°á»›c khi qua activation function) vÃ  `h_i = sigmoid(z_i)` (xÃ¡c suáº¥t dá»± Ä‘oÃ¡n sau khi qua sigmoid) Ä‘Æ°á»£c tÃ­nh. Káº¿t quáº£ Ä‘Æ°á»£c in ra vá»›i `.1f` cho z (1 chá»¯ sá»‘ tháº­p phÃ¢n) vÃ  `.4f` cho h (4 chá»¯ sá»‘ tháº­p phÃ¢n) giÃºp ngÆ°á»i Ä‘á»c tháº¥y rÃµ quÃ¡ trÃ¬nh biáº¿n Ä‘á»•i tá»« dá»¯ liá»‡u Ä‘áº§u vÃ o sang xÃ¡c suáº¥t dá»± Ä‘oÃ¡n.

Cuá»‘i cÃ¹ng, cÃ´ng thá»©c Cost function Ä‘Æ°á»£c hiá»ƒn thá»‹ dÆ°á»›i dáº¡ng kÃ½ hiá»‡u toÃ¡n há»c:

$$J(w,b) = -\frac{1}{m} \times \Sigma[y \times \log(h) + (1-y) \times \log(1-h)]$$

Äá»ƒ ngÆ°á»i Ä‘á»c hiá»ƒu rÃµ phÆ°Æ¡ng phÃ¡p tÃ­nh, rá»“i in káº¿t quáº£ cuá»‘i cÃ¹ng `J(0, 0)` vá»›i 8 chá»¯ sá»‘ tháº­p phÃ¢n Ä‘á»ƒ Ä‘áº£m báº£o Ä‘á»™ chÃ­nh xÃ¡c cao.

---

## ğŸ“Š Output vÃ  Káº¿t Quáº£

### ğŸ–¥ï¸ Console Output

```
============================================================
BÃ€I 1 - CÃ‚U A: TÃ­nh J(w,b)
============================================================
Tham sá»‘ ban Ä‘áº§u: w = 0, b = 0, alpha = 0.0001

Sá»‘ máº«u dá»¯ liá»‡u (m): 6

Dá»¯ liá»‡u tá»«ng Ä‘iá»ƒm:
  x[0] = 0.5, y[0] = 0 => z = 0.0, h(x) = 0.5000
  x[1] = 1.0, y[1] = 0 => z = 0.0, h(x) = 0.5000
  x[2] = 1.5, y[2] = 0 => z = 0.0, h(x) = 0.5000
  x[3] = 3.0, y[3] = 1 => z = 0.0, h(x) = 0.5000
  x[4] = 2.0, y[4] = 1 => z = 0.0, h(x) = 0.5000
  x[5] = 1.0, y[5] = 1 => z = 0.0, h(x) = 0.5000

CÃ´ng thá»©c Cost function: J(w,b) = -(1/m) * Î£[y*log(h) + (1-y)*log(1-h)]
Káº¿t quáº£ cuá»‘i cÃ¹ng: J(0, 0) = 0.69314718
============================================================
```

### ğŸ“ˆ PhÃ¢n TÃ­ch Káº¿t Quáº£

#### **1. GiÃ¡ trá»‹ J(0, 0) = 0.69314718**

ÄÃ¢y lÃ  giÃ¡ trá»‹ hÃ m chi phÃ­ khi mÃ´ hÃ¬nh chÆ°a Ä‘Æ°á»£c huáº¥n luyá»‡n vá»›i w=0 vÃ  b=0. GiÃ¡ trá»‹ nÃ y thá»ƒ hiá»‡n mÃ´ hÃ¬nh Ä‘ang dá»± Ä‘oÃ¡n xÃ¡c suáº¥t 0.5 cho má»i Ä‘iá»ƒm, tá»©c lÃ  dá»± Ä‘oÃ¡n hoÃ n toÃ n ngáº«u nhiÃªn (50-50). Äiá»u thÃº vá»‹ lÃ  J(0,0) gáº§n báº±ng ln(2) â‰ˆ 0.693147, vÃ  Ä‘Ã¢y khÃ´ng pháº£i trÃ¹ng há»£p ngáº«u nhiÃªn.

**Giáº£i thÃ­ch toÃ¡n há»c:**

Khi h = 0.5 cho má»i Ä‘iá»ƒm, hÃ m chi phÃ­ Ä‘Æ°á»£c tÃ­nh nhÆ° sau:

$$J = -\frac{1}{m}\sum_{i=1}^{m}[y_i\log(0.5) + (1-y_i)\log(0.5)]$$

$$= -\frac{1}{m}\sum_{i=1}^{m}\log(0.5)$$

$$= -\log(0.5) = \log(2) \approx 0.693$$

Káº¿t quáº£ nÃ y Ä‘áº¡i diá»‡n cho chi phÃ­ tá»‘i Ä‘a cá»§a má»™t mÃ´ hÃ¬nh phÃ¢n loáº¡i nhá»‹ phÃ¢n khi dá»± Ä‘oÃ¡n hoÃ n toÃ n ngáº«u nhiÃªn (50-50), pháº£n Ã¡nh viá»‡c mÃ´ hÃ¬nh chÆ°a há»c Ä‘Æ°á»£c báº¥t ká»³ thÃ´ng tin há»¯u Ã­ch nÃ o tá»« dá»¯ liá»‡u.

#### **2. Táº¡i sao h(x) = 0.5 cho má»i x?**

Vá»›i tham sá»‘ ban Ä‘áº§u w=0 vÃ  b=0, giÃ¡ trá»‹ tuyáº¿n tÃ­nh z luÃ´n báº±ng 0 báº¥t ká»ƒ giÃ¡ trá»‹ x lÃ  bao nhiÃªu:

$$z = 0 \cdot x + 0 = 0 \text{ (cho má»i x)}$$

Khi Ä‘Æ°a qua hÃ m sigmoid:

$$h(x) = \sigma(0) = \frac{1}{1+e^0} = \frac{1}{2} = 0.5$$

Káº¿t quáº£ nÃ y dáº«n Ä‘áº¿n Ä‘Æ°á»ng sigmoid lÃ  má»™t Ä‘Æ°á»ng tháº³ng ngang táº¡i y=0.5, hoÃ n toÃ n khÃ´ng cÃ³ kháº£ nÄƒng phÃ¢n loáº¡i vÃ¬ má»i Ä‘iá»ƒm dá»¯ liá»‡u Ä‘á»u nháº­n Ä‘Æ°á»£c xÃ¡c suáº¥t giá»‘ng nhau.

#### **3. Biá»ƒu Ä‘á»“ 1: Dá»¯ liá»‡u vÃ  Sigmoid**

Biá»ƒu Ä‘á»“ bÃªn trÃ¡i thá»ƒ hiá»‡n sá»± phÃ¢n bá»‘ dá»¯ liá»‡u vÃ  Ä‘Æ°á»ng dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh. CÃ¡c Ä‘iá»ƒm xanh (y=0) táº­p trung á»Ÿ phÃ­a trÃ¡i vá»›i giÃ¡ trá»‹ x nhá», trong khi cÃ¡c Ä‘iá»ƒm Ä‘á» (y=1) náº±m á»Ÿ phÃ­a pháº£i vá»›i giÃ¡ trá»‹ x lá»›n hÆ¡n. ÄÆ°á»ng sigmoid mÃ u xanh lÃ¡ hiá»‡n táº¡i lÃ  má»™t Ä‘Æ°á»ng ngang táº¡i má»©c 0.5 do w=0 vÃ  b=0, trÃ¹ng vá»›i Ä‘Æ°á»ng quyáº¿t Ä‘á»‹nh (decision boundary) mÃ u cam gáº¡ch ngang. Nháº­n xÃ©t quan trá»ng lÃ  mÃ´ hÃ¬nh hiá»‡n táº¡i hoÃ n toÃ n khÃ´ng phÃ¢n loáº¡i Ä‘Æ°á»£c gÃ¬ vÃ¬ táº¥t cáº£ Ä‘iá»ƒm Ä‘á»u nháº­n xÃ¡c suáº¥t dá»± Ä‘oÃ¡n giá»‘ng nhau lÃ  0.5, khÃ´ng pháº£n Ã¡nh sá»± khÃ¡c biá»‡t giá»¯a hai lá»›p dá»¯ liá»‡u.

#### **4. Biá»ƒu Ä‘á»“ 2: Cost Function Surface**

Biá»ƒu Ä‘á»“ bÃªn pháº£i thá»ƒ hiá»‡n "báº£n Ä‘á»“ Ä‘á»‹a hÃ¬nh" cá»§a hÃ m Cost trong khÃ´ng gian (w, b). CÃ¡c vÃ¹ng mÃ u tÃ­m Ä‘áº­m thá»ƒ hiá»‡n nhá»¯ng khu vá»±c cÃ³ giÃ¡ trá»‹ Cost cao, tÆ°Æ¡ng á»©ng vá»›i mÃ´ hÃ¬nh hoáº¡t Ä‘á»™ng kÃ©m, trong khi cÃ¡c vÃ¹ng mÃ u vÃ ng biá»ƒu thá»‹ nhá»¯ng vÃ¹ng cÃ³ Cost tháº¥p vá»›i mÃ´ hÃ¬nh tá»‘t hÆ¡n. NgÃ´i sao Ä‘á» táº¡i toáº¡ Ä‘á»™ (0,0) Ä‘Ã¡nh dáº¥u vá»‹ trÃ­ ban Ä‘áº§u cá»§a tham sá»‘, cÃ²n cÃ¡c vÃ¹ng trÅ©ng (valley) chá»‰ hÆ°á»›ng Ä‘áº¿n Ä‘iá»ƒm tá»‘i Æ°u.

Quan sÃ¡t trÃªn biá»ƒu Ä‘á»“ cho tháº¥y Ä‘iá»ƒm (0,0) náº±m á»Ÿ vÃ¹ng cÃ³ Cost xáº¥p xá»‰ 0.693, khÃ´ng pháº£i tá»‡ nháº¥t nhÆ°ng cÅ©ng khÃ´ng tá»‘t. Biá»ƒu Ä‘á»“ thá»ƒ hiá»‡n rÃµ rÃ ng má»™t vÃ¹ng trÅ©ng hÆ°á»›ng vá» phÃ­a w dÆ°Æ¡ng vÃ  b Ã¢m, Ä‘Ã¢y chÃ­nh lÃ  hÆ°á»›ng mÃ  thuáº­t toÃ¡n Gradient Descent sáº½ di chuyá»ƒn Ä‘á»ƒ giáº£m Cost vÃ  tÃ¬m Ä‘iá»ƒm tá»‘i Æ°u.

---
