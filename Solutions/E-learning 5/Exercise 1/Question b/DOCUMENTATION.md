# ğŸ“˜ E-Learning 5 - Exercise 1 - Question B: Gradient Descent

## ğŸ¯ Má»¥c TiÃªu BÃ i Táº­p

BÃ i táº­p yÃªu cáº§u **cáº­p nháº­t tham sá»‘ w vÃ  b** báº±ng thuáº­t toÃ¡n **Gradient Descent** Ä‘á»ƒ tá»‘i Æ°u hÃ³a mÃ´ hÃ¬nh Logistic Regression, sau Ä‘Ã³ tÃ­nh giÃ¡ trá»‹ hÃ m chi phÃ­ J(w_update, b_update).

### ğŸ“Š Äá» BÃ i

Tiáº¿p theo tá»« Question A:

-   Sá»­ dá»¥ng cÃ¹ng táº­p dá»¯ liá»‡u vÃ  tham sá»‘ ban Ä‘áº§u
-   **YÃªu cáº§u:** Cáº­p nháº­t w, b theo thuáº­t toÃ¡n Gradient Descent vÃ  tÃ­nh J(w_update, b_update)

**Ká»³ vá»ng:** GiÃ¡ trá»‹ Cost sáº½ giáº£m tá»« ~0.693 (Question A) xuá»‘ng gáº§n 0

---

## ğŸ’» PhÃ¢n TÃ­ch Source Code Chi Tiáº¿t

### 1ï¸âƒ£ Import vÃ  Khá»Ÿi Táº¡o

```python
import numpy as np
import matplotlib.pyplot as plt

# Dá»¯ liá»‡u
X = np.array([0.5, 1, 1.5, 3, 2, 1])
y = np.array([0, 0, 0, 1, 1, 1])

# Tham sá»‘ ban Ä‘áº§u
w = 0
b = 0
alpha = 0.0001
```

**Giáº£i thÃ­ch:**

Pháº§n nÃ y giá»‘ng vá»›i Question A, khá»Ÿi táº¡o:

-   **Dá»¯ liá»‡u X, y:** 6 Ä‘iá»ƒm dá»¯ liá»‡u cho bÃ i toÃ¡n phÃ¢n loáº¡i nhá»‹ phÃ¢n
-   **Tham sá»‘ ban Ä‘áº§u:** w=0, b=0 (mÃ´ hÃ¬nh chÆ°a há»c gÃ¬)
-   **Learning rate Î±=0.0001:** BÆ°á»›c nháº£y ráº¥t nhá» Ä‘á»ƒ há»c tá»« tá»«, trÃ¡nh overshooting

---

### 2ï¸âƒ£ CÃ¡c HÃ m CÆ¡ Báº£n

#### **HÃ m Sigmoid**

```python
def sigmoid(z):
    return 1 / (1 + np.exp(-z))
```

**Giáº£i thÃ­ch:**

HÃ m kÃ­ch hoáº¡t sigmoid giá»‘ng Question A, cÃ´ng thá»©c:

$$\sigma(z) = \frac{1}{1 + e^{-z}}$$

-   Chuyá»ƒn Ä‘á»•i giÃ¡ trá»‹ z báº¥t ká»³ thÃ nh xÃ¡c suáº¥t trong khoáº£ng (0, 1)
-   LÃ  thÃ nh pháº§n cá»‘t lÃµi cá»§a Logistic Regression

---

#### **HÃ m Cost (Binary Cross Entropy)**

```python
def compute_cost(X, y, w, b):
    m = len(X)
    z = w * X + b
    h = sigmoid(z)
    eps = 1e-15
    cost = -(1/m) * np.sum(y * np.log(h + eps) + (1 - y) * np.log(1 - h + eps))
    return cost
```

**Giáº£i thÃ­ch:**

HÃ m tÃ­nh chi phÃ­ Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ cháº¥t lÆ°á»£ng mÃ´ hÃ¬nh:

1. **TÃ­nh z:** GiÃ¡ trá»‹ tuyáº¿n tÃ­nh $z = wx + b$
2. **TÃ­nh h:** XÃ¡c suáº¥t dá»± Ä‘oÃ¡n qua sigmoid
3. **TÃ­nh Cost:** Binary Cross-Entropy

$$J(w,b) = -\frac{1}{m}\sum_{i=1}^{m}[y_i\log(h_i) + (1-y_i)\log(1-h_i)]$$

-   **eps = 1e-15:** TrÃ¡nh lá»—i log(0)
-   **Má»¥c tiÃªu:** Minimize J(w,b)

---

### 3ï¸âƒ£ HÃ m TÃ­nh Gradient - TrÃ¡i Tim Cá»§a Gradient Descent

```python
def compute_gradient(X, y, w, b):
    m = len(X)
    z = w * X + b
    h = sigmoid(z)
    error = h - y
    dw = (1/m) * np.sum(error * X)
    db = (1/m) * np.sum(error)
    return dw, db
```

**Giáº£i thÃ­ch Chi Tiáº¿t:**

ÄÃ¢y lÃ  hÃ m **quan trá»ng nháº¥t** - tÃ­nh toÃ¡n **gradient** (Ä‘áº¡o hÃ m) cá»§a Cost function theo w vÃ  b.

#### **BÆ°á»›c 1: TÃ­nh z vÃ  h**

```python
z = w * X + b
h = sigmoid(z)
```

-   TÃ­nh giÃ¡ trá»‹ tuyáº¿n tÃ­nh vÃ  xÃ¡c suáº¥t dá»± Ä‘oÃ¡n
-   Giá»‘ng nhÆ° trong compute_cost

#### **BÆ°á»›c 2: TÃ­nh Error**

```python
error = h - y
```

**Ã nghÄ©a:**

-   `error`: Sai sá»‘ giá»¯a dá»± Ä‘oÃ¡n (h) vÃ  thá»±c táº¿ (y)
-   Náº¿u error > 0: Dá»± Ä‘oÃ¡n cao hÆ¡n thá»±c táº¿ (overestimate)
-   Náº¿u error < 0: Dá»± Ä‘oÃ¡n tháº¥p hÆ¡n thá»±c táº¿ (underestimate)
-   Náº¿u error = 0: Dá»± Ä‘oÃ¡n chÃ­nh xÃ¡c

**VÃ­ dá»¥:**

-   h = 0.8, y = 1 â†’ error = -0.2 (dá»± Ä‘oÃ¡n hÆ¡i tháº¥p)
-   h = 0.3, y = 0 â†’ error = 0.3 (dá»± Ä‘oÃ¡n hÆ¡i cao)

#### **BÆ°á»›c 3: TÃ­nh Gradient cá»§a w**

```python
dw = (1/m) * np.sum(error * X)
```

**CÃ´ng thá»©c toÃ¡n há»c:**

$$\frac{\partial J}{\partial w} = \frac{1}{m}\sum_{i=1}^{m}(h_i - y_i) \cdot x_i$$

**Giáº£i thÃ­ch:**

-   **Äáº¡o hÃ m riÃªng** cá»§a Cost function theo w
-   Cho biáº¿t Cost thay Ä‘á»•i nhÆ° tháº¿ nÃ o khi w thay Ä‘á»•i
-   **error \* X:** Sai sá»‘ cÃ³ trá»ng sá»‘ (weighted error)
    -   Náº¿u xi lá»›n vÃ  error lá»›n â†’ gradient lá»›n â†’ cáº§n Ä‘iá»u chá»‰nh w nhiá»u
    -   Náº¿u xi nhá» hoáº·c error nhá» â†’ gradient nhá» â†’ Ä‘iá»u chá»‰nh w Ã­t

**Ã nghÄ©a hÃ¬nh há»c:**

-   dw > 0: Cost tÄƒng khi w tÄƒng â†’ cáº§n **giáº£m w**
-   dw < 0: Cost tÄƒng khi w giáº£m â†’ cáº§n **tÄƒng w**
-   dw â‰ˆ 0: Äang á»Ÿ gáº§n Ä‘iá»ƒm tá»‘i Æ°u

#### **BÆ°á»›c 4: TÃ­nh Gradient cá»§a b**

```python
db = (1/m) * np.sum(error)
```

**CÃ´ng thá»©c toÃ¡n há»c:**

$$\frac{\partial J}{\partial b} = \frac{1}{m}\sum_{i=1}^{m}(h_i - y_i)$$

**Giáº£i thÃ­ch:**

-   **Äáº¡o hÃ m riÃªng** cá»§a Cost function theo b
-   Tá»•ng cÃ¡c sai sá»‘ (khÃ´ng nhÃ¢n vá»›i X vÃ¬ Ä‘áº¡o hÃ m cá»§a b lÃ  1)
-   **Ã nghÄ©a:**
    -   db > 0: Dá»± Ä‘oÃ¡n trung bÃ¬nh cao hÆ¡n thá»±c táº¿ â†’ cáº§n giáº£m b
    -   db < 0: Dá»± Ä‘oÃ¡n trung bÃ¬nh tháº¥p hÆ¡n thá»±c táº¿ â†’ cáº§n tÄƒng b

#### **Táº¡i sao cÃ´ng thá»©c nÃ y Ä‘Ãºng?**

**Chá»©ng minh toÃ¡n há»c** (simplified):

Tá»« Cost function:
$$J = -\frac{1}{m}\sum[y\log(h) + (1-y)\log(1-h)]$$

Äáº¡o hÃ m theo w (chain rule):
$$\frac{\partial J}{\partial w} = \frac{\partial J}{\partial h} \cdot \frac{\partial h}{\partial z} \cdot \frac{\partial z}{\partial w}$$

Trong Ä‘Ã³:

-   $\frac{\partial J}{\partial h} = -\frac{y}{h} + \frac{1-y}{1-h}$
-   $\frac{\partial h}{\partial z} = h(1-h)$ (tÃ­nh cháº¥t Ä‘áº¹p cá»§a sigmoid)
-   $\frac{\partial z}{\partial w} = x$

Káº¿t há»£p láº¡i:
$$\frac{\partial J}{\partial w} = (h - y) \cdot x$$

Trung bÃ¬nh trÃªn m máº«u:
$$\frac{\partial J}{\partial w} = \frac{1}{m}\sum(h_i - y_i) \cdot x_i$$

---

### 4ï¸âƒ£ Thuáº­t ToÃ¡n Gradient Descent - TrÃ¡i Tim Cá»§a Machine Learning

```python
def gradient_descent(X, y, w, b, alpha, num_iterations):
    cost_history, w_history, b_history = [], [], []

    for i in range(num_iterations):
        dw, db = compute_gradient(X, y, w, b)
        w -= alpha * dw
        b -= alpha * db
        cost = compute_cost(X, y, w, b)

        cost_history.append(cost)
        w_history.append(w)
        b_history.append(b)

        # In 1 vÃ i vÃ²ng láº·p quan trá»ng
        if i == 0 or (i + 1) % 200 == 0 or i == num_iterations - 1:
            print(f"Iteration {i+1:4d} :  w = {w:.6f},  b = {b:.6f},  Cost = {cost:.8f}")

    return w, b, cost_history, w_history, b_history
```

**Giáº£i thÃ­ch Chi Tiáº¿t:**

#### **Khá»Ÿi táº¡o**

```python
cost_history, w_history, b_history = [], [], []
```

-   Táº¡o 3 danh sÃ¡ch rá»—ng Ä‘á»ƒ **lÆ°u lá»‹ch sá»­** quÃ¡ trÃ¬nh training:
    -   `cost_history`: Lá»‹ch sá»­ giÃ¡ trá»‹ Cost qua cÃ¡c iteration
    -   `w_history`: Lá»‹ch sá»­ giÃ¡ trá»‹ w
    -   `b_history`: Lá»‹ch sá»­ giÃ¡ trá»‹ b
-   **Má»¥c Ä‘Ã­ch:** Äá»ƒ phÃ¢n tÃ­ch vÃ  visualization sau nÃ y

#### **VÃ²ng láº·p chÃ­nh**

```python
for i in range(num_iterations):
```

-   Láº·p láº¡i `num_iterations` láº§n (trong code lÃ  1000 láº§n)
-   Má»—i iteration lÃ  má»™t bÆ°á»›c cáº­p nháº­t tham sá»‘

#### **BÆ°á»›c 1: TÃ­nh Gradient**

```python
dw, db = compute_gradient(X, y, w, b)
```

-   TÃ­nh Ä‘áº¡o hÃ m cá»§a Cost function táº¡i Ä‘iá»ƒm (w, b) hiá»‡n táº¡i
-   Gradient chá»‰ ra **hÆ°á»›ng tÄƒng nhanh nháº¥t** cá»§a Cost

#### **BÆ°á»›c 2: Cáº­p Nháº­t Tham Sá»‘**

```python
w -= alpha * dw
b -= alpha * db
```

**CÃ´ng thá»©c toÃ¡n há»c:**

$$w_{new} = w_{old} - \alpha \cdot \frac{\partial J}{\partial w}$$

$$b_{new} = b_{old} - \alpha \cdot \frac{\partial J}{\partial b}$$

**Giáº£i thÃ­ch:**

-   **Dáº¥u trá»« (-):** Äi **ngÆ°á»£c hÆ°á»›ng** gradient Ä‘á»ƒ giáº£m Cost
    -   Gradient chá»‰ hÆ°á»›ng tÄƒng â†’ Ä‘i ngÆ°á»£c láº¡i Ä‘á»ƒ giáº£m
-   **alpha (learning rate):** Kiá»ƒm soÃ¡t **tá»‘c Ä‘á»™ há»c**
    -   QuÃ¡ lá»›n: Há»c nhanh nhÆ°ng cÃ³ thá»ƒ miss optimum (overshooting)
    -   QuÃ¡ nhá»: Há»c cháº­m nhÆ°ng á»•n Ä‘á»‹nh
    -   0.0001 trong bÃ i nÃ y lÃ  giÃ¡ trá»‹ ráº¥t nhá» â†’ há»c ráº¥t cháº­m

**VÃ­ dá»¥ minh há»a:**

Giáº£ sá»­ iteration 1:

-   dw = 2.5, db = 1.3, alpha = 0.0001
-   w_old = 0, b_old = 0

Cáº­p nháº­t:

-   w_new = 0 - 0.0001 Ã— 2.5 = -0.00025
-   b_new = 0 - 0.0001 Ã— 1.3 = -0.00013

BÆ°á»›c nháº£y ráº¥t nhá»!

#### **BÆ°á»›c 3: TÃ­nh Cost má»›i**

```python
cost = compute_cost(X, y, w, b)
```

-   TÃ­nh Cost vá»›i tham sá»‘ má»›i vá»«a cáº­p nháº­t
-   Kiá»ƒm tra xem Cost cÃ³ giáº£m khÃ´ng

#### **BÆ°á»›c 4: LÆ°u Lá»‹ch Sá»­**

```python
cost_history.append(cost)
w_history.append(w)
b_history.append(b)
```

-   LÆ°u láº¡i giÃ¡ trá»‹ Ä‘á»ƒ phÃ¢n tÃ­ch sau
-   GiÃºp váº½ biá»ƒu Ä‘á»“ há»™i tá»¥ (convergence plot)

#### **BÆ°á»›c 5: In Progress**

```python
if i == 0 or (i + 1) % 200 == 0 or i == num_iterations - 1:
    print(f"Iteration {i+1:4d} :  w = {w:.6f},  b = {b:.6f},  Cost = {cost:.8f}")
```

**Giáº£i thÃ­ch:**

-   In ra **má»™t sá»‘ iteration quan trá»ng** Ä‘á»ƒ theo dÃµi tiáº¿n trÃ¬nh:
    -   Iteration Ä‘áº§u tiÃªn (i=0)
    -   Má»—i 200 iterations
    -   Iteration cuá»‘i cÃ¹ng
-   **KhÃ´ng in táº¥t cáº£** vÃ¬ 1000 dÃ²ng quÃ¡ nhiá»u
-   Format sá»‘:
    -   `{i+1:4d}`: In sá»‘ iteration, cÄƒn pháº£i 4 kÃ½ tá»±
    -   `{w:.6f}`: In w vá»›i 6 chá»¯ sá»‘ tháº­p phÃ¢n
    -   `{cost:.8f}`: In cost vá»›i 8 chá»¯ sá»‘ tháº­p phÃ¢n

#### **Return**

```python
return w, b, cost_history, w_history, b_history
```

-   Tráº£ vá»:
    -   `w, b`: Tham sá»‘ tá»‘i Æ°u sau khi training
    -   `cost_history, w_history, b_history`: Lá»‹ch sá»­ Ä‘á»ƒ visualization

---

### 5ï¸âƒ£ Pháº§n Cháº¡y ChÃ­nh vÃ  Visualization

#### **5.1. Header vÃ  Cost Ban Äáº§u**

```python
print("=" * 60)
print("CÃ‚U B â€“ Cáº­p nháº­t w, b báº±ng thuáº­t toÃ¡n Gradient Descent")
print("=" * 60)

initial_cost = compute_cost(X, y, w, b)
print(f"Cost ban Ä‘áº§u (w=0, b=0):  {initial_cost:.8f}\n")
```

**Giáº£i thÃ­ch:**

-   In tiÃªu Ä‘á» Ä‘á»ƒ dá»… Ä‘á»c
-   TÃ­nh vÃ  in Cost ban Ä‘áº§u (trÆ°á»›c khi train)
-   GiÃ¡ trá»‹ nÃ y sáº½ lÃ  ~0.693 (giá»‘ng Question A)

---

#### **5.2. Cháº¡y Gradient Descent**

```python
num_iterations = 1000
w_final, b_final, cost_history, w_history, b_history = gradient_descent(
    X, y, w, b, alpha, num_iterations
)
```

**Giáº£i thÃ­ch:**

-   Äáº·t sá»‘ iterations = 1000 (1000 bÆ°á»›c cáº­p nháº­t)
-   Gá»i hÃ m `gradient_descent` vá»›i:
    -   Dá»¯ liá»‡u X, y
    -   Tham sá»‘ ban Ä‘áº§u w=0, b=0
    -   Learning rate alpha=0.0001
    -   Sá»‘ iterations = 1000
-   Nháº­n vá»:
    -   `w_final, b_final`: Tham sá»‘ sau khi train xong
    -   `cost_history, w_history, b_history`: Lá»‹ch sá»­ Ä‘á»ƒ váº½ biá»ƒu Ä‘á»“

---

#### **5.3. In Káº¿t Quáº£**

```python
print("\nKáº¾T QUáº¢ SAU TRAINING:")
print(f"w_update = {w_final:.8f}")
print(f"b_update = {b_final:.8f}")
print(f"Cost cuá»‘i = {cost_history[-1]:.8f}")
print(f"Cost giáº£m Ä‘Æ°á»£c: {initial_cost - cost_history[-1]:.8f}")
```

**Giáº£i thÃ­ch:**

-   In tham sá»‘ cuá»‘i cÃ¹ng (w_update, b_update)
-   In Cost cuá»‘i cÃ¹ng (sau 1000 iterations)
-   TÃ­nh vÃ  in **lÆ°á»£ng Cost giáº£m Ä‘Æ°á»£c** = Cost ban Ä‘áº§u - Cost cuá»‘i
-   `cost_history[-1]`: Pháº§n tá»­ cuá»‘i cÃ¹ng cá»§a list (Python indexing)

**Ká»³ vá»ng:**

-   Cost giáº£m tá»« ~0.693 xuá»‘ng gáº§n 0
-   w, b sáº½ cÃ³ giÃ¡ trá»‹ khÃ¡c 0

---

#### **5.4. Váº½ Biá»ƒu Äá»“ Há»™i Tá»¥**

```python
plt.figure(figsize=(8,5))
plt.subplot()
plt.plot(cost_history, 'b', linewidth=2)
plt.title(f"Sá»± há»™i tá»¥ cá»§a hÃ m Cost J(w,b) = {cost_history[-1]:.8f}", fontsize=14, fontweight='bold')
plt.xlabel("Iteration")
plt.ylabel("Cost")
plt.grid(True, linestyle='--', alpha=0.4)
```

**Giáº£i thÃ­ch:**

-   **`plt.figure(figsize=(8,5))`:** Táº¡o khung hÃ¬nh kÃ­ch thÆ°á»›c 8Ã—5 inch
-   **`plt.plot(cost_history, 'b', linewidth=2)`:**
    -   Váº½ Ä‘Æ°á»ng biá»ƒu diá»…n Cost qua cÃ¡c iteration
    -   `'b'`: MÃ u xanh (blue)
    -   `linewidth=2`: Äá»™ dÃ y 2
-   **Title:** Hiá»ƒn thá»‹ giÃ¡ trá»‹ Cost cuá»‘i cÃ¹ng
-   **Trá»¥c X:** Sá»‘ iteration (0, 1, 2, ..., 999)
-   **Trá»¥c Y:** GiÃ¡ trá»‹ Cost
-   **Grid:** LÆ°á»›i ná»n Ä‘á»ƒ dá»… Ä‘á»c giÃ¡ trá»‹

**Ã nghÄ©a biá»ƒu Ä‘á»“:**

Biá»ƒu Ä‘á»“ nÃ y gá»i lÃ  **Convergence Plot** (Biá»ƒu Ä‘á»“ há»™i tá»¥):

-   Cho tháº¥y Cost giáº£m dáº§n qua tá»«ng iteration
-   Náº¿u Cost giáº£m Ä‘á»u Ä‘áº·n â†’ thuáº­t toÃ¡n Ä‘ang hoáº¡t Ä‘á»™ng tá»‘t
-   Náº¿u Cost tÄƒng â†’ cÃ³ váº¥n Ä‘á» (learning rate quÃ¡ lá»›n, bug code, ...)
-   Náº¿u Cost khÃ´ng Ä‘á»•i â†’ Ä‘Ã£ há»™i tá»¥ hoáº·c learning rate quÃ¡ nhá»

**HÃ¬nh dáº¡ng mong Ä‘á»£i:**

-   Giai Ä‘oáº¡n Ä‘áº§u: Giáº£m nhanh (gradient lá»›n)
-   Giai Ä‘oáº¡n giá»¯a: Giáº£m cháº­m dáº§n
-   Giai Ä‘oáº¡n cuá»‘i: Gáº§n nhÆ° pháº³ng (Ä‘Ã£ há»™i tá»¥)

---

#### **5.5. LÆ°u vÃ  Hiá»ƒn Thá»‹**

```python
plt.tight_layout()
plt.savefig('results/ex1b_gradient_descent_convergence.png',
            dpi=300, bbox_inches='tight')
plt.show()
```

**Giáº£i thÃ­ch:**

-   **`tight_layout()`:** Tá»± Ä‘á»™ng Ä‘iá»u chá»‰nh khoáº£ng cÃ¡ch
-   **`savefig`:** LÆ°u vÃ o thÆ° má»¥c `results/` vá»›i tÃªn file rÃµ rÃ ng
    -   `dpi=300`: Äá»™ phÃ¢n giáº£i cao (cháº¥t lÆ°á»£ng in áº¥n)
    -   `bbox_inches='tight'`: Cáº¯t bá» khoáº£ng tráº¯ng thá»«a
-   **`show()`:** Hiá»ƒn thá»‹ biá»ƒu Ä‘á»“ lÃªn mÃ n hÃ¬nh

---

## ğŸ“Š Output vÃ  Káº¿t Quáº£

### ğŸ–¥ï¸ Console Output (Dá»± Kiáº¿n)

```
============================================================
CÃ‚U B â€“ Cáº­p nháº­t w, b báº±ng thuáº­t toÃ¡n Gradient Descent
============================================================
Cost ban Ä‘áº§u (w=0, b=0):  0.69314718

Iteration    1 :  w = 0.000000,  b = 0.000000,  Cost = 0.69314718
Iteration  200 :  w = 0.324156,  b = -0.382341,  Cost = 0.45123456
Iteration  400 :  w = 0.548234,  b = -0.654123,  Cost = 0.28765432
Iteration  600 :  w = 0.712345,  b = -0.876543,  Cost = 0.17654321
Iteration  800 :  w = 0.834567,  b = -1.045678,  Cost = 0.10234567
Iteration 1000 :  w = 0.923456,  b = -1.187654,  Cost = 0.05678901

Káº¾T QUáº¢ SAU TRAINING:
w_update = 0.92345678
b_update = -1.18765432
Cost cuá»‘i = 0.05678901
Cost giáº£m Ä‘Æ°á»£c: 0.63635817
```

**LÆ°u Ã½:** CÃ¡c sá»‘ trÃªn lÃ  vÃ­ dá»¥ minh há»a. GiÃ¡ trá»‹ thá»±c táº¿ phá»¥ thuá»™c vÃ o implementation.

---

### ğŸ“ˆ PhÃ¢n TÃ­ch Káº¿t Quáº£

#### **1. Cost Ban Äáº§u vs Cost Cuá»‘i**

-   **Cost ban Ä‘áº§u:** 0.69314718 (mÃ´ hÃ¬nh ngáº«u nhiÃªn)
-   **Cost cuá»‘i cÃ¹ng:** ~0.057 (giáº£m hÆ¡n **91%**)
-   **Cost giáº£m Ä‘Æ°á»£c:** ~0.636

**Ã nghÄ©a:**

MÃ´ hÃ¬nh Ä‘Ã£ há»c Ä‘Æ°á»£c **pattern** trong dá»¯ liá»‡u:

-   Ban Ä‘áº§u: Dá»± Ä‘oÃ¡n 50-50 cho má»i Ä‘iá»ƒm
-   Sau training: Dá»± Ä‘oÃ¡n chÃ­nh xÃ¡c hÆ¡n ráº¥t nhiá»u

---

#### **2. Tham Sá»‘ w_update vÃ  b_update**

**Giáº£ sá»­ w â‰ˆ 0.92, b â‰ˆ -1.19**

**Ã nghÄ©a:**

-   **w > 0:** Quan há»‡ **dÆ°Æ¡ng** giá»¯a x vÃ  y
    -   x cÃ ng lá»›n â†’ xÃ¡c suáº¥t y=1 cÃ ng cao
    -   PhÃ¹ há»£p vá»›i dá»¯ liá»‡u: Ä‘iá»ƒm cÃ³ x lá»›n (3, 2) thÃ¬ y=1
-   **b < 0:** Há»‡ sá»‘ cháº·n Ã¢m
    -   Dá»‹ch chuyá»ƒn Ä‘Æ°á»ng sigmoid sang pháº£i
    -   GiÃºp phÃ¢n loáº¡i chÃ­nh xÃ¡c hÆ¡n

**Decision Boundary:**

Äiá»ƒm phÃ¢n chia giá»¯a 2 lá»›p xáº£y ra khi h(x) = 0.5:

$$\sigma(wx + b) = 0.5$$
$$wx + b = 0$$
$$x = -\frac{b}{w}$$

Vá»›i w â‰ˆ 0.92, b â‰ˆ -1.19:

$$x_{boundary} = -\frac{-1.19}{0.92} \approx 1.29$$

**Diá»…n giáº£i:**

-   Náº¿u x < 1.29 â†’ dá»± Ä‘oÃ¡n y=0
-   Náº¿u x > 1.29 â†’ dá»± Ä‘oÃ¡n y=1

Kiá»ƒm tra vá»›i dá»¯ liá»‡u:

-   x = 0.5, 1.0, 1.5: Gáº§n hoáº·c nhá» hÆ¡n 1.29 â†’ y=0 âœ“
-   x = 2.0, 3.0: Lá»›n hÆ¡n 1.29 â†’ y=1 âœ“
-   x = 1.0 (cÃ³ y=1): HÆ¡i trÃ¹ng nhÆ°ng gáº§n boundary

---

#### **3. Biá»ƒu Äá»“ Há»™i Tá»¥**

Biá»ƒu Ä‘á»“ cho tháº¥y:

**Giai Ä‘oáº¡n 1 (Iteration 0-200):**

-   Cost giáº£m **nhanh** tá»« 0.693 â†’ ~0.45
-   Gradient lá»›n â†’ cáº­p nháº­t máº¡nh
-   MÃ´ hÃ¬nh há»c Ä‘Æ°á»£c pattern cÆ¡ báº£n

**Giai Ä‘oáº¡n 2 (Iteration 200-600):**

-   Cost giáº£m **cháº­m hÆ¡n** tá»« 0.45 â†’ ~0.18
-   Gradient giáº£m dáº§n
-   MÃ´ hÃ¬nh tinh chá»‰nh chi tiáº¿t

**Giai Ä‘oáº¡n 3 (Iteration 600-1000):**

-   Cost giáº£m **ráº¥t cháº­m** tá»« 0.18 â†’ ~0.06
-   ÄÆ°á»ng cong gáº§n nhÆ° pháº³ng
-   MÃ´ hÃ¬nh Ä‘Ã£ gáº§n **há»™i tá»¥** (convergence)

**HÃ¬nh dáº¡ng:** ÄÆ°á»ng cong giáº£m mÆ°á»£t, khÃ´ng dao Ä‘á»™ng

-   âœ“ Chá»©ng tá» learning rate phÃ¹ há»£p
-   âœ“ Thuáº­t toÃ¡n á»•n Ä‘á»‹nh

---

#### **4. So SÃ¡nh Question A vs Question B**

| TiÃªu chÃ­           | Question A          | Question B                |
| ------------------ | ------------------- | ------------------------- |
| w                  | 0                   | ~0.92                     |
| b                  | 0                   | ~-1.19                    |
| J(w,b)             | 0.693               | ~0.057                    |
| Kháº£ nÄƒng phÃ¢n loáº¡i | KhÃ´ng cÃ³ (50-50)    | Tá»‘t (~94% accuracy)       |
| ÄÆ°á»ng sigmoid      | Tháº³ng ngang táº¡i 0.5 | S-curve phÃ¢n loáº¡i rÃµ rÃ ng |

---

## ğŸ“š Kiáº¿n Thá»©c Bá»• Sung

### **CÃ´ng Thá»©c Äáº¡o HÃ m (Chá»©ng Minh)**

**Chain Rule cho Gradient:**

$$\frac{\partial J}{\partial w} = \frac{\partial J}{\partial h} \cdot \frac{\partial h}{\partial z} \cdot \frac{\partial z}{\partial w}$$

**TÃ­nh tá»«ng thÃ nh pháº§n:**

1. $\frac{\partial J}{\partial h} = -\frac{y}{h} + \frac{1-y}{1-h}$

2. $\frac{\partial h}{\partial z} = h(1-h)$ (tÃ­nh cháº¥t sigmoid)

3. $\frac{\partial z}{\partial w} = x$

**Káº¿t há»£p:**

$$\frac{\partial J}{\partial w} = \left(-\frac{y}{h} + \frac{1-y}{1-h}\right) \cdot h(1-h) \cdot x$$

$$= \left(-\frac{y(1-h) - (1-y)h}{h(1-h)}\right) \cdot h(1-h) \cdot x$$

$$= \left(-\frac{y - yh - h + yh}{h(1-h)}\right) \cdot h(1-h) \cdot x$$

$$= (h - y) \cdot x$$

**Káº¿t luáº­n:** $\frac{\partial J}{\partial w} = (h - y) \cdot x$ âœ“

TÆ°Æ¡ng tá»± cho b!

---