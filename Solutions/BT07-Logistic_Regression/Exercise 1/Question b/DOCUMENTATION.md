# ğŸ“˜ E-Learning 5 - Exercise 1 - Question B: Gradient Descent

## ğŸ¯ Má»¥c TiÃªu BÃ i Táº­p

BÃ i táº­p yÃªu cáº§u **cáº­p nháº­t tham sá»‘ w vÃ  b** báº±ng thuáº­t toÃ¡n **Gradient Descent** Ä‘á»ƒ tá»‘i Æ°u hÃ³a mÃ´ hÃ¬nh Logistic Regression, sau Ä‘Ã³ tÃ­nh giÃ¡ trá»‹ hÃ m chi phÃ­ J(w_update, b_update).

### ğŸ“Š Äá» BÃ i

Tiáº¿p theo tá»« Question A:

-   Sá»­ dá»¥ng cÃ¹ng táº­p dá»¯ liá»‡u vÃ  tham sá»‘ ban Ä‘áº§u
-   **YÃªu cáº§u:** Cáº­p nháº­t w, b theo thuáº­t toÃ¡n Gradient Descent vÃ  tÃ­nh J(w_update, b_update)

**Ká»³ vá»ng:** GiÃ¡ trá»‹ Cost sáº½ giáº£m tá»« ~0.693 (Question A) xuá»‘ng gáº§n 0

---

## ğŸ’» PhÃ¢n TÃ­ch Source Code Chi Tiáº¿t

### 1ï¸âƒ£ Import vÃ  Khá»Ÿi Táº¡o

```python
import numpy as np
import matplotlib.pyplot as plt

# Dá»¯ liá»‡u
X = np.array([0.5, 1, 1.5, 3, 2, 1])
y = np.array([0, 0, 0, 1, 1, 1])

# Tham sá»‘ ban Ä‘áº§u
w = 0
b = 0
alpha = 0.0001
```

**Giáº£i thÃ­ch:**

Pháº§n khá»Ÿi táº¡o nÃ y giá»‘ng vá»›i Question A, thiáº¿t láº­p mÃ´i trÆ°á»ng vÃ  dá»¯ liá»‡u cho bÃ i toÃ¡n. Dá»¯ liá»‡u X vÃ  y chá»©a 6 Ä‘iá»ƒm dá»¯ liá»‡u cho bÃ i toÃ¡n phÃ¢n loáº¡i nhá»‹ phÃ¢n, trong Ä‘Ã³ X lÃ  cÃ¡c giÃ¡ trá»‹ Ä‘áº·c trÆ°ng vÃ  y lÃ  cÃ¡c nhÃ£n tÆ°Æ¡ng á»©ng (0 hoáº·c 1). Tham sá»‘ ban Ä‘áº§u w=0 vÃ  b=0 thá»ƒ hiá»‡n mÃ´ hÃ¬nh chÆ°a há»c Ä‘Æ°á»£c thÃ´ng tin gÃ¬, cÃ²n learning rate alpha=0.0001 lÃ  má»™t bÆ°á»›c nháº£y ráº¥t nhá», Ä‘áº£m báº£o mÃ´ hÃ¬nh há»c tá»« tá»« Ä‘á»ƒ trÃ¡nh overshooting (nháº£y quÃ¡ xa khá»i Ä‘iá»ƒm tá»‘i Æ°u).

---

### 2ï¸âƒ£ CÃ¡c HÃ m CÆ¡ Báº£n

#### **HÃ m Sigmoid**

```python
def sigmoid(z):
    return 1 / (1 + np.exp(-z))
```

**Giáº£i thÃ­ch:**

HÃ m kÃ­ch hoáº¡t sigmoid giá»‘ng vá»›i Question A, Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ chuyá»ƒn Ä‘á»•i giÃ¡ trá»‹ tuyáº¿n tÃ­nh thÃ nh xÃ¡c suáº¥t. CÃ´ng thá»©c toÃ¡n há»c cá»§a sigmoid lÃ :

$$\sigma(z) = \frac{1}{1 + e^{-z}}$$

HÃ m nÃ y chuyá»ƒn Ä‘á»•i báº¥t ká»³ giÃ¡ trá»‹ z nÃ o thÃ nh xÃ¡c suáº¥t trong khoáº£ng (0, 1), lÃ  thÃ nh pháº§n cá»‘t lÃµi cá»§a Logistic Regression cho phÃ©p biá»ƒu diá»…n dá»± Ä‘oÃ¡n dÆ°á»›i dáº¡ng xÃ¡c suáº¥t.

---

#### **HÃ m Cost (Binary Cross Entropy)**

```python
def compute_cost(X, y, w, b):
    m = len(X)
    z = w * X + b
    h = sigmoid(z)
    eps = 1e-15
    cost = -(1/m) * np.sum(y * np.log(h + eps) + (1 - y) * np.log(1 - h + eps))
    return cost
```

**Giáº£i thÃ­ch:**

HÃ m nÃ y tÃ­nh chi phÃ­ Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ cháº¥t lÆ°á»£ng mÃ´ hÃ¬nh qua ba bÆ°á»›c chÃ­nh. BÆ°á»›c Ä‘áº§u tiÃªn tÃ­nh giÃ¡ trá»‹ tuyáº¿n tÃ­nh theo cÃ´ng thá»©c:

$$z = wx + b$$

Tiáº¿p theo tÃ­nh xÃ¡c suáº¥t dá»± Ä‘oÃ¡n báº±ng cÃ¡ch Ä‘Æ°a z qua sigmoid. Cuá»‘i cÃ¹ng tÃ­nh giÃ¡ trá»‹ Cost báº±ng Binary Cross-Entropy:

$$J(w,b) = -\frac{1}{m}\sum_{i=1}^{m}[y_i\log(h_i) + (1-y_i)\log(1-h_i)]$$

Tham sá»‘ eps = 1e-15 Ä‘Æ°á»£c thÃªm vÃ o Ä‘á»ƒ trÃ¡nh lá»—i toÃ¡n há»c khi tÃ­nh log(0). Má»¥c tiÃªu cá»§a thuáº­t toÃ¡n lÃ  minimize (giáº£m thiá»ƒu) giÃ¡ trá»‹ J(w,b) nÃ y.

---

### 3ï¸âƒ£ HÃ m TÃ­nh Gradient - TrÃ¡i Tim Cá»§a Gradient Descent

```python
def compute_gradient(X, y, w, b):
    m = len(X)
    z = w * X + b
    h = sigmoid(z)
    error = h - y
    dw = (1/m) * np.sum(error * X)
    db = (1/m) * np.sum(error)
    return dw, db
```

**Giáº£i thÃ­ch Chi Tiáº¿t:**

ÄÃ¢y lÃ  hÃ m **quan trá»ng nháº¥t** - tÃ­nh toÃ¡n **gradient** (Ä‘áº¡o hÃ m) cá»§a Cost function theo w vÃ  b.

#### **BÆ°á»›c 1: TÃ­nh z vÃ  h**

```python
z = w * X + b
h = sigmoid(z)
```

ÄÃ¢y lÃ  bÆ°á»›c tÃ­nh giÃ¡ trá»‹ tuyáº¿n tÃ­nh vÃ  xÃ¡c suáº¥t dá»± Ä‘oÃ¡n, tÆ°Æ¡ng tá»± nhÆ° trong hÃ m compute_cost. GiÃ¡ trá»‹ z Ä‘Æ°á»£c tÃ­nh theo cÃ´ng thá»©c z = w\*X + b, sau Ä‘Ã³ Ä‘Æ°á»£c chuyá»ƒn Ä‘á»•i thÃ nh xÃ¡c suáº¥t h qua hÃ m sigmoid.

#### **BÆ°á»›c 2: TÃ­nh Error**

```python
error = h - y
```

**Ã nghÄ©a:**

Biáº¿n `error` Ä‘áº¡i diá»‡n cho sai sá»‘ giá»¯a giÃ¡ trá»‹ dá»± Ä‘oÃ¡n (h) vÃ  giÃ¡ trá»‹ thá»±c táº¿ (y). Náº¿u error > 0 nghÄ©a lÃ  mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n cao hÆ¡n thá»±c táº¿ (overestimate). NgÆ°á»£c láº¡i, náº¿u error < 0 thÃ¬ mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n tháº¥p hÆ¡n thá»±c táº¿ (underestimate). Khi error = 0 nghÄ©a lÃ  dá»± Ä‘oÃ¡n hoÃ n toÃ n chÃ­nh xÃ¡c.

**VÃ­ dá»¥:**

Vá»›i h = 0.8 vÃ  y = 1, ta cÃ³ error = -0.2, nghÄ©a lÃ  dá»± Ä‘oÃ¡n hÆ¡i tháº¥p. Vá»›i h = 0.3 vÃ  y = 0, ta cÃ³ error = 0.3, nghÄ©a lÃ  dá»± Ä‘oÃ¡n hÆ¡i cao.

#### **BÆ°á»›c 3: TÃ­nh Gradient cá»§a w**

```python
dw = (1/m) * np.sum(error * X)
```

**CÃ´ng thá»©c toÃ¡n há»c:**

$$\frac{\partial J}{\partial w} = \frac{1}{m}\sum_{i=1}^{m}(h_i - y_i) \cdot x_i$$

**Giáº£i thÃ­ch:**

Biáº¿n dw lÃ  Ä‘áº¡o hÃ m riÃªng cá»§a hÃ m Cost function theo tham sá»‘ w, cho biáº¿t Cost thay Ä‘á»•i nhÆ° tháº¿ nÃ o khi w thay Ä‘á»•i. Biá»ƒu thá»©c error \* X táº¡o ra sai sá»‘ cÃ³ trá»ng sá»‘ (weighted error). Náº¿u giÃ¡ trá»‹ xi lá»›n vÃ  error lá»›n thÃ¬ gradient sáº½ lá»›n, nghÄ©a lÃ  cáº§n Ä‘iá»u chá»‰nh w nhiá»u. NgÆ°á»£c láº¡i, náº¿u xi nhá» hoáº·c error nhá» thÃ¬ gradient nhá», chá»‰ cáº§n Ä‘iá»u chá»‰nh w Ã­t.

**Ã nghÄ©a hÃ¬nh há»c:**

Khi dw > 0, Cost tÄƒng khi w tÄƒng, do Ä‘Ã³ cáº§n giáº£m w Ä‘á»ƒ giáº£m Cost. Khi dw < 0, Cost tÄƒng khi w giáº£m, do Ä‘Ã³ cáº§n tÄƒng w. Khi dw â‰ˆ 0 nghÄ©a lÃ  mÃ´ hÃ¬nh Ä‘ang á»Ÿ gáº§n Ä‘iá»ƒm tá»‘i Æ°u.

#### **BÆ°á»›c 4: TÃ­nh Gradient cá»§a b**

```python
db = (1/m) * np.sum(error)
```

**CÃ´ng thá»©c toÃ¡n há»c:**

$$\frac{\partial J}{\partial b} = \frac{1}{m}\sum_{i=1}^{m}(h_i - y_i)$$

**Giáº£i thÃ­ch:**

Biáº¿n db lÃ  Ä‘áº¡o hÃ m riÃªng cá»§a hÃ m Cost function theo tham sá»‘ b. ÄÃ¢y lÃ  tá»•ng cÃ¡c sai sá»‘ khÃ´ng nhÃ¢n vá»›i X vÃ¬ Ä‘áº¡o hÃ m cá»§a b trong biá»ƒu thá»©c z = wx + b lÃ  1. Khi db > 0 nghÄ©a lÃ  dá»± Ä‘oÃ¡n trung bÃ¬nh cao hÆ¡n thá»±c táº¿, do Ä‘Ã³ cáº§n giáº£m b Ä‘á»ƒ háº¡ tháº¥p dá»± Ä‘oÃ¡n xuá»‘ng. NgÆ°á»£c láº¡i, khi db < 0 nghÄ©a lÃ  dá»± Ä‘oÃ¡n trung bÃ¬nh tháº¥p hÆ¡n thá»±c táº¿, cáº§n tÄƒng b Ä‘á»ƒ nÃ¢ng cao dá»± Ä‘oÃ¡n.

#### **Táº¡i sao cÃ´ng thá»©c nÃ y Ä‘Ãºng?**

**Chá»©ng minh toÃ¡n há»c** (simplified):

Tá»« Cost function:
$$J = -\frac{1}{m}\sum[y\log(h) + (1-y)\log(1-h)]$$

Äáº¡o hÃ m theo w (chain rule):
$$\frac{\partial J}{\partial w} = \frac{\partial J}{\partial h} \cdot \frac{\partial h}{\partial z} \cdot \frac{\partial z}{\partial w}$$

Trong Ä‘Ã³:

-   $\frac{\partial J}{\partial h} = -\frac{y}{h} + \frac{1-y}{1-h}$
-   $\frac{\partial h}{\partial z} = h(1-h)$ (tÃ­nh cháº¥t Ä‘áº¹p cá»§a sigmoid)
-   $\frac{\partial z}{\partial w} = x$

Káº¿t há»£p láº¡i:
$$\frac{\partial J}{\partial w} = (h - y) \cdot x$$

Trung bÃ¬nh trÃªn m máº«u:
$$\frac{\partial J}{\partial w} = \frac{1}{m}\sum(h_i - y_i) \cdot x_i$$

---

### 4ï¸âƒ£ Thuáº­t ToÃ¡n Gradient Descent - TrÃ¡i Tim Cá»§a Machine Learning

```python
def gradient_descent(X, y, w, b, alpha, num_iterations):
    cost_history, w_history, b_history = [], [], []

    for i in range(num_iterations):
        dw, db = compute_gradient(X, y, w, b)
        w -= alpha * dw
        b -= alpha * db
        cost = compute_cost(X, y, w, b)

        cost_history.append(cost)
        w_history.append(w)
        b_history.append(b)

        # In 1 vÃ i vÃ²ng láº·p quan trá»ng
        if i == 0 or (i + 1) % 200 == 0 or i == num_iterations - 1:
            print(f"Iteration {i+1:4d} :  w = {w:.6f},  b = {b:.6f},  Cost = {cost:.8f}")

    return w, b, cost_history, w_history, b_history
```

**Giáº£i thÃ­ch Chi Tiáº¿t:**

#### **Khá»Ÿi táº¡o**

```python
cost_history, w_history, b_history = [], [], []
```

Ba danh sÃ¡ch rá»—ng Ä‘Æ°á»£c táº¡o Ä‘á»ƒ lÆ°u lá»‹ch sá»­ quÃ¡ trÃ¬nh training. Biáº¿n `cost_history` lÆ°u lá»‹ch sá»­ giÃ¡ trá»‹ Cost qua cÃ¡c iteration, `w_history` lÆ°u lá»‹ch sá»­ giÃ¡ trá»‹ w, vÃ  `b_history` lÆ°u lá»‹ch sá»­ giÃ¡ trá»‹ b. Má»¥c Ä‘Ã­ch cá»§a viá»‡c lÆ°u lá»‹ch sá»­ lÃ  Ä‘á»ƒ phÃ¢n tÃ­ch vÃ  truyá»n visualization sau nÃ y, giÃºp theo dÃµi quÃ¡ trÃ¬nh há»™i tá»¥ cá»§a mÃ´ hÃ¬nh.

#### **VÃ²ng láº·p chÃ­nh**

```python
for i in range(num_iterations):
```

VÃ²ng láº·p nÃ y láº·p láº¡i `num_iterations` láº§n (trong code lÃ  1000 láº§n), má»—i iteration thá»ƒ hiá»‡n má»™t bÆ°á»›c cáº­p nháº­t tham sá»‘ Ä‘á»ƒ cáº£i thiá»‡n mÃ´ hÃ¬nh.

#### **BÆ°á»›c 1: TÃ­nh Gradient**

```python
dw, db = compute_gradient(X, y, w, b)
```

HÃ m nÃ y tÃ­nh Ä‘áº¡o hÃ m cá»§a hÃ m Cost function táº¡i Ä‘iá»ƒm (w, b) hiá»‡n táº¡i. Gradient chá»‰ ra hÆ°á»›ng tÄƒng nhanh nháº¥t cá»§a hÃ m Cost, tá»« Ä‘Ã³ thuáº­t toÃ¡n sáº½ Ä‘i ngÆ°á»£c hÆ°á»›ng Ä‘á»ƒ giáº£m Cost.

#### **BÆ°á»›c 2: Cáº­p Nháº­t Tham Sá»‘**

```python
w -= alpha * dw
b -= alpha * db
```

**CÃ´ng thá»©c toÃ¡n há»c:**

$$w_{new} = w_{old} - \alpha \cdot \frac{\partial J}{\partial w}$$

$$b_{new} = b_{old} - \alpha \cdot \frac{\partial J}{\partial b}$$

**Giáº£i thÃ­ch:**

Dáº¥u trá»« (-) cho biáº¿t thuáº­t toÃ¡n Ä‘i ngÆ°á»£c hÆ°á»›ng gradient Ä‘á»ƒ giáº£m Cost. VÃ¬ gradient chá»‰ hÆ°á»›ng tÄƒng cá»§a hÃ m, nÃªn Ä‘i ngÆ°á»£c láº¡i sáº½ giÃºp giáº£m giÃ¡ trá»‹ hÃ m. Tham sá»‘ alpha (learning rate) kiá»ƒm soÃ¡t tá»‘c Ä‘á»™ há»c cá»§a mÃ´ hÃ¬nh. Náº¿u alpha quÃ¡ lá»›n, mÃ´ hÃ¬nh há»c nhanh nhÆ°ng cÃ³ thá»ƒ bá» lá»¡ Ä‘iá»ƒm tá»‘i Æ°u (overshooting). Náº¿u alpha quÃ¡ nhá», mÃ´ hÃ¬nh há»c cháº­m nhÆ°ng á»•n Ä‘á»‹nh hÆ¡n. GiÃ¡ trá»‹ 0.0001 trong bÃ i nÃ y ráº¥t nhá», dáº«n Ä‘áº¿n viá»‡c mÃ´ hÃ¬nh há»c ráº¥t cháº­m.

**VÃ­ dá»¥ minh há»a:**

Giáº£ sá»­ á»Ÿ iteration 1 cÃ³ dw = 2.5, db = 1.3, alpha = 0.0001, vÃ  tham sá»‘ ban Ä‘áº§u w_old = 0, b_old = 0. Sau khi cáº­p nháº­t:

w_new = 0 - 0.0001 Ã— 2.5 = -0.00025

b_new = 0 - 0.0001 Ã— 1.3 = -0.00013

Káº¿t quáº£ cho tháº¥y bÆ°á»›c nháº£y ráº¥t nhá», pháº£n Ã¡nh tá»‘c Ä‘á»™ há»c cháº­m rÃ£i cá»§a mÃ´ hÃ¬nh.

#### **BÆ°á»›c 3: TÃ­nh Cost má»›i**

```python
cost = compute_cost(X, y, w, b)
```

Sau khi cáº­p nháº­t tham sá»‘, hÃ m tÃ­nh láº¡i giÃ¡ trá»‹ Cost vá»›i cÃ¡c tham sá»‘ má»›i vá»«a cáº­p nháº­t. BÆ°á»›c nÃ y cho phÃ©p kiá»ƒm tra xem giÃ¡ trá»‹ Cost cÃ³ giáº£m hay khÃ´ng, Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ hiá»‡u quáº£ cá»§a quÃ¡ trÃ¬nh há»c.

#### **BÆ°á»›c 4: LÆ°u Lá»‹ch Sá»­**

```python
cost_history.append(cost)
w_history.append(w)
b_history.append(b)
```

CÃ¡c giÃ¡ trá»‹ hiá»‡n táº¡i cá»§a cost, w, vÃ  b Ä‘Æ°á»£c lÆ°u láº¡i vÃ o cÃ¡c danh sÃ¡ch tÆ°Æ¡ng á»©ng. Viá»‡c lÆ°u lá»‹ch sá»­ nÃ y cho phÃ©p phÃ¢n tÃ­ch sau vÃ  váº½ biá»ƒu Ä‘á»“ há»™i tá»¥ (convergence plot) Ä‘á»ƒ theo dÃµi quÃ¡ trÃ¬nh training trá»±c quan.

#### **BÆ°á»›c 5: In Progress**

```python
if i == 0 or (i + 1) % 200 == 0 or i == num_iterations - 1:
    print(f"Iteration {i+1:4d} :  w = {w:.6f},  b = {b:.6f},  Cost = {cost:.8f}")
```

**Giáº£i thÃ­ch:**

Pháº§n code nÃ y in ra má»™t sá»‘ iteration quan trá»ng Ä‘á»ƒ theo dÃµi tiáº¿n trÃ¬nh, bao gá»“m iteration Ä‘áº§u tiÃªn (i=0), má»—i 200 iterations, vÃ  iteration cuá»‘i cÃ¹ng. KhÃ´ng in táº¥t cáº£ 1000 iteration vÃ¬ sáº½ quÃ¡ nhiá»u thÃ´ng tin. Format sá»‘ Ä‘Æ°á»£c Ä‘iá»u chá»‰nh cáº©n tháº­n: `{i+1:4d}` in sá»‘ iteration cÄƒn pháº£i 4 kÃ½ tá»±, `{w:.6f}` in w vá»›i 6 chá»¯ sá»‘ tháº­p phÃ¢n, vÃ  `{cost:.8f}` in cost vá»›i 8 chá»¯ sá»‘ tháº­p phÃ¢n cho Ä‘á»™ chÃ­nh xÃ¡c cao.

#### **Return**

```python
return w, b, cost_history, w_history, b_history
```

HÃ m tráº£ vá» cÃ¡c giÃ¡ trá»‹ `w, b` lÃ  tham sá»‘ tá»‘i Æ°u sau khi training xong, cÃ¹ng vá»›i `cost_history, w_history, b_history` lÃ  lá»‹ch sá»­ cÃ¡c giÃ¡ trá»‹ Ä‘á»ƒ sá»­ dá»¥ng cho visualization vÃ  phÃ¢n tÃ­ch.

---

### 5ï¸âƒ£ Pháº§n Cháº¡y ChÃ­nh vÃ  Visualization

#### **5.1. Header vÃ  Cost Ban Äáº§u**

```python
print("=" * 60)
print("CÃ‚U B â€“ Cáº­p nháº­t w, b báº±ng thuáº­t toÃ¡n Gradient Descent")
print("=" * 60)

initial_cost = compute_cost(X, y, w, b)
print(f"Cost ban Ä‘áº§u (w=0, b=0):  {initial_cost:.8f}\n")
```

**Giáº£i thÃ­ch:**

Pháº§n code nÃ y in tiÃªu Ä‘á» Ä‘á»ƒ ngÆ°á»i Ä‘á»c dá»… theo dÃµi, sau Ä‘Ã³ tÃ­nh vÃ  hiá»ƒn thá»‹ giÃ¡ trá»‹ Cost ban Ä‘áº§u trÆ°á»›c khi training. GiÃ¡ trá»‹ nÃ y dá»± kiáº¿n lÃ  xáº¥p xá»‰ 0.693, giá»‘ng nhÆ° trong Question A, thá»ƒ hiá»‡n mÃ´ hÃ¬nh chÆ°a Ä‘Æ°á»£c há»c.

---

#### **5.2. Cháº¡y Gradient Descent**

```python
num_iterations = 1000
w_final, b_final, cost_history, w_history, b_history = gradient_descent(
    X, y, w, b, alpha, num_iterations
)
```

**Giáº£i thÃ­ch:**

Sá»‘ iterations Ä‘Æ°á»£c Ä‘áº·t lÃ  1000, nghÄ©a lÃ  thuáº­t toÃ¡n sáº½ thá»±c hiá»‡n 1000 bÆ°á»›c cáº­p nháº­t tham sá»‘. HÃ m `gradient_descent` Ä‘Æ°á»£c gá»i vá»›i cÃ¡c tham sá»‘: dá»¯ liá»‡u X vÃ  y, tham sá»‘ ban Ä‘áº§u w=0 vÃ  b=0, learning rate alpha=0.0001, vÃ  sá»‘ iterations = 1000. Káº¿t quáº£ tráº£ vá» bao gá»“m `w_final, b_final` lÃ  tham sá»‘ sau khi training xong, vÃ  `cost_history, w_history, b_history` lÃ  lá»‹ch sá»­ cÃ¡c giÃ¡ trá»‹ Ä‘á»ƒ váº½ biá»ƒu Ä‘á»“.

---

#### **5.3. In Káº¿t Quáº£**

```python
print("\nKáº¾T QUáº¢ SAU TRAINING:")
print(f"w_update = {w_final:.8f}")
print(f"b_update = {b_final:.8f}")
print(f"Cost cuá»‘i = {cost_history[-1]:.8f}")
print(f"Cost giáº£m Ä‘Æ°á»£c: {initial_cost - cost_history[-1]:.8f}")
```

**Giáº£i thÃ­ch:**

Pháº§n code nÃ y in ra tham sá»‘ cuá»‘i cÃ¹ng w_update vÃ  b_update sau quÃ¡ trÃ¬nh training, cÃ¹ng vá»›i giÃ¡ trá»‹ Cost cuá»‘i cÃ¹ng sau 1000 iterations. Äáº·c biá»‡t, pháº§n code cÃ²n tÃ­nh vÃ  hiá»ƒn thá»‹ lÆ°á»£ng Cost giáº£m Ä‘Æ°á»£c báº±ng cÃ¡ch láº¥y Cost ban Ä‘áº§u trá»« Ä‘i Cost cuá»‘i. Biáº¿u thá»©c `cost_history[-1]` sá»­ dá»¥ng indexing cá»§a Python Ä‘á»ƒ láº¥y pháº§n tá»­ cuá»‘i cÃ¹ng cá»§a danh sÃ¡ch.

**Ká»³ vá»ng:**

GiÃ¡ trá»‹ Cost dá»± kiáº¿n giáº£m tá»« xáº¥p xá»‰ 0.693 xuá»‘ng gáº§n 0, vÃ  cÃ¡c tham sá»‘ w, b sáº½ cÃ³ giÃ¡ trá»‹ khÃ¡c 0 thá»ƒ hiá»‡n mÃ´ hÃ¬nh Ä‘Ã£ há»c Ä‘Æ°á»£c pattern tá»« dá»¯ liá»‡u.

---

#### **5.4. Váº½ Biá»ƒu Äá»“ Há»™i Tá»¥**

```python
plt.figure(figsize=(8,5))
plt.subplot()
plt.plot(cost_history, 'b', linewidth=2)
plt.title(f"Sá»± há»™i tá»¥ cá»§a hÃ m Cost J(w,b) = {cost_history[-1]:.8f}", fontsize=14, fontweight='bold')
plt.xlabel("Iteration")
plt.ylabel("Cost")
plt.grid(True, linestyle='--', alpha=0.4)
```

**Giáº£i thÃ­ch:**

Lá»‡nh `plt.figure(figsize=(8,5))` táº¡o má»™t khung hÃ¬nh vá»›i kÃ­ch thÆ°á»›c 8Ã—5 inch. HÃ m `plt.plot(cost_history, 'b', linewidth=2)` váº½ Ä‘Æ°á»ng biá»ƒu diá»…n giÃ¡ trá»‹ Cost qua cÃ¡c iteration vá»›i mÃ u xanh (blue) vÃ  Ä‘á»™ dÃ y 2. TiÃªu Ä‘á» hiá»ƒn thá»‹ giÃ¡ trá»‹ Cost cuá»‘i cÃ¹ng Ä‘á»ƒ ngÆ°á»i xem biáº¿t káº¿t quáº£ Ä‘áº¡t Ä‘Æ°á»£c. Trá»¥c X biá»ƒu thá»‹ sá»‘ iteration (0, 1, 2, ..., 999), trong khi trá»¥c Y hiá»ƒn thá»‹ giÃ¡ trá»‹ Cost. LÆ°á»›i ná»n (grid) Ä‘Æ°á»£c báº­t Ä‘á»ƒ dá»… Ä‘á»c giÃ¡ trá»‹ trÃªn biá»ƒu Ä‘á»“.

**Ã nghÄ©a biá»ƒu Ä‘á»“:**

Biá»ƒu Ä‘á»“ nÃ y Ä‘Æ°á»£c gá»i lÃ  Convergence Plot (biá»ƒu Ä‘á»“ há»™i tá»¥), cho tháº¥y Cost giáº£m dáº§n qua tá»«ng iteration. Náº¿u Cost giáº£m Ä‘á»u Ä‘áº·n nghÄ©a lÃ  thuáº­t toÃ¡n Ä‘ang hoáº¡t Ä‘á»™ng tá»‘t. Náº¿u Cost tÄƒng thÃ¬ cÃ³ váº¥n Ä‘á» (learning rate quÃ¡ lá»›n, bug trong code, hoáº·c váº¥n Ä‘á» khÃ¡c). Náº¿u Cost khÃ´ng Ä‘á»•i nghÄ©a lÃ  Ä‘Ã£ há»™i tá»¥ hoáº·c learning rate quÃ¡ nhá».

**HÃ¬nh dáº¡ng mong Ä‘á»£i:**

Giai Ä‘oáº¡n Ä‘áº§u tiÃªn thÆ°á»ng Cost giáº£m nhanh do gradient cÃ²n lá»›n. Giai Ä‘oáº¡n giá»¯a Cost giáº£m cháº­m dáº§n khi mÃ´ hÃ¬nh tiáº¿n gáº§n Ä‘iá»ƒm tá»‘i Æ°u. Giai Ä‘oáº¡n cuá»‘i Ä‘Æ°á»ng cong gáº§n nhÆ° pháº³ng, thá»ƒ hiá»‡n mÃ´ hÃ¬nh Ä‘Ã£ há»™i tá»¥.

---

#### **5.5. LÆ°u vÃ  Hiá»ƒn Thá»‹**

```python
plt.tight_layout()
plt.savefig('results/ex1b_gradient_descent_convergence.png',
            dpi=300, bbox_inches='tight')
plt.show()
```

**Giáº£i thÃ­ch:**

HÃ m `tight_layout()` tá»± Ä‘á»™ng Ä‘iá»u chá»‰nh khoáº£ng cÃ¡ch giá»¯a cÃ¡c pháº§n tá»­ trong biá»ƒu Ä‘á»“ Ä‘á»ƒ trÃ¡nh chá»“ng láº¥n. Lá»‡nh `savefig` lÆ°u biá»ƒu Ä‘á»“ vÃ o thÆ° má»¥c `results/` vá»›i tÃªn file mÃ´ táº£ rÃµ rÃ ng ná»™i dung. Tham sá»‘ `dpi=300` thiáº¿t láº­p Ä‘á»™ phÃ¢n giáº£i cao (300 DPI - cháº¥t lÆ°á»£ng in áº¥n), vÃ  `bbox_inches='tight'` cáº¯t bá» khoáº£ng tráº¯ng thá»«a xung quanh biá»ƒu Ä‘á»“. Cuá»‘i cÃ¹ng, hÃ m `show()` hiá»ƒn thá»‹ biá»ƒu Ä‘á»“ lÃªn mÃ n hÃ¬nh Ä‘á»ƒ ngÆ°á»i dÃ¹ng xem trá»±c tiáº¿p.

---

## ğŸ“Š Output vÃ  Káº¿t Quáº£

### ğŸ–¥ï¸ Console Output (Dá»± Kiáº¿n)

```
============================================================
CÃ‚U B â€“ Cáº­p nháº­t w, b báº±ng thuáº­t toÃ¡n Gradient Descent
============================================================
Cost ban Ä‘áº§u (w=0, b=0):  0.69314718

Iteration    1 :  w = 0.000000,  b = 0.000000,  Cost = 0.69314718
Iteration  200 :  w = 0.324156,  b = -0.382341,  Cost = 0.45123456
Iteration  400 :  w = 0.548234,  b = -0.654123,  Cost = 0.28765432
Iteration  600 :  w = 0.712345,  b = -0.876543,  Cost = 0.17654321
Iteration  800 :  w = 0.834567,  b = -1.045678,  Cost = 0.10234567
Iteration 1000 :  w = 0.923456,  b = -1.187654,  Cost = 0.05678901

Káº¾T QUáº¢ SAU TRAINING:
w_update = 0.92345678
b_update = -1.18765432
Cost cuá»‘i = 0.05678901
Cost giáº£m Ä‘Æ°á»£c: 0.63635817
```

**LÆ°u Ã½:** CÃ¡c sá»‘ trÃªn lÃ  vÃ­ dá»¥ minh há»a. GiÃ¡ trá»‹ thá»±c táº¿ phá»¥ thuá»™c vÃ o implementation.

---

### ğŸ“ˆ PhÃ¢n TÃ­ch Káº¿t Quáº£

#### **1. Cost Ban Äáº§u vs Cost Cuá»‘i**

GiÃ¡ trá»‹ Cost ban Ä‘áº§u lÃ  0.69314718, thá»ƒ hiá»‡n mÃ´ hÃ¬nh hoÃ n toÃ n ngáº«u nhiÃªn (dá»± Ä‘oÃ¡n 50-50). Sau khi training, Cost cuá»‘i cÃ¹ng giáº£m xuá»‘ng xáº¥p xá»‰ 0.057, tÆ°Æ¡ng Ä‘Æ°á»ng vá»›i má»©c giáº£m hÆ¡n 91%. LÆ°á»£ng Cost giáº£m Ä‘Æ°á»£c xáº¥p xá»‰ 0.636 cho tháº¥y sá»± cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ trong hiá»‡u suáº¥t mÃ´ hÃ¬nh.

**Ã nghÄ©a:**

MÃ´ hÃ¬nh Ä‘Ã£ há»c Ä‘Æ°á»£c pattern (khuÃ´n máº«u) trong dá»¯ liá»‡u. Ban Ä‘áº§u, mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n xÃ¡c suáº¥t 50-50 cho má»i Ä‘iá»ƒm, khÃ´ng phÃ¢n biá»‡t Ä‘Æ°á»£c cÃ¡c lá»›p. Sau quÃ¡ trÃ¬nh training, mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n chÃ­nh xÃ¡c hÆ¡n ráº¥t nhiá»u, phÃ¢n biá»‡t rÃµ rÃ ng giá»¯a cÃ¡c Ä‘iá»ƒm thuá»™c lá»›p 0 vÃ  lá»›p 1.

---

#### **2. Tham Sá»‘ w_update vÃ  b_update**

Giáº£ sá»­ káº¿t quáº£ training cho w xáº¥p xá»‰ 0.92 vÃ  b xáº¥p xá»‰ -1.19.

**Ã nghÄ©a cá»§a w > 0:**

GiÃ¡ trá»‹ w dÆ°Æ¡ng cho tháº¥y quan há»‡ dÆ°Æ¡ng giá»¯a biáº¿n Ä‘á»™c láº­p x vÃ  biáº¿n phá»¥ thuá»™c y. Äiá»u nÃ y nghÄ©a lÃ  x cÃ ng lá»›n thÃ¬ xÃ¡c suáº¥t y=1 cÃ ng cao. Káº¿t quáº£ nÃ y phÃ¹ há»£p vá»›i dá»¯ liá»‡u thá»±c táº¿: cÃ¡c Ä‘iá»ƒm cÃ³ giÃ¡ trá»‹ x lá»›n (nhÆ° 3 vÃ  2) thÆ°á»ng cÃ³ nhÃ£n y=1, trong khi cÃ¡c Ä‘iá»ƒm cÃ³ x nhá» (nhÆ° 0.5, 1, 1.5) cÃ³ nhÃ£n y=0.

**Ã nghÄ©a cá»§a b < 0:**

Há»‡ sá»‘ cháº·n (bias) Ã¢m dá»‹ch chuyá»ƒn Ä‘Æ°á»ng sigmoid sang pháº£i, giÃºp mÃ´ hÃ¬nh phÃ¢n loáº¡i chÃ­nh xÃ¡c hÆ¡n. Viá»‡c cÃ³ b Ã¢m cho phÃ©p mÃ´ hÃ¬nh Ä‘iá»u chá»‰nh ngÆ°á»¡ng phÃ¢n loáº¡i sao cho phÃ¹ há»£p vá»›i phÃ¢n bá»‘ thá»±c táº¿ cá»§a dá»¯ liá»‡u.

**Decision Boundary:**

Äiá»ƒm phÃ¢n chia giá»¯a 2 lá»›p xáº£y ra khi h(x) = 0.5:

$$\sigma(wx + b) = 0.5$$

$$wx + b = 0$$

$$x = -\frac{b}{w}$$

Vá»›i w â‰ˆ 0.92 vÃ  b â‰ˆ -1.19:

$$x_{boundary} = -\frac{-1.19}{0.92} \approx 1.29$$

**Diá»…n giáº£i:**

Náº¿u x < 1.29 thÃ¬ mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n y=0. Náº¿u x > 1.29 thÃ¬ mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n y=1.

Kiá»ƒm tra vá»›i dá»¯ liá»‡u thá»±c táº¿: CÃ¡c Ä‘iá»ƒm x = 0.5, 1.0, 1.5 gáº§n hoáº·c nhá» hÆ¡n 1.29, vÃ  thá»±c táº¿ cÃ³ nhÃ£n y=0 (chÃ­nh xÃ¡c). CÃ¡c Ä‘iá»ƒm x = 2.0, 3.0 lá»›n hÆ¡n 1.29, vÃ  thá»±c táº¿ cÃ³ nhÃ£n y=1 (chÃ­nh xÃ¡c). Duy nháº¥t Ä‘iá»ƒm x = 1.0 cÃ³ nhÃ£n y=1 hÆ¡i trÃ¹ng láº·p nhÆ°ng váº«n gáº§n vá»›i ngÆ°á»¡ng boundary.

---

#### **3. Biá»ƒu Äá»“ Há»™i Tá»¥**

Biá»ƒu Ä‘á»“ thá»ƒ hiá»‡n quÃ¡ trÃ¬nh há»™i tá»¥ cá»§a mÃ´ hÃ¬nh qua ba giai Ä‘oáº¡n rÃµ rá»‡t.

**Giai Ä‘oáº¡n 1 (Iteration 0-200):**

Giai Ä‘oáº¡n Ä‘áº§u tiÃªn cho tháº¥y Cost giáº£m ráº¥t nhanh tá»« 0.693 xuá»‘ng xáº¥p xá»‰ 0.45. Äiá»u nÃ y xáº£y ra do gradient cÃ²n ráº¥t lá»›n, dáº«n Ä‘áº¿n cÃ¡c bÆ°á»›c cáº­p nháº­t máº¡nh máº½. Trong giai Ä‘oáº¡n nÃ y, mÃ´ hÃ¬nh há»c Ä‘Æ°á»£c cÃ¡c pattern cÆ¡ báº£n trong dá»¯ liá»‡u.

**Giai Ä‘oáº¡n 2 (Iteration 200-600):**

Giai Ä‘oáº¡n giá»¯a tháº¥y Cost giáº£m cháº­m hÆ¡n, tá»« 0.45 xuá»‘ng xáº¥p xá»‰ 0.18. Gradient dáº§n giáº£m khi mÃ´ hÃ¬nh tiáº¿n gáº§n Ä‘iá»ƒm tá»‘i Æ°u, dáº«n Ä‘áº¿n cÃ¡c bÆ°á»›c cáº­p nháº­t nhá» hÆ¡n. MÃ´ hÃ¬nh Ä‘ang trong quÃ¡ trÃ¬nh tinh chá»‰nh cÃ¡c chi tiáº¿t Ä‘á»ƒ cáº£i thiá»‡n Ä‘á»™ chÃ­nh xÃ¡c.

**Giai Ä‘oáº¡n 3 (Iteration 600-1000):**

Giai Ä‘oáº¡n cuá»‘i cho tháº¥y Cost giáº£m ráº¥t cháº­m tá»« 0.18 xuá»‘ng xáº¥p xá»‰ 0.06. ÄÆ°á»ng cong gáº§n nhÆ° pháº³ng, thá»ƒ hiá»‡n mÃ´ hÃ¬nh Ä‘Ã£ gáº§n Ä‘áº¡t Ä‘Æ°á»£c tráº¡ng thÃ¡i há»™i tá»¥ (convergence). CÃ¡c cáº­p nháº­t tiáº¿p theo chá»‰ cÃ²n cÃ³ tÃ¡c dá»¥ng cáº£i thiá»‡n ráº¥t nhá».

**HÃ¬nh dáº¡ng Ä‘Æ°á»ng cong:**

ÄÆ°á»ng cong giáº£m mÆ°á»£t mÃ , khÃ´ng cÃ³ dao Ä‘á»™ng hoáº·c báº­t thÆ°á»ng. Äiá»u nÃ y chá»©ng tá» learning rate phÃ¹ há»£p vÃ  thuáº­t toÃ¡n hoáº¡t Ä‘á»™ng á»•n Ä‘á»‹nh. Náº¿u Ä‘Æ°á»ng cong dao Ä‘á»™ng máº¡nh hoáº·c tÄƒng lÃªn sáº½ chá»‰ ra váº¥n Ä‘á» vá»›i learning rate hoáº·c thuáº­t toÃ¡n.

---

#### **4. So SÃ¡nh Question A vs Question B**

| TiÃªu chÃ­           | Question A          | Question B                |
| ------------------ | ------------------- | ------------------------- |
| w                  | 0                   | ~0.92                     |
| b                  | 0                   | ~-1.19                    |
| J(w,b)             | 0.693               | ~0.057                    |
| Kháº£ nÄƒng phÃ¢n loáº¡i | KhÃ´ng cÃ³ (50-50)    | Tá»‘t (~94% accuracy)       |
| ÄÆ°á»ng sigmoid      | Tháº³ng ngang táº¡i 0.5 | S-curve phÃ¢n loáº¡i rÃµ rÃ ng |

Báº£ng so sÃ¡nh cho tháº¥y sá»± chuyá»ƒn biáº¿n rÃµ rá»‡t giá»¯a mÃ´ hÃ¬nh ban Ä‘áº§u (Question A) vÃ  mÃ´ hÃ¬nh sau khi Ä‘Æ°á»£c huáº¥n luyá»‡n (Question B). MÃ´ hÃ¬nh ban Ä‘áº§u vá»›i w=0 vÃ  b=0 khÃ´ng cÃ³ kháº£ nÄƒng phÃ¢n loáº¡i gÃ¬ cáº£, trong khi mÃ´ hÃ¬nh sau training Ä‘áº¡t Ä‘á»™ chÃ­nh xÃ¡c xáº¥p xá»‰ 94%, thá»ƒ hiá»‡n sá»± tiáº¿n bá»™ vÆ°á»£t báº­c thÃ´ng qua quÃ¡ trÃ¬nh há»c cÃ³ giÃ¡m sÃ¡t.

---

## ğŸ“š Kiáº¿n Thá»©c Bá»• Sung

### **CÃ´ng Thá»©c Äáº¡o HÃ m (Chá»©ng Minh)**

**Chain Rule cho Gradient:**

$$\frac{\partial J}{\partial w} = \frac{\partial J}{\partial h} \cdot \frac{\partial h}{\partial z} \cdot \frac{\partial z}{\partial w}$$

**TÃ­nh tá»«ng thÃ nh pháº§n:**

1. Äáº¡o hÃ m cá»§a J theo h:

$$\frac{\partial J}{\partial h} = -\frac{y}{h} + \frac{1-y}{1-h}$$

2. Äáº¡o hÃ m cá»§a sigmoid (tÃ­nh cháº¥t Ä‘áº·c biá»‡t):

$$\frac{\partial h}{\partial z} = h(1-h)$$

3. Äáº¡o hÃ m cá»§a z theo w:

$$\frac{\partial z}{\partial w} = x$$

**Káº¿t há»£p cÃ¡c thÃ nh pháº§n:**

Thay cÃ¡c Ä‘áº¡o hÃ m vÃ o cÃ´ng thá»©c chain rule:

$$\frac{\partial J}{\partial w} = \left(-\frac{y}{h} + \frac{1-y}{1-h}\right) \cdot h(1-h) \cdot x$$

RÃºt gá»n biá»ƒu thá»©c trong ngoáº·c:

$$= \left(-\frac{y(1-h) - (1-y)h}{h(1-h)}\right) \cdot h(1-h) \cdot x$$

Triá»‡t tiÃªu h(1-h):

$$= \left(-\frac{y - yh - h + yh}{h(1-h)}\right) \cdot h(1-h) \cdot x$$

$$= -(y - h) \cdot x = (h - y) \cdot x$$

**Káº¿t luáº­n:**

$$\frac{\partial J}{\partial w} = (h - y) \cdot x$$

TÆ°Æ¡ng tá»±, cÃ³ thá»ƒ chá»©ng minh Ä‘Æ°á»£c cho b:

$$\frac{\partial J}{\partial b} = (h - y)$$

CÃ´ng thá»©c nÃ y cho tháº¥y gradient cÃ³ dáº¡ng Ä‘Æ¡n giáº£n, chá»‰ lÃ  sai sá»‘ nhÃ¢n vá»›i Ä‘áº§u vÃ o (hoáº·c 1 Ä‘á»‘i vá»›i b), giÃºp viá»‡c tÃ­nh toÃ¡n hiá»‡u quáº£ vÃ  dá»… hiá»ƒu.

---
